We thank the reviewers for their insightful comments. In what follows, we will
tried to extensively attend all of the mentionned concerns.

First off, and before going into answers to detailled comment, some of the
reviewers complained about the scale of the experiments:
Reviewer #8: "A few small scale experiments"
Reviewer #7: "the experiments are not ambitious in this sense"
Let us emphasize that the computational time required to run an experiment with
time discretization $\delta t$ is proportional to $\frac{1}{\delta t}$ (this is
the best you can expect, since you need to have the same amount of physical
time experience to learn the same behavior). With standard discretization, on
the mujoco environments, it takes approximately 1 hour with decent hardware to
obtain reasonable results. If we want to use a discretization 100 times finer
grained than the standard discretization, we can only expect to obtain
reasonable results after approximately 100 hours. Using radically finer grained
discretization, and typically going to the next order of magnitude would
require a full month of computation for a single experiment, which is
impossible to obtain with the ressources at our disposal.  At the moment, for
our smallest time discretizations, each experiment on Mujoco takes
approximately a day and a half. 
We propose adding the following mention as a footnote to the experimental section
in the text: ""

Reviewer #1:
Thank you for your encouraging review! We propose adding the following mention
at the end of the introduction, to make the contribution more precise.
"In what follows, theorem 1, 2 and 3 were informally stated in [Doya, Baird], but
not formally proven, while theorem 4 and 5 are novel".

Thank you for pointing out the theorem numbering issue.

Reviewer #2:
a. We believe there was a confusion between the BipedalWalker environment that
we are using (https://gym.openai.com/envs/BipedalWalker-v2/) whose best
obtainable result is around 300, and the walker2d environment which is used in
[Haarnoja 2019] whose return values are not comparable. For other environments,
(Ant and Cheetah) our baseline DDPG seem to behave comparably to their implementation,
and DAU provides better or comparable results on Ant, irrelevant of the time
discretization, and better results on Cheetah for the default time discretization. Do
note that, except for BipedalWalker, where our highest time discretiation is smaller
than the default one, the highest time discretization used is the standard one.

b.c. TODO

d. We have deffered the presentation of the discrete algorithm to the appendix, and
now present the continuous version in the main paper instead.

f. Thank you for pointing out the typos.

Reviewer #7:
- "Dueling DQN, as the closest relative to DAU should be a baseline in the
  experiment section.": We experimented with a DDPG variant of dueling DQN and
  found that it provided similar results to DDPG, we did not present the
  result, to prevent clouding the figures.
- In the $\delta t = 1$ case, we recover an algorithm which is close, but not exactly
equivalent to Dueling DQN, since
  1. We do not use a target network, while Dueling DQN does
  2. In their final version, Dueling DQN reparameterizes A as A(s, a) - mean_{a'} A(s, a'),
  while we remain closer to the theory and use A(s, a) - max_{a'} A(s, a') instead.
  We propose adding the following as a footnote in the text:
  "TODO"

- "The theoretical arguments are about the limit \delta t going to 0, however,
  the experiments are not ambitious in this sense, they barely cover two orders
  of magnitude." see comment above

- "you claim that performance “becomes more variable”, can you capture this
  diagnosis in a quantitative metric?". We are currently using the standard
  deviation across runs to quantify variability. For DDPG, the pendulum
  learning curves show increased standard deviation when $\delta t$ becomes
  smaller. We propose rephrasing "becomes more variable" by "the standard
  deviation accross runs increases".

- memory buffer: When experimenting, we found that, for all $\delta t$'s, using
  too small replay buffers had a very negative impact on performances. This is why
  we chose to use one, even though this does not exactly match our theory.

Thanks for pointing out the typos, we will make sure to fix them.

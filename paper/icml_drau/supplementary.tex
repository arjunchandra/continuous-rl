\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}

\newtheorem{theorem}{Theorem}
\input{macros.tex}

\begin{document}
\section{Proofs}
We hereby give proofs for all the results presented in the paper.
The first result presented is a proof of convergence for discretized
trajectories.
\begin{theorem}
	Let $F: {\cal S} \times {\cal A} \rightarrow \bb{R}^n$ and $\pi: {\cal S}
	\rightarrow {\cal A}$ be the dynamic and policy functions. Assume that,
	for any $a$, $s \rightarrow F(s, a)$ and $s \rightarrow F(s, \pi(s))$
	are ${\cal C}^1$, bounded and $K$-lipschitz.  For
	a given $s_0$, define the trajectory $(s_t)_{t\geq 0}$ as the unique
	solution of the differential equation
	\begin{equation}
		\frac{ds_t}{dt} = F(s_t, \pi(s_t)).
		\label{eq:diff}
	\end{equation}
	For any $\deltat > 0$, define the discretized trajectory
	$(s_\deltat^k)_k$ recurrently as $s_\deltat^0 = s_0$,
	$s_\deltat^{k + 1}$ is the value at time $\deltat$ of
	the unique solution of
	\begin{equation}
		\frac{d\tilde{s}_t}{dt} = F(\tilde{s}_t, \pi(s_\deltat^k))
	\end{equation}
	with initial point $s_\deltat^k$.
	Then, there exists $C > 0$ such that, for every $t \geq 0$
	\begin{equation}
		\|s_t - s_\deltat^{\lfloor \frac{t}{\deltat} \rfloor}\|
		\leq \deltat \frac{C}{K}e^{Kt}.
	\end{equation}
	Notably, discretized trajectories converge pointwise to continuous trajectories.
	\label{th:traj-conv}
\end{theorem}
\begin{proof}
	\input{proof_traj.tex}
\end{proof}
In what follows, we will assume that the reward function $r: {\cal S} \times {\cal A} \rightarrow \bb{R}$
is bounded, to ensure existence of $V^\pi$ and $V^\pi_\deltat$ for all $\deltat$.\begin{theorem}
	Under suitable assumptions \TODO{ref appendix}, for all $s \in {\cal
	S}$, one has
	%\begin{equation}
	%\label{eq:conv-value}
	$V^\pi_\deltat(s) = V^\pi(s) + \bigO(\deltat)$
	%\end{equation}
	when $\deltat\to 0$.
	\label{th:conv-value}
\end{theorem}
\begin{proof}
	\input{proof_conv_value.tex}
\end{proof}

For the following proof, we further assume that both $V^\pi$ and
$V^\pi_\deltat$ are continuously differentiable, and that the gradient and
hessian of $V^\pi_\deltat$ w.r.t. $s$ are uniformly bounded in both $s$ and $\deltat$.
We also assume convergence of $\partial_s V^\pi_\deltat(s)$ to $\partial_s V^\pi(s)$ for
all $s$.
\begin{theorem}
	Under the above hypothesis, there exists $A^\pi: {\cal S} \rightarrow
	\bb{R}$ such that $A^\pi_\deltat$ converges pointwise to $A^\pi$ as
	$\deltat$ goes to $0$.
\end{theorem}
\begin{proof}
	\input{proof_conv_adv.tex}
\end{proof}

We now show that policy improvement works with the continous time advantage function, i.e.\
\begin{theorem}
	Let $\pi$ and $\pi'$ be two policies such that both $s \rightarrow r(s, \pi(s))$ and
	$s \rightarrow r(s, \pi'(s))$ are continuous.
	Assume that both $V^\pi$ and $V^{\pi'}$ are continuously differentiable.
	Define the advantage function for policies $\pi$ and $\pi'$ as in Eq.~\eqref{eq:adv_function}.

	If for all $s$, $A^\pi(s, \pi'(s)) \geq 0$, then for all $s$, $V^\pi(s) \leq V^{\pi'}(s)$.
\end{theorem}
\begin{proof}
	\input{policy_improvement.tex}
\end{proof}
\begin{theorem}
	Discrete parameter trajectories converge to continuous trajectories.
\end{theorem}
\begin{proof}
	\input{proof_conv_params.tex}
\end{proof}
\section{Implementation details}
\input{impl_details.tex}
\end{document}

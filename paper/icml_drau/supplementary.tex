\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}

\newtheorem{theorem}{Theorem}
\input{macros.tex}

\begin{document}
\section{Proofs}

We now give proofs for all the results presented in the paper. Most
proofs follow standard patterns from calculus and numerical schemes for
differential equations, except for Theorem~\ref{thm:policyimprovement},
which uses an argument specific to reinforcement learning to prove that
the continuous-time advantage function contains all the necessary
information for policy values.


The first result presented is a proof of convergence for discretized
trajectories.
\begin{theorem}
	Let $F\from {\cal S} \times {\cal A} \rightarrow \bb{R}^n$ and $\pi\from {\cal S}
	\rightarrow {\cal A}$ be the dynamic and policy functions. Assume that,
	for any $a$, $s \rightarrow F(s, a)$ and $s \rightarrow F(s, \pi(s))$
	are ${\cal C}^1$, bounded and $K$-lipschitz.  For
	a given $s_0$, define the trajectory $(s_t)_{t\geq 0}$ as the unique
	solution of the differential equation
	\begin{equation}
		\frac{ds_t}{dt} = F(s_t, \pi(s_t)).
		\label{eq:diff}
	\end{equation}
	For any $\deltat > 0$, define the discretized trajectory
	$(s_\deltat^k)_k$ which amounts to maintaining each action for a
	time interval $\deltat$; it is defined by induction as $s_\deltat^0 = s_0$,
	$s_\deltat^{k + 1}$ is the value at time $\deltat$ of
	the unique solution of
	\begin{equation}
		\frac{d\tilde{s}_t}{dt} = F(\tilde{s}_t, \pi(s_\deltat^k))
	\end{equation}
	with initial point $s_\deltat^k$.
	Then, there exists $C > 0$ such that, for every $t \geq 0$
	\begin{equation}
		\|s_t - s_\deltat^{\lfloor \frac{t}{\deltat} \rfloor}\|
		\leq \deltat \frac{C}{K}e^{Kt}.
	\end{equation}
	Therefore, discretized trajectories converge pointwise to continuous trajectories.
	\label{th:traj-conv}
\end{theorem}
\begin{proof}
	\input{proof_traj.tex}
\end{proof}

In what follows, we assume that the continuous-time reward function $r\from {\cal S} \times {\cal A} \rightarrow \bb{R}$
is bounded, to ensure existence of $V^\pi$ and $V^\pi_\deltat$ for all $\deltat$
.\begin{theorem}
	Assume that $r\from {\cal S} \times {\cal A} \rightarrow \bb R$ is bounded, and
	that $s \rightarrow r(s, \pi(s))$ is $L_r$-Lipschitz continuous, then
	for all $s \in {\cal S}$, one has
	%\begin{equation}
	%\label{eq:conv-value}
	$V^\pi_\deltat(s) = V^\pi(s) + \smallo(\deltat)$
	%\end{equation}
	Lhen $\deltat\to 0$.
	\label{th:conv-value}
\end{theorem}
\begin{proof}
	\input{proof_conv_value.tex}
\end{proof}

For the following proof, we further assume that both $V^\pi$ and
$V^\pi_\deltat$ are continuously differentiable, and that the gradient and
Hessian of $V^\pi_\deltat$ w.r.t. $s$ are uniformly bounded in both $s$ and $\deltat$.
We also assume convergence of $\partial_s V^\pi_\deltat(s)$ to $\partial_s V^\pi(s)$ for
all $s$.

\begin{theorem}
	Under the hypothesis above, there exists $A^\pi\from {\cal S} \rightarrow
	\bb{R}$ such that $A^\pi_\deltat$ converges pointwise to $A^\pi$ as
	$\deltat$ goes to $0$. \TODO{give expression for $A^\pi$ (used
	later + interesting per se)}
\end{theorem}

\begin{proof}
	\input{proof_conv_adv.tex}
\end{proof}

We now show that policy improvement works with the continous time advantage function, i.e.\
\begin{theorem}
\label{thm:policyimprovement}
	Let $\pi$ and $\pi'$ be two policies such that both $s \rightarrow r(s, \pi(s))$ and
	$s \rightarrow r(s, \pi'(s))$ are continuous.
	Assume that both $V^\pi$ and $V^{\pi'}$ are continuously differentiable.
	Define the advantage function for policies $\pi$ and $\pi'$ as in Eq.~\eqref{eq:adv_function}.

	If for all $s$, $A^\pi(s, \pi'(s)) \geq 0$, then for all $s$,
	$V^\pi(s) \leq V^{\pi'}(s)$. \TODO{Moreover, if
	$V^{\pi'}(s)>V^{\pi}(s)$, then $A^\pi(s,\pi'(s))>0$ for some state $s$.}
\end{theorem}

\begin{proof}
	\input{policy_improvement.tex}
\end{proof}
\begin{theorem}
	Discrete parameter trajectories converge to continuous trajectories.
\end{theorem}
\begin{proof}
	\input{proof_conv_params.tex}
      \end{proof}

      \begin{theorem}
	Exploration strategy
\end{theorem}
\begin{proof}
	\input{proof_conv_expl.tex}
\end{proof}
\section{Implementation details}
\input{impl_details.tex}
\end{document}

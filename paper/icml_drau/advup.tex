%! TEX root = icml_drau.tex
\section{Reinforcement learning with a continuous time limit}
As seen in Sec.~\ref{sec:framework}, there is no continuous time limit to
Q-learning. In continuous time, $Q^\pi$ is independant of actions and can thus
not be used to select promising actions.  In near continuous time, $Q^\pi_\deltat$
still depends on actions, and can still be used to choose promising actions
under the current policy. However, when approximating $Q^\pi_\deltat$, if the
approximation error is much bigger than $\bigO(\deltat)$, approximation errors
dominates, and the hierarchy of actions given by the approximation is likely to
be erroneous.  When the approximate Q-function is initialized, if the effect of
actions on the Q-function is order of magnitudes higher than what it should be,
approximating $Q^\pi_\deltat$ is likely to be difficult. Besides, the error is likely
to be further propagated when the equation used to update our approximate $Q$
function relies on bootstraping, as is the case for usual temporal difference
derived methods.

In what follows, a discrete algorithm that has a well defined time continuous
limit is defined. The algorithm relies on three elements, namely, defining and
learning a quantity that contains some information on the hierarchy of actions
while admitting a time continuous limit, defining exploration methods that
admit a continuous time limit and defining learning rates that induce well
behaved parameter trajectories when $\deltat$ goes to $0$.

\subsection{Advantage Updating}
\label{subsec:reparam}
To define an object which contains the same information on actions as
$Q^\pi_\deltat$, but admits an action dependant continuous time limit, , it is
natural to define
\begin{align}
	A^\pi_\deltat(s, a) &= \frac{Q^\pi_\deltat(s,a) - V^\pi_\deltat(s)}{\deltat},
    \label{eq:adv}
\end{align}
a rescaled version of the advantage function, as the difference between between
$Q^\pi_\deltat(s, a)$ and $V^\pi\deltat(s)$ is of order $\bigO(\deltat)$.
Contrary to the action state value function, the rescaled advantage function converges
to an action dependent quantity when $\deltat$ goes to $0$
\begin{align}
	A^\pi_\deltat(s, a)=&\reward(s, a)+\frac{\gamma^{\deltat}\E_{\delta s \sim \delta s(s, a)}V^\pi_\deltat(s + \delta s)-V^\pi\deltat(s)}{\deltat}\nonumber\\
	=&\reward(s, a)+\log(\gamma) V^\pi(s)+\partial_s V^\pi(s)f(s, a)\nonumber\\
         &+\frac{\text{tr}\left(\Sigma^T(s, a)\partial^2_{s^2}V^\pi(s) \Sigma(s,a)\right)}{2} + O(\deltat)\nonumber.
\end{align}
The discretized state action value function can then be rewritten as
\begin{equation}
	Q^\pi_\deltat(s, a) = V^\pi_\deltat(s) + \deltat A^\pi_\deltat(s, a).
	\label{eq:reparam_q_pi}
\end{equation}
A natural way of approximating $V^\pi_\delta$ and $A^\pi_\deltat$ is to apply
\emph{Sarsa} or Q-learning to a reparameterized Q-function approximator
\begin{equation}
	Q_\Theta(s, a) = V_{\theta}(s) + \deltat A_{\psi}(s, a).
\end{equation}
To avoid cumbersome notations, in what follows, parameter indices are dropped.
At initialization, if both $V_{\theta}$ and $A_{\psi}$ are initialized
independantly of $\deltat$, this parameterization reasonably scales the
contribution of actions relative to states in the value of $Q$.  

However, the proposed reparameterization does not, on its own, guarantee that
when $Q$ correctly approximates $Q^\pi_\deltat$, $A$ approximates $A^\pi_\deltat$.
Indeed, for any given pair $V$, $A$,
\begin{align}
	\tilde{V}(s) &= V(s) - f(s)\\
	\tilde{A}(s, a) &= A(s, a) + \frac{f(s)}{\deltat}
\end{align}
yields the exact same $Q$ function. Consequently, $Q$ can correctly approximate
$Q^\pi$ even if $A$ is arbitrarily far from $A^\pi$.
To enforce, identifiability of $A$, one must enforce the consistency equation
\begin{equation}
	Q^\pi_\deltat(s, \pi(s)) - V^\pi_\deltat(s) = 0
\end{equation}
on the approximate $A$ and $V$. This translates to
\begin{equation}
	A(s, \pi(s)) = 0.
\end{equation}
With this additional constraint, if $Q^\pi_\deltat = Q$,
\begin{align}
	A^\pi_\deltat(s, a) &= \frac{Q^\pi_\deltat(s,a) - V^\pi_\deltat(s)}{\deltat}\\
		    &= \frac{Q(s,a) - Q(s, \pi(s))}{\deltat}\\
		    &= A(s, a).
\end{align}
In the spirit of~\cite{dueling_nets}, this condition is enforced by writing $A$ as
\begin{equation}
	A(s, a) = \bar{A}(s, a) - \bar{A}(s, \pi(s)).
\end{equation}
With this parameterization, one can hope to learn an approximation of $A^\pi_\deltat$,
from which a proper hiearchy of actions can be derived for any $\deltat$.

\subsection{Time invariant exploration}
\label{subsec:explo}
To obtain a time invariant reinforcement learning algorithm, a time invariant
exploration scheme is required. All the exploration schemes hereby considered
generate sequences of actions as
\begin{equation}
	a^k_\deltat = \pi^\text{explore}_\deltat(s^k_\deltat, \nu^k_\deltat)
\end{equation}
with $(\nu^k_\deltat)_k$ a sequence of random variables, independant from the $a$'s and $s$'s.
For instance, $\varepsilon$-greedy exploration naturally falls in this framework, with each $\nu^k_\deltat$
being a pair composed of a bernoulli random variables $b^k_\deltat$ with probability of being $1$ equal to $\varepsilon$,
and of a uniform categorical random variable $\xi^k_\deltat$ on actions. Exploratory actions are then selected as
\begin{equation}
	a^k_\deltat = (1 - b^k_\deltat) \text{argmax}_{a'} Q^\pi_\deltat(s, a') + b^k_\deltat \xi^k_\deltat.
\end{equation}

A sufficient condition for the exploratory policy to admit a time continuous
limit is for $\pi^\text{explore}_\deltat$ to be a measurable function, and for
$\nu_\deltat$ to admit one, i.e. that the sequence $\nu_\deltat$ converges to a
well defined continuous stochastic process $\nu$ as $\deltat$ goes to $0$.
\TODO{I'm convinced that this is true, reviewers might not be.}

The exploration scheme used~\cite{ddpg}, which consists in adding an
\emph{Ornstein-Uhlenbeck}~\cite{orn-uhl} process (OU process) to actions, is naturally time.
invariant. It can be rewritten as
\begin{equation}
	a^k_\deltat = \pi(s^k_\deltat) + \nu^k_\deltat
\end{equation}
where $\nu^k_\deltat$ is the discretization of a time continuous \emph{Ornstein-Uhlenbeck}
process, defined as
\begin{equation}
	d\nu_t = - \nu_t \theta dt + \sigma \dbt.
	\label{eq:orn_uhl}
\end{equation}

On the other hand, the same argument cannot be applied to $\varepsilon$-greedy
exploration, as the $\nu_\deltat$'s used to define it do not converge to a 
well defined stochastic process, but to a white noise.\footnote{\TODO{$\varepsilon$-greedy
		does not, in general, converges to a well defined exploration scheme in continuous time.
		For instance, if actions directly affects position, $\varepsilon$-greedy results in white-noise
trajectories in state space. Show this in more detail in appendix ?}}

To obtain a temporally coherent exploration scheme in discrete action environment
$\nu_\deltat$ is chosen to be a discretization of a $|\mathcal{A}|$ dimensional
continuous OU process, and set
\begin{equation}
	a^k_\deltat = \text{argmax}_{a'} \left(Q^\pi_\deltat(s, a') + \nu_\deltat^k[a']\right)
\end{equation}
where $\nu_\deltat^k[a']$ denotes the $a'$-th component of the discretized OU process at time
$k$. The resulting exploration scheme converges to a proper time continuous exploration scheme.

\subsection{Algorithms}
\label{subsec:algorithm}
To learn $V_{\theta}$ and $A_{\psi}$, variants of Q-learning for continuous~\cite{ddpg} and
discrete~\cite{dqn} action spaces are used. In details, $V_{\theta}$ and $A_{\psi}$ are learnt
using approximate version of the following bellman equation
\begin{align}
	V^{\pi}_\deltat(s) + A^{\pi}_\deltat(s, a) &= \E_{\delta r \sim \delta r(s, a)}\delta r\nonumber\\ 
				   &+ \gamma^{\deltat}  \E_{\delta s \sim \delta s(s, a)} V^{\pi}_\deltat(s + \delta s)\label{eq:bellman_A}\\
	A^{\pi}_\deltat(s, \pi(s)) &= 0\label{eq:max_A}
\end{align}
for a greedy exploitation policy $\pi(s) = \text{argmax}_{a'}A^\pi_\deltat(s, a')$,
exactly implemented in the case of discrete actions, and approximated by a
neural network $\pi_\phi(s)$, trained to maximize (an approximation of)
$A^\pi_\deltat(s, \pi_\phi(s))$ in the continuous case.  Eq.~\eqref{eq:max_A} is
directly verified by $A_{\theta}$, owing to the reparameterization described
in~\ref{subsec:reparam}.  To approximately verify~\eqref{eq:bellman_A}, the
corresponding squared residual is minimized by an approximate gradient descent.
The corresponding update equations when learning from incoming transitions of
an exploratory trajectory are
\begin{align}
	\delta Q^k_\deltat &= A(s^k, a^k) - \frac{r^k \deltat + \gamma^{\deltat} V(s^{k}+\delta s^{k}) - V(s^k)}{\deltat}\\
	\theta^{k+1}_\deltat &= \theta^k_\deltat + \eta^1_\deltat \partial_{\theta} V(s^k) \delta Q^k_\deltat\\
	\psi^{k+1}_\deltat &= \psi^k_\deltat + \eta^2_\deltat \partial_{\psi} A(s^k, a^k) \delta Q^k_\deltat.
	\label{eq:approx_bellman_A}
\end{align}
where $\eta$'s are learning rates. In practice, $A$ and $V$ are learned using
transitions sampled from a replay buffer instead of trajectory samples, to
prevent excessive correlation between training samples~\cite{dqn}. Pseudocode
for the discrete actions algorithm is given in Alg.~\ref{alg:dau}. Pseudocode
for the continuous actions algorithm is given in the supplementary material.
\begin{algorithm}[ht]
	\caption{Deep Advantage Updating (Discrete actions)}
	\input{dauc.tex}
	\label{alg:dau}
\end{algorithm}

\subsection{Learning rates scalings}
\label{subsec:lr}
For the discrete algorithm to admit a continuous time limit, the trajectories
of parameters must converge to well defined trajectories as $\deltat$ goes to
$0$.  Intuitively, this means that optimization steps should not be too large,
in which case parameter trajectories could diverge in a single gradient step,
or too small, in which case the trajectories would converge to single points as
$\deltat$ goes to $0$.  This in turn imposes conditions on the scalings of the
learning rates of invariant algorithms with respect to the discretization
timestep.

The updates equations on $\theta$ and $\psi$ are described in
Eq.~\eqref{eq:approx_bellman_A}.  To have parameter steps that do not grow
larger or shrink to $0$ as $\deltat \leftarrow 0$, both $\delta \theta^1$ and
$\delta \theta^2$ should verify stochastic differential equations, and notably
have a deterministic component of order $\deltat$. As $\deltat$ goes to $0$,
updates are rephrased as
\begin{align}
	\delta Q^k_\deltat &= A(s^k_\deltat, a^k_\deltat) - r^k_\deltat - \log(\gamma) V(s^k_\deltat) - \partial_s V(s^k_\deltat) \frac{\delta s^k_\deltat}{\deltat}\nonumber \\&- \frac{(\delta s^k_\deltat)^T \partial_{s^2}^2 V(s^k_\deltat) \delta s^k_\deltat}{2\deltat} + \smallo\left(\frac{\eta^1_\deltat}{\sqrt{\deltat}}\right)\\
	\delta \theta^k_\deltat &= \eta^1_\deltat \partial_{\theta} V(s^k_\deltat) \delta Q^k_\deltat + \smallo\left(\frac{\eta^1_\deltat}{\sqrt{\deltat}}\right)\\
	\delta \psi^k_\deltat &= \eta^2_\deltat \partial_{\psi} A(s^k_\deltat) \delta Q^k_\deltat + \smallo\left(\frac{\eta^2_\deltat}{\sqrt{\deltat}}\right).
\end{align}
For parameter updates to follow a stochastic differential equations, one must
have $\eta^1_\deltat = \alpha^1 \deltat$ and $\eta^2_\deltat = \alpha^2
\deltat$. With this choice of learning rates, the limit SDEs are
\begin{align}
	F &= A(s_t, a_t) - r_t - \log(\gamma) V(s_t) - \partial_s V(s_t) f(s_t, a_t)\nonumber\\
	G &= - \partial_s V(s_t, a_t) \Sigma(s_t, a_t)\nonumber\\
	d\theta_t &= (\alpha^1 F  + D \dbt)\partial_{\theta} V(s_t)\nonumber\\
	d\psi_t &= (\alpha^2 F  + D \dbt)\partial_{\psi} A(s_t, a_t)\nonumber.
\end{align}


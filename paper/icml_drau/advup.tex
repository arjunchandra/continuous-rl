%! TEX root = icml_drau.tex
\section{Reinforcement Learning with a Continuous-Time Limit}

We now define a discrete algorithm with a well-defined continuous-time
limit.  It relies on three elements: defining and learning a quantity
that still contains information on action rankings in the limit, using
exploration methods with a meaningful limit, and scaling learning rates
to induce well-behaved parameter trajectories when $\deltat$ goes to $0$.

% When the approximate $Q$-function is initialized, if the effect of
% actions on the $Q$-function is order of magnitudes higher than what it should be,
% approximating $Q^\pi_\deltat$ is likely to be difficult.
%Besides, the error is likely
%to be further propagated when the equation used to update our approximate $Q$
%function relies on bootstraping, as is the case for usual temporal difference
%derived methods.

\subsection{Advantage Updating}
\label{subsec:reparam}

As seen above, there is no continuous time limit to $Q$-learning, because
$Q^\pi$ becomes independent of actions and thus cannot be
used to select actions.  In near-continuous time,
$Q^\pi_\deltat$ still depends on actions, and could still be used to
choose actions. However, when
approximating $Q^\pi_\deltat$, if the approximation error is much larger
than $\bigO(\deltat)$, this error dominates, the ranking of
actions given by the approximated $Q^\pi_\deltat$ is likely to be erroneous.

To define an object which contains the same information on actions as
$Q^\pi_\deltat$, but admits a learnable action-dependent limit, it is
natural to define \cite{adv_upd}
\begin{align}
	A^\pi_\deltat(s, a) &\mathdef \frac{Q^\pi_\deltat(s,a) - V^\pi_\deltat(s)}{\deltat},
    \label{eq:adv}
\end{align}
a rescaled version of the advantage function, as the difference between between
$Q^\pi_\deltat(s, a)$ and $V^\pi_\deltat(s)$ is of order
$\bigO(\deltat)$. This amounts to splitting $Q$ into value and advantage,
and observing that these scale very differently when $\deltat\to 0$.

Contrary to the action state value function,
this
rescaled advantage function converges when $\deltat\to 0$
to an action-dependent quantity which, if it can be learned, contains the
necessary information for policy improvement. A formal statement is given in the supplementary material.


% \LB{In next equation, tiny detail but I would write $\frac{1}{\deltat}(r\deltat+...)$, easier to read.}
% \begin{align}
% 	A^\pi_\deltat(s, a)=&\reward(s, a)+\frac{\gamma^{\deltat}\E_{\deltas \sim \deltas(s, a)}V^\pi_\deltat(s + \deltas)-V^\pi_\deltat(s)}{\deltat}\nonumber\\
% 	=&\reward(s, a)+\log(\gamma) V^\pi(s)+\partial_s V^\pi(s)f(s, a)\nonumber\\
%          &+\frac{\text{tr}\left(\Sigma^T(s, a)\partial^2_{s^2}V^\pi(s) \Sigma(s,a)\right)}{2} + O(\deltat)\nonumber.
% \end{align}

%   \begin{theorem}
%     There exists $A^\pi(s, a)$ such that for all $a, s$:
%     \begin{equation}
%       A^\pi_\deltat(s, a) \rightarrow_{\deltat \rightarrow 0} A^\pi(s, a)
%     \end{equation}
%     Moreover, $A^\pi$ contains all the necessary information for policy improvement. The policy $\pi$ optimal if and only if $A^\pi(s, \pi(s)) = \max_{a'}A^\pi(s, a')$ for all $s \in {\cal S}$.
%     \TODO{Is this true? What is true? Is this real life? We should probably add a reasonable hypothesis, for example $\pi_\deltat^* \rightarrow \pi^*$}
%   \end{theorem}

The discretized $Q$-function rewrites as
\begin{equation}
	Q^\pi_\deltat(s, a) = V^\pi_\deltat(s) + \deltat A^\pi_\deltat(s, a).
	\label{eq:reparam_q_pi}
\end{equation}
A natural way to approximate $V^\pi_\deltat$ and $A^\pi_\deltat$ is to apply
\emph{Sarsa} or $Q$-learning to a reparameterized $Q$-function approximator
\begin{equation}
	Q_\Theta(s, a) \deq V_{\theta}(s) + \deltat A_{\psi}(s, a).
\end{equation}
with $\Theta \deq (\theta, \psi)$. (To avoid cumbersome notation, 
parameter indices are dropped in what follows.)  At initialization, if both $V_{\theta}$ and
$A_{\psi}$ are initialized independently of $\deltat$, this parameterization
provides reasonable scaling of the contribution of actions versus states
in $Q$.
Our goal is for $V_\theta$ to approximate $V^\pi_\deltat$ and for
$A_{\psi}$ to approximate $A^\pi_\deltat$.


%\TODO{CT and LB are fighting each other about the importance which should be given to the next paragraph. A impartial judge should give his opinion. (CT is wrong)}
Still, this reparameterization does not, on its own, guarantee that $A$
correctly approximates $A^\pi_\deltat$ if
$Q$ approximates $Q^\pi_\deltat$% \footnote{
% 	Ensuring that $A$ does not differ from $A^\pi_\deltat$ by a state dependent function only affects
% 	$A$'s interpretability. Indeed, for any function $f$, the ranking of actions given by
% 	$A^\pi_\deltat(s, \cdot)$ and $A^\pi_\deltat(s, \cdot) + f(s)$ is
% 	the same. \TODO{still unclear why we'd care about identifiability}
%}
.
Indeed, for any given pair $(V,\,A)$, the pair $(V(s) -
f(s),\,A(s,a)+f(s)/\deltat)$ (for an arbitrary $f$)
% \begin{align}
% 	\tilde{V}(s) &= V(s) - f(s),\qquad
% 	\tilde{A}(s, a) = A(s, a) + \frac{f(s)}{\deltat}
% \end{align}
yields the exact same $Q$ function. This new $A$ still defines %the same $Q$ and
the same ranking of actions, yet this phenomenon might cause
numerical problems or instability of $A$ when $\deltat\to 0$, and prevents direct
interpretation of the learned $A$.
% Consequently, $Q$ can correctly approximate
% $Q^\pi$ even if $A$ is arbitrarily far from $A^\pi$.
To enforce identifiability of $A$, one must enforce the consistency equation
\begin{equation}
	Q^\pi_\deltat(s, \pi(s)) - V^\pi_\deltat(s) = 0
\end{equation}
on the approximate $A$ and $V$. This translates to
\begin{equation}
	A(s, \pi(s)) = 0.
\end{equation}
With this additional constraint, if $Q = Q^\pi_\deltat$, then $A =
A^\pi_\deltat$ and $V = V^\pi_\deltat$: indeed \TODO{keep or  leave as
an exercise?}
\begin{align}
	A^\pi_\deltat(s, a) &= \frac{Q^\pi_\deltat(s,a) - V^\pi_\deltat(s)}{\deltat}\\
		    &= \frac{Q(s,a) - Q(s, \pi(s))}{\deltat}
		    %\\ &
		    = A(s, a).
\end{align}
In the spirit of~\cite{dueling_nets}, this condition is enforced by
parameterizing $A$ through another function $\bar{A}$ and setting
\begin{equation}
\label{eq:Aparam}
	A(s, a) \deq \bar{A}(s, a) - \bar{A}(s, \pi(s)).
\end{equation}

This approach will lead to $\deltat$-robust algorithms for
approximating $A^\pi_\deltat$, from which a ranking of actions can be
derived.

\begin{theorem}
Under suitable smoothness assumptions, $A^\pi_\deltat(s,a)$ has a limit
$A^\pi(s,a)$ when $\deltat\to 0$. The limit is informative about
actions: namely, if a policy $\pi'$ strictly dominates $\pi$, 
%satisfies $V^{\pi'}>V^{\pi}$ for all states,
then
$A^\pi(s,\pi'(s))>0$ for some state $s$.
\end{theorem}

\subsection{Timestep-Invariant Exploration}
\label{subsec:explo}

To obtain a timestep-invariant RL algorithm, a timestep-invariant
exploration scheme is required. 
For continuous actions, \cite{ddpg} already introduced a time discretization invariant exploration scheme, which consists in adding an
\emph{Ornstein--Uhlenbeck} \cite{orn-uhl} (OU) process to actions. Formally, it is defined as
\begin{equation}
	\pi^\text{explore}(s^k_\deltat, z^k_\deltat) \deq \pi(s^k_\deltat) + z^k_\deltat
\end{equation}
with $z^k_\deltat$ the discretization of a continuous-time OU process,
\begin{equation}
	dz_t = - z_t \,\kappa\, dt + \sigma \,\dbt.
	\label{eq:orn_uhl}
\end{equation}
where $B_t$ is a brownian motion, $\kappa$ a stiffness parameter and
$\sigma$ a noise scaling parameter. The discretized trajectories converge
to non trivial continuous time trajectories, exhibiting a Brownian
behavior with a recall force towards $0$.

This exploration can be extended to schemes of the form
\begin{equation}
  \label{eq:explore}
	a^k_\deltat = \pi^\text{explore}_\deltat(s^k_\deltat, z^k_\deltat)
\end{equation}
with $(z^k_\deltat)_{k\geq 0}$ a sequence of random variables independent from the $a$'s and $s$'s.
A sufficient condition for this policy to admit a continuous-time
limit is for the sequence
$z_\deltat$ to converge in law to a
well-defined continuous stochastic process $z$ as $\deltat$ goes to $0$.
% For instance, $\varepsilon$-greedy exploration naturally falls in this framework, \TODO{[[CT would like to give these details, LB thinks it is not necessary:]] with each $z^k_\deltat$
% being a pair composed of a bernoulli random variables $b^k_\deltat$ with probability of being $1$ equal to $\varepsilon$,
% and of a uniform categorical random variable $\xi^k_\deltat$ on actions. Exploratory actions are then selected as
% \begin{equation}
% 	a^k_\deltat = (1 - b^k_\deltat) \text{argmax}_{a'} Q^\pi_\deltat(s, a') + b^k_\deltat \xi^k_\deltat.
% \end{equation}}

Thus, for discrete actions we can obtain a temporally consistent
exploration scheme by taking
$z_\deltat$ to be a discretization of an $|\mathcal{A}|$-dimensional
continuous OU process, and setting
\begin{equation}
  \pi^\text{explore}(s_t, z_t)\deq \text{argmax}_{a'} \left(A(s, a') + z_t[a']\right)
\end{equation}
where $z_t[a']$ denotes the $a'$-th component of $z_t$. Namely, we
perturb advantage values by a random process before selecting an action. The resulting scheme
converges in continuous time to a nontrivial exploration scheme.

On the other hand,
$\varepsilon$-greedy
exploration is likely \emph{not} to explore, i.e. to collapse to a deterministic
policy, when $\deltat$ goes to $0$.
% Indeed, in a
% near-continuous deterministic environment, %the exploration policy resulting from
% applying $\varepsilon$-greedy exploration to a given exploitation policy
% converges to a determinist policy as $\deltat$ goes to $0$.
Intuitively,
with very small $\deltat$, changing the action at random every $\deltat$ time
step just averages out the randomness due to the law of large numbers.
More precisely:%, let
% $(a_\deltat)$ be a sequence of i.i.d.\ actions drawn from a distribution $p_A$, (for instance
% the sequence of actions generated by $\varepsilon$-greedy exploration on a
% fixed policy) in a near-continuous environment. Then, as $\deltat$ goes
% to $0$, the trajectories generated by
% this policy tend to solutions of the deterministic equation
% \begin{equation}
% 	ds_t/dt  = \E_{a\sim p_A}\left[F(s_t, a)\right]
% \end{equation}
% where the expectation cancels the exploration noise. The proof is given in the
% supplementary material. \TODO{Is there a stronger theorm? Should we talk more about that?}


\begin{theorem}
Consider a near-continuous MDP in which an agent selects an
action according to an $\eps$-greedy policy that mixes a deterministic
exploitation policy $\pi$ with an action taken from a noise policy
$\pi^\text{noise}(a|s)$ with probability $\eps$ at each step. Then the
agent's trajectories converge when $\deltat\to 0$ to the solutions of the
\emph{deterministic} equation
\begin{equation}
d s_t/dt= (1-\eps) F(s_t,\pi(s_t))+\eps \E_{a\sim
\pi^\text{noise}(a|s)}F(s_t,a)
\end{equation}
\end{theorem}

\subsection{Algorithms for Deep Advantage Updating}
\label{subsec:algorithm}
\begin{algorithm}[ht]
  \caption{Deep Advantage Updating (Discrete actions)}
  \label{alg:dau}
	\input{dauc.tex}
\end{algorithm}

We learn $V_{\theta}$ and $A_{\psi}$ via suitable variants of $Q$-learning for continuous and
discrete action spaces. Namely, 
the true $A^\pi_\deltat$
and $V^\pi_\deltat$ of a near-continuous MDP with greedy exploitation
policy $\pi(s) \deq
\text{argmax}_{a'}A^\pi_\deltat(s, a')$
are the unique solution to the Bellman and consistency equations
\begin{align}
	V^{\pi}_\deltat(s) + \deltat\, A^{\pi}_\deltat(s, a) &=
	r\,\deltat + \gamma^{\deltat}  \,\E_{s'} V^{\pi}_\deltat(s')\label{eq:bellman_A}\\
	A^{\pi}_\deltat(s, \pi(s)) &= 0\label{eq:max_A}.
\end{align}
as seen in \ref{subsec:reparam}. Then $V_{\theta}$
and $A_{\psi}$ are trained to approximately solve these equations.

Maximization over actions in $\pi$ is implemented exactly for discrete actions,
and approximated by a policy neural network $\pi_\phi(s)$, trained to maximize $A(s,
\pi_\phi(s))$ for continuous actions. This point is similar to
\cite{ddpg}.

Eq.~\eqref{eq:max_A}
is directly verified by $A_{\psi}$, owing to the reparametrization
$A_\psi(s, a) = \bar{A}_\psi(s, a) - \bar{A}_\psi(s, \pi(s))$, described
in~\ref{subsec:reparam}.  To approximately verify~\eqref{eq:bellman_A}, the
corresponding squared residual is minimized by an approximate gradient descent.
The update equations when learning from a transition $(s, a, r, s')$, either from
an exploratory trajectory or from a replay buffer~\cite{dqn}, are
\begin{align}
	\delta Q_\deltat &\leftarrow A(s, a)\, \deltat - \left(r
	\,\deltat + \gamma^{\deltat}\, V(s') - V(s)\right)
	\label{eq:approx_deltaQ}\\
	\theta_\deltat &\leftarrow \theta_\deltat + \eta^V_\deltat
	\,\partial_{\theta} V(s) \,\frac{\delta Q_\deltat}{\deltat}
	\label{eq:approx_bellman_V}\\
	\psi_\deltat &\leftarrow \psi_\deltat + \eta^A_\deltat
	\,\partial_{\psi} A(s, a) \,\frac{\delta Q_\deltat}{\deltat}.
	\label{eq:approx_bellman_A}
\end{align} 
where the $\eta$'s are learning rates.
Appropriate scalings for the learning rates $\eta^V_\deltat$ and
$\eta^A_\deltat$ in term of $\deltat$ to obtain a well defined continuous
limit are derived next.
% As we will see in the following section,
% to obtain a well defined continuous time limit, the learning rates can be rewritten as
% $\eta^i_\deltat = \alpha^i \deltat$, where $\alpha^1$ and $\alpha^2$ are independant
% of $\deltat$.

\subsection{Scaling the Learning Rates}
\label{subsec:lr}
\input{pendulum_fig.tex}
\TODO{
For the algorithm to admit a continuous-time limit, the discrete-time trajectories
of parameters must converge to well defined trajectories as $\deltat$ goes to
$0$.
% The optimization steps should not be too large,
% in which case parameter trajectories could jump in a single gradient step,
% or too small, in which case the trajectories would become stationary as
% $\deltat$ goes to $0$.
This in turn imposes conditions on the scalings of the
learning rates of algorithms robust to changes of the time discretization.
}

Informally, in the parameter updates
\eqref{eq:approx_deltaQ}--\eqref{eq:approx_bellman_A}, the quantity $\delta
Q_\deltat$ is of order $O(\deltat)$, because $s'=s+O(\deltat)$ in a
near-continuous system. Therefore, $\delta
Q_\deltat/\deltat$ is $O(1)$, so that the gradients used to
update $\theta$ and $\psi$ are $O(1)$. Therefore, if the
learning rates are of order $\deltat$, one would expect 
the parameters $\theta$ and $\psi$ to move by $O(\deltat)$ in
each time interval $\deltat$, thus hopefully converging to smooth
continuous-time trajectories. The next theorem formally confirms that
learning rates of order $\deltat$ are the only possibility.

% The updates of $\theta$ and $\psi$ are described in
% \eqref{eq:approx_bellman_A}.  To have parameter updates that do not explode or shrink to $0$ too
% fast as $\deltat \rightarrow 0$, both $\delta \theta$ and
% $\delta \psi$ should verify stochastic differential equations in the
% limit, and notably,
% have a deterministic component of order $\deltat$. As $\deltat$ goes to $0$,
% a Taylor expansion yields (with $\deltas\deq s'-s$)
% \begin{align}
% 	\delta Q_\deltat &= A(s, a)\,\deltat - r\,\deltat - \log(\gamma)
% 	\,V(s)\,\deltat - \partial_s V(s) \,\deltas\nonumber \\&- \frac{(\deltas)^T \partial_{s}^2 V(s) \deltas}{2} + \smallo\left(\deltat\right)\\
% 	\delta \theta_\deltat &= \eta^V_\deltat \partial_{\theta} V(s)
% 	\frac{\delta Q_\deltat}{\deltat} \\%+ \smallo\left(\eta^V_\deltat\right)\\
% 	\delta \psi_\deltat &= \eta^A_\deltat \partial_{\psi} A(s)
% 	\frac{\delta Q_\deltat}{\deltat} %+ \smallo\left(\eta^A_\deltat\right).
% \end{align}

% The next theorem confirms this analysis: with
% $\eta^V_\deltat = \alpha^V \deltat$ and $\eta^A_\deltat = \alpha^A
% \deltat$, the parameter updates follow a Euler discretization scheme
% for a continuous-time equation and have a well-defined limit behavior. On
% the other hand, with smaller learning rates the parameters do not move,
% and can explode instantaneously with larger learning rates.\TODO{keep? or
% just say "the next theorem confirms this"?}

\begin{theorem}
	\label{th:cont-params}
Let $(s_t,a_t)$ be some exploration trajectory in a near-continuous MDP. Set the learning rates to $\eta^V_\deltat =
\alpha^V \deltat^\beta$ and $\eta^A = \alpha^A \deltat^\beta$ for some
$\beta\geq 0$, and learn the parameters $\theta$ and $\psi$ by iterating
\eqref{eq:approx_deltaQ}--\eqref{eq:approx_bellman_A} along the
trajectory $(s_t,a_t)$. Then, when
$\deltat\to 0$:
% 	\TODO{Assume a continuous action space. Let $(s_t, a_t)_{t\geq 0}$ be a
% 	time continuous exploratory trajectory. Let $\pi$ be a sufficiently
% regular policy, and $\bar{A}$ and $\bar{V}$ be regular function approximators.}
% 	Let $\eta^V_\deltat = \alpha^V \deltat^\beta$ (resp. $\eta^A = \alpha^A \deltat^\beta$) be the learning rate
% 	of $V$ (resp. $\bar{A}$). Then, when learning on $(s_t, a_t)_{t\geq 0}$, 
	\begin{itemize}
		\item If $\beta = 1$ the discrete parameter trajectories converge to continuous parameter
			trajectories.% as $\deltat$ goes to $0$.
		\item If $\beta>1$ the parameters stay at their initial
		values.
		%trajectories become
		%stationary.% as $\deltat$ goes to $0$.
		\item If $\beta < 1$, the parameters can reach infinity
		in arbitrarily small physical time.%	parameters grow arbitrarily large in after an arbitrarily small physical time when $\deltat$ goes to $0$.
	\end{itemize}
\end{theorem}

% With this choice of learning rates, the limit SDEs are
% \begin{align}
% 	F &= A(s_t, a_t) - r_t - \log(\gamma) V(s_t) - \partial_s V(s_t) f(s_t, a_t)\nonumber\\
% 	G &= - \partial_s V(s_t, a_t) \Sigma(s_t, a_t)\nonumber\\
% 	d\theta_t &= (\alpha^V Fdt  + D \dbt)\partial_{\theta} V(s_t)\nonumber\\
% 	d\psi_t &= (\alpha^A Fdt  + D \dbt)\partial_{\psi} A(s_t, a_t)\nonumber.
% \end{align}

The resulting algorithm with suitable scalings,
\emph{Deep Advantage Updating} (DAU), is fully specified in Alg.~\ref{alg:dau} for
discrete actions, and in the supplementary material for continuous
actions.

%%% Local Variables:
%%% TeX-master: "icml_drau"
%%% End:

%! TEX root = icml_drau.tex
\section{Reinforcement learning with a continuous time limit}
As seen in Sec.~\ref{sec:framework}, there is no continuous time limit to
Q-learning. In continuous time, $Q^\pi$ is independant of actions and can thus
not be used to select promising actions.  In near continuous time, $Q^\pi_\deltat$
still depends on actions, and can still be used to choose promising actions
under the current policy. However, when approximating $Q^\pi_\deltat$, if the
approximation error is much bigger than $\bigO(\deltat)$, approximation errors
dominates, and the hierarchy of actions given by the approximation is likely to
be erroneous.  When the approximate Q-function is initialized, if the effect of
actions on the Q-function is order of magnitudes higher than what it should be,
approximating $Q^\pi_\deltat$ is likely to be difficult. Besides, the error is likely
to be further propagated when the equation used to update our approximate $Q$
function relies on bootstraping, as is the case for usual temporal difference
derived methods.

In what follows, a discrete algorithm that has a well defined time continuous
limit is defined. The algorithm relies on three elements, namely, defining and
learning a quantity that contains some information on the hierarchy of actions
while admitting a time continuous limit, defining exploration methods that
admit a continuous time limit and defining learning rates that induce well
behaved parameter trajectories when $\deltat$ goes to $0$.

\subsection{Advantage Updating}
\label{subsec:reparam}
To define an object which contains the same information on actions as
$Q^\pi_\deltat$, but admits an action dependant continuous time limit, , it is
natural to define
\begin{align}
	A^\pi_\deltat(s, a) &= \frac{Q^\pi_\deltat(s,a) - V^\pi_\deltat(s)}{\deltat},
    \label{eq:adv}
\end{align}
a rescaled version of the advantage function, as the difference between between
$Q^\pi_\deltat(s, a)$ and $V^\pi\deltat(s)$ is of order $\bigO(\deltat)$.
Contrary to the action state value function, the rescaled advantage function converges
to an action dependent quantity when $\deltat$ goes to $0$
\begin{align}
	A^\pi_\deltat(s, a)=&\reward(s, a)+\frac{\gamma^{\deltat}\E_{\delta s \sim \delta s(s, a)}V^\pi_\deltat(s + \delta s)-V^\pi\deltat(s)}{\deltat}\nonumber\\
	=&\reward(s, a)+(\gamma - 1) V^\pi(s)+\partial_s V^\pi(s)f(s, a)\nonumber\\
         &+\frac{\text{tr}\left(\Sigma^T(s, a)\partial^2_{s^2}V^\pi(s) \Sigma(s,a)\right)}{2} + O(\deltat)\nonumber.
\end{align}
The discretized state action value function can then be rewritten as
\begin{equation}
	Q^\pi_\deltat(s, a) = V^\pi_\deltat(s) + \deltat A^\pi_\deltat(s, a).
	\label{eq:reparam_q_pi}
\end{equation}
A natural way of approximating $V^\pi_\delta$ and $A^\pi_\deltat$ is to apply
\emph{Sarsa} or Q-learning to a reparameterized Q-function approximator
\begin{equation}
	Q_\theta(s, a) = V_{\theta_1}(s) + \deltat A_{\theta_2}(s, a).
\end{equation}
To avoid cumbersome notations, in what follows, parameter indices are dropped.
At initialization, if both $V_{\theta_1}$ and $A_{\theta_2}$ are initialized
independantly of $\deltat$, this parameterization reasonably scales the
contribution of actions relative to states in the value of $Q$.  

However, the proposed reparameterization does not, on its own, guarantee that
when $Q$ correctly approximates $Q^\pi_\deltat$, $A$ approximates $A^\pi_\deltat$.
Indeed, for any given pair $V$, $A$,
\begin{align}
	\tilde{V}(s) &= V(s) - f(s)\\
	\tilde{A}(s, a) &= A(s, a) + \frac{f(s)}{\deltat}
\end{align}
yields the exact same $Q$ function. Consequently, $Q$ can correctly approximate
$Q^\pi$ even if $A$ is arbitrarily far from $A^\pi$.
To enforce, identifiability of $A$, one must enforce the consistency equation
\begin{equation}
	Q^\pi_\deltat(s, \pi(s)) - V^\pi_\deltat(s) = 0
\end{equation}
on the approximate $A$ and $V$. This translates to
\begin{equation}
	A(s, \pi(s)) = 0.
\end{equation}
With this additional constraint, if $Q^\pi_\deltat = Q$,
\begin{align}
	A^\pi_\deltat(s, a) &= \frac{Q^\pi_\deltat(s,a) - V^\pi_\deltat(s)}{\deltat}\\
		    &= \frac{Q(s,a) - Q(s, \pi(s))}{\deltat}\\
		    &= A(s, a).
\end{align}
In the spirit of~\cite{dueling_nets}, this condition is enforced by writing $A$ as
\begin{equation}
	A(s, a) = \bar{A}(s, a) - \bar{A}(s, \pi(s)).
\end{equation}

\subsection{Time invariant exploration}
\label{subsec:explo}
To obtain a time invariant reinforcement learning algorithm, a time invariant
exploration scheme is required. 

The exploration scheme used~\cite{ddpg}, which consists in adding an
\emph{Ornstein-Uhlenbeck}~\cite{orn-uhl} process to actions, is naturally time
invariant in the sense that it is first defined as a time continuous stochastic
process, and that discretization of this process are used in practice. Indeed,
a continuous Ornstein-Uhlenbeck process is defined as
\begin{equation}
	dx_t = - (x_t - \mu) \theta dt + \sigma \dbt
	\label{eq:orn_uhl}
\end{equation}
and is naturally discretized by turning $d$'s into $\delta$'s, and using
the same $\deltat$ for both the learning algorithm and the exploration scheme.
Consequently, this exploration scheme can readily be used in continuous action
environments.

On the other hand, the usual exploration scheme used in discrete actions
Q-learning, $\varepsilon$-greedy exploration~\cite{sutton} is not time
invariant, in the sense that resulting state trajectories do not converge to a
well defined stochastic process as $\deltat \rightarrow 0$. As $\dt$ goes to $0$,
for any $\varepsilon$ independant of $\dt$, the behavior of state trajectories get
closer to that of a white noise.

To obtain a temporally coherent exploration scheme in discrete action environment
when using Q-learning we instead directly apply a discretization of a time continuous
stochastic process to the output of the approximate Q-function, and select actions
by maximizing the resulting perturbed Q-function. Typically, the approximate Q-function
values are also perturbed using a discretized Ornstein-Uhlenbeck noise.

\subsection{Algorithms}
\label{subsec:algorithm}
Two variants of \emph{Deep Advantage Updating} are derived to cover both the continuous
and discrete action cases. They both rely on variants of Q-learning. More precisely,
we learn $V$ and $A$ using approximate version of the following bellman equation
\begin{align}
	V^{\pi}(s) + A^{\pi}(s, a) &= \E_{\delta r \sim \delta r(s, a)}\delta r\nonumber\\ 
				   &+ \gamma^{\deltat}  \E_{\delta s \sim \delta s(s, a)} V^{\pi}(s + \delta s)\label{eq:bellman_A}\\
	A^{\pi}(s, \pi(s)) &= 0\label{eq:max_A}
\end{align}
for a greedy exploitation policy $\pi(s) = \text{argmax}_{a'}A^\pi(s, a')$.
Eq.~\eqref{eq:max_A} is directly verified owing to the parameterization of $A$.
To approximately verify~\eqref{eq:bellman_A}, the corresponding squared residual
is minimized by an approximate gradient descent.
The corresponding update equations when learning from incoming transitions of
an exploratory trajectory are
\begin{align}
	\delta Q_t &= A(s_t, a_t) - \frac{\delta r_t + \gamma^{\deltat} V(s_{t+\deltat}) - V(s_t)}{\deltat}\\
	\theta^1_{t+\deltat} &= \theta^1_t + \eta_1 \partial_{\theta^1} V(s_t) \delta Q_t\\
	\theta^2_{t+\deltat} &= \theta^2_t + \eta_2 \partial_{\theta^2} A(s_t, a_t) \delta Q_t.
	\label{eq:approx_bellman_A}
\end{align}
where $\eta$'s are learning rates. We are now left to derive appropriate learning rates
to provide invariance to time discretization.
\begin{algorithm}[ht]
	\caption{Deep Advantage Updating (Discrete actions)}
	\input{dauc.tex}
\end{algorithm}

\subsection{Learning rates scalings}
\label{subsec:lr}
A necessary condition to guarantee invariance to time discretization is that
the trajectory of the parameters of our function approximators should remain
well behaved when the discretization parameter $\deltat$ decreases. This notably
means that optimization steps should not be too large, in which case parameter
trajectories could diverge in a single gradient step, or two small, in which case
the trajectories would converge to single points as $\deltat$ goes to $0$.
This in turn imposes conditions on the scalings of the learning rates of invariant
algorithms with respect to the discretization timestep.

If we optimize $\theta_1$ and $\theta_2$ according to Eq.~\eqref{eq:approx_bellman_A},
both $\theta_1$ and $\theta_2$ verify update equations of the form
\begin{align}
	\
	\delta \theta^1_t = \eta_1 \partial_{\theta_1} V(s_t) (
\end{align}
To have parameter steps that do not grow larger or shrink to $0$ as $\deltat
\leftarrow 0$, both $\delta \theta^1$ and $\delta \theta^2$ should verify
stochastic differential equations, and notably have a deterministic component
of order $\deltat$. If the deterministic component 

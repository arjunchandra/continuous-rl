%! TEX root = icml_drau.tex
\section{Reinforcement learning with a continuous time limit}
As seen in Sec.~\ref{sec:framework}, there is no continuous time limit to
Q-learning. In continuous time, $Q^\pi$ is independant of actions and can thus
not be used to select promising actions.  In near continuous time, $Q^\pi_\deltat$
still depends on actions, and can still be used to choose promising actions
under the current policy. However, when approximating $Q^\pi_\deltat$, if the
approximation error is much bigger than $\bigO(\deltat)$, approximation errors
dominates, and the hierarchy of actions given by the approximation is likely to
be erroneous.  When the approximate Q-function is initialized, if the effect of
actions on the Q-function is order of magnitudes higher than what it should be,
approximating $Q^\pi_\deltat$ is likely to be difficult. Besides, the error is likely
to be further propagated when the equation used to update our approximate $Q$
function relies on bootstraping, as is the case for usual temporal difference
derived methods.

In what follows, a discrete algorithm that has a well defined time continuous
limit is defined. The algorithm relies on three elements, namely, defining and
learning a quantity that contains some information on the hierarchy of actions
while admitting a time continuous limit, defining exploration methods that
admit a continuous time limit and defining learning rates that induce well
behaved parameter trajectories when $\deltat$ goes to $0$.

\subsection{Advantage Updating}
\label{subsec:reparam}
To define an object which contains the same information on actions as
$Q^\pi_\deltat$, but admits an action dependant continuous time limit, , it is
natural to define
\begin{align}
	A^\pi_\deltat(s, a) &= \frac{Q^\pi_\deltat(s,a) - V^\pi_\deltat(s)}{\deltat},
    \label{eq:adv}
\end{align}
a rescaled version of the advantage function, as the difference between between
$Q^\pi_\deltat(s, a)$ and $V^\pi\deltat(s)$ is of order $\bigO(\deltat)$.
Contrary to the action state value function, the rescaled advantage function converges
to an action dependent quantity when $\deltat$ goes to $0$
\begin{align}
	A^\pi_\deltat(s, a)=&\reward(s, a)+\frac{\gamma^{\deltat}\E_{\delta s \sim \delta s(s, a)}V^\pi_\deltat(s + \delta s)-V^\pi\deltat(s)}{\deltat}\nonumber\\
	=&\reward(s, a)+(\gamma - 1) V^\pi(s)+\partial_s V^\pi(s)f(s, a)\nonumber\\
         &+\frac{\text{tr}\left(\Sigma^T(s, a)\partial^2_{s^2}V^\pi(s) \Sigma(s,a)\right)}{2} + O(\deltat)\nonumber.
\end{align}
The discretized state action value function can then be rewritten as
\begin{equation}
	Q^\pi_\deltat(s, a) = V^\pi_\deltat(s) + \deltat A^\pi_\deltat(s, a).
	\label{eq:reparam_q_pi}
\end{equation}
A natural way of approximating $V^\pi_\delta$ and $A^\pi_\deltat$ is to apply
\emph{Sarsa} or Q-learning to a reparameterized Q-function approximator
\begin{equation}
	Q_\theta(s, a) = V_{\theta_1}(s) + \deltat A_{\theta_2}(s, a).
\end{equation}
To avoid cumbersome notations, in what follows, parameter indices are dropped.
At initialization, if both $V_{\theta_1}$ and $A_{\theta_2}$ are initialized
independantly of $\deltat$, this parameterization reasonably scales the
contribution of actions relative to states in the value of $Q$.  

However, the proposed reparameterization does not, on its own, guarantee that
when $Q$ correctly approximates $Q^\pi_\deltat$, $A$ approximates $A^\pi_\deltat$.
Indeed, for any given pair $V$, $A$,
\begin{align}
	\tilde{V}(s) &= V(s) - f(s)\\
	\tilde{A}(s, a) &= A(s, a) + \frac{f(s)}{\deltat}
\end{align}
yields the exact same $Q$ function. Consequently, $Q$ can correctly approximate
$Q^\pi$ even if $A$ is arbitrarily far from $A^\pi$.
To enforce, identifiability of $A$, one must enforce the consistency equation
\begin{equation}
	Q^\pi_\deltat(s, \pi(s)) - V^\pi_\deltat(s) = 0
\end{equation}
on the approximate $A$ and $V$. This translates to
\begin{equation}
	A(s, \pi(s)) = 0.
\end{equation}
With this additional constraint, if $Q^\pi_\deltat = Q$,
\begin{align}
	A^\pi_\deltat(s, a) &= \frac{Q^\pi_\deltat(s,a) - V^\pi_\deltat(s)}{\deltat}\\
		    &= \frac{Q(s,a) - Q(s, \pi(s))}{\deltat}\\
		    &= A(s, a).
\end{align}
In the spirit of~\cite{dueling_nets}, this condition is enforced by writing $A$ as
\begin{equation}
	A(s, a) = \bar{A}(s, a) - \bar{A}(s, \pi(s)).
\end{equation}
With this parameterization, one can hope to learn an approximation of $A^\pi_\deltat$,
from which a proper hiearchy of actions can be derived for any $\deltat$.

\subsection{Time invariant exploration}
\label{subsec:explo}
To obtain a time invariant reinforcement learning algorithm, a time invariant
exploration scheme is required. All the exploration schemes hereby considered
generate sequences of actions as
\begin{equation}
	a^k_\deltat = \pi^\text{explore}_\deltat(s^k_\deltat, \nu^k_\deltat)
\end{equation}
with $(\nu^k_\deltat)_k$ a sequence of random variables, independant from the $a$'s and $s$'s.
For instance, $\varepsilon$-greedy exploration naturally falls in this framework, with each $\nu^k_\deltat$
being a pair composed of a bernoulli random variables $b^k_\deltat$ with probability of being $1$ equal to $\varepsilon$,
and of a uniform categorical random variable $\xi^k_\deltat$ on actions. Exploratory actions are then selected as
\begin{equation}
	a^k_\deltat = (1 - b^k_\deltat) \text{argmax}_{a'} Q^\pi_\deltat(s, a') + b^k_\deltat \xi^k_\deltat.
\end{equation}

A sufficient condition for the exploratory policy to admit a time continuous
limit is for $\pi^\text{explore}_\deltat$ to be a measurable function, and for
$\nu_\deltat$ to admit one, i.e. that the sequence $\nu_\deltat$ converges to a
well defined continuous stochastic process $\nu$ as $\deltat$ goes to $0$.
\TODO{I'm convinced that this is true, reviewers might not be.}

The exploration scheme used~\cite{ddpg}, which consists in adding an
\emph{Ornstein-Uhlenbeck}~\cite{orn-uhl} process (OU process) to actions, is naturally time.
invariant. It can be rewritten as
\begin{equation}
	a^k_\deltat = \pi(s^k_\deltat) + \nu^k_\deltat
\end{equation}
where $\nu^k_\deltat$ is the discretization of a time continuous \emph{Ornstein-Uhlenbeck}
process, defined as
\begin{equation}
	d\nu_t = - \nu_t \theta dt + \sigma \dbt.
	\label{eq:orn_uhl}
\end{equation}

On the other hand, the same argument cannot be applied to $\varepsilon$-greedy
exploration, as the $\nu_\deltat$'s used to define it do not converge to a 
well defined stochastic process, but to a white noise.\footnote{\TODO{$\varepsilon$-greedy
		does not, in general, converges to a well defined exploration scheme in continuous time.
		For instance, if actions directly affects position, $\varepsilon$-greedy results in white-noise
trajectories in state space. Show this in more detail in appendix ?}}

To obtain a temporally coherent exploration scheme in discrete action environment
$\nu_\deltat$ is chosen to be a discretization of a $|\mathcal{A}|$ dimensional
continuous OU process, and set
\begin{equation}
	a^k_\deltat = \text{argmax}_{a'} \left(Q^\pi_\deltat(s, a') + \nu_\deltat^k[a']\right)
\end{equation}
where $\nu_\deltat^k[a']$ denotes the $a'$-th component of the discretized OU process at time
$k$. The resulting exploration scheme converges to a proper time continuous exploration scheme.

\subsection{Algorithms}
\label{subsec:algorithm}
To learn $V_{\theta_1}$ and $A_{\theta_2}$, variants of Q-learning for continuous~\cite{ddpg} and
discrete~\cite{dqn} action spaces are used. In details, $V_{\theta_1}$ and $A_{\theta_2}$ are learnt
using approximate version of the following bellman equation
\begin{align}
	V^{\pi}(s) + A^{\pi}(s, a) &= \E_{\delta r \sim \delta r(s, a)}\delta r\nonumber\\ 
				   &+ \gamma^{\deltat}  \E_{\delta s \sim \delta s(s, a)} V^{\pi}(s + \delta s)\label{eq:bellman_A}\\
	A^{\pi}(s, \pi(s)) &= 0\label{eq:max_A}
\end{align}
for a greedy exploitation policy $\pi(s) = \text{argmax}_{a'}A^\pi(s, a')$,
exactly implemented in the case of discrete actions, and approximated by a
neural network $\pi_\phi(s)$, trained to maximize (an approximation of)
$A^\pi(s, \pi_\phi(s))$ in the continuous case.  Eq.~\eqref{eq:max_A} is
directly verified by $A_{\theta_1}$, owing to the reparameterization described
in~\ref{subsec:reparam}.  To approximately verify~\eqref{eq:bellman_A}, the
corresponding squared residual is minimized by an approximate gradient descent.
The corresponding update equations when learning from incoming transitions of
an exploratory trajectory are
\begin{align}
	\delta Q_t &= A(s_t, a_t) - \frac{\delta r_t + \gamma^{\deltat} V(s_{t}+\delta s_{t}) - V(s_t)}{\deltat}\\
	\theta^1_{t+\deltat} &= \theta^1_t + \eta_1 \partial_{\theta^1} V(s_t) \delta Q_t\\
	\theta^2_{t+\deltat} &= \theta^2_t + \eta_2 \partial_{\theta^2} A(s_t, a_t) \delta Q_t.
	\label{eq:approx_bellman_A}
\end{align}
where $\eta$'s are learning rates. In practice, $A$ and $V$ are learned using
transitions sampled from a replay buffer instead of trajectory samples, to
prevent excessive correlation between training samples~\cite{dqn}. Pseudocode
for the discrete actions algorithm is given in Alg.~\ref{alg:dau}. Pseudocode
for the continuous actions algorithm is given in the supplementary material.
\begin{algorithm}[ht]
	\caption{Deep Advantage Updating (Discrete actions)}
	\input{dauc.tex}
	\label{alg:dau}
\end{algorithm}

\subsection{Learning rates scalings}
\label{subsec:lr}
A necessary condition to guarantee invariance to time discretization is that
the trajectory of the parameters of our function approximators should remain
well behaved when the discretization parameter $\deltat$ decreases. This notably
means that optimization steps should not be too large, in which case parameter
trajectories could diverge in a single gradient step, or two small, in which case
the trajectories would converge to single points as $\deltat$ goes to $0$.
This in turn imposes conditions on the scalings of the learning rates of invariant
algorithms with respect to the discretization timestep.

If we optimize $\theta_1$ and $\theta_2$ according to Eq.~\eqref{eq:approx_bellman_A},
both $\theta_1$ and $\theta_2$ verify update equations of the form
\begin{align}
	\
	\delta \theta^1_t = \eta_1 \partial_{\theta_1} V(s_t) (
\end{align}
To have parameter steps that do not grow larger or shrink to $0$ as $\deltat
\leftarrow 0$, both $\delta \theta^1$ and $\delta \theta^2$ should verify
stochastic differential equations, and notably have a deterministic component
of order $\deltat$. If the deterministic component 

%! TEX root = icml_drau.tex
\section{Reinforcement Learning with a Continuous-Time Limit}

We now define a discrete algorithm with a well-defined continuous-time
limit.  It relies on three elements: defining and learning a quantity
that still contains information on action rankings in the limit, using
exploration methods with a meaningful limit, and scaling learning rates
to induce well-behaved parameter trajectories when $\deltat$ goes to $0$.

% When the approximate $Q$-function is initialized, if the effect of
% actions on the $Q$-function is order of magnitudes higher than what it should be,
% approximating $Q^\pi_\deltat$ is likely to be difficult.
%Besides, the error is likely
%to be further propagated when the equation used to update our approximate $Q$
%function relies on bootstraping, as is the case for usual temporal difference
%derived methods.

\subsection{Advantage Updating}
\label{subsec:reparam}

As seen above, there is no continuous time limit to $Q$-learning, because
$Q^\pi$ becomes independent of actions and thus cannot be
used to select actions.  In near-continuous time,
$Q^\pi_\deltat$ still depends on actions, and could still be used to
choose actions. However, when
approximating $Q^\pi_\deltat$, if the approximation error is much larger
than $\bigO(\deltat)$, this error dominates, the ranking of
actions given by the approximated $Q^\pi_\deltat$ is likely to be erroneous.

To define an object which contains the same information on actions as
$Q^\pi_\deltat$, but admits a learnable action-dependent limit, it is
natural to define \cite{adv_upd}
\begin{align}
	A^\pi_\deltat(s, a) &\mathdef \frac{Q^\pi_\deltat(s,a) - V^\pi_\deltat(s)}{\deltat},
    \label{eq:adv}
\end{align}
a rescaled version of the advantage function, as the difference between between
$Q^\pi_\deltat(s, a)$ and $V^\pi_\deltat(s)$ is of order
$\bigO(\deltat)$. This amounts to splitting $Q$ into value and advantage,
and observing that these scale very differently when $\deltat\to 0$.

Contrary to the action state value function,
this
rescaled advantage function converges when $\deltat\to 0$
to an action-dependent quantity which, if it can be learned, contains the
necessary information for policy improvement \TODO{See Thm... in the appendix; we
should put the theorem in the appendix, because otherwise the theorem is
just a paraphrase of the previous sentence}.


% \LB{In next equation, tiny detail but I would write $\frac{1}{\deltat}(r\deltat+...)$, easier to read.}
% \begin{align}
% 	A^\pi_\deltat(s, a)=&\reward(s, a)+\frac{\gamma^{\deltat}\E_{\delta s \sim \delta s(s, a)}V^\pi_\deltat(s + \delta s)-V^\pi_\deltat(s)}{\deltat}\nonumber\\
% 	=&\reward(s, a)+\log(\gamma) V^\pi(s)+\partial_s V^\pi(s)f(s, a)\nonumber\\
%          &+\frac{\text{tr}\left(\Sigma^T(s, a)\partial^2_{s^2}V^\pi(s) \Sigma(s,a)\right)}{2} + O(\deltat)\nonumber.
% \end{align}

%   \begin{theorem}
%     There exists $A^\pi(s, a)$ such that for all $a, s$:
%     \begin{equation}
%       A^\pi_\deltat(s, a) \rightarrow_{\deltat \rightarrow 0} A^\pi(s, a)
%     \end{equation}
%     Moreover, $A^\pi$ contains all the necessary information for policy improvement. The policy $\pi$ optimal if and only if $A^\pi(s, \pi(s)) = \max_{a'}A^\pi(s, a')$ for all $s \in {\cal S}$.
%     \TODO{Is this true? What is true? Is this real life? We should probably add a reasonable hypothesis, for example $\pi_\deltat^* \rightarrow \pi^*$}
%   \end{theorem}

The discretized $Q$-function rewrites as
\begin{equation}
	Q^\pi_\deltat(s, a) = V^\pi_\deltat(s) + \deltat A^\pi_\deltat(s, a).
	\label{eq:reparam_q_pi}
\end{equation}
A natural way to approximate $V^\pi_\deltat$ and $A^\pi_\deltat$ is to apply
\emph{Sarsa} or $Q$-learning to a reparameterized $Q$-function approximator
\begin{equation}
	Q_\Theta(s, a) \deq V_{\theta}(s) + \deltat A_{\psi}(s, a).
\end{equation}
with $\Theta \deq (\theta, \psi)$. (To avoid cumbersome notation, 
parameter indices are dropped in what follows.)  At initialization, if both $V_{\theta}$ and
$A_{\psi}$ are initialized independently of $\deltat$, this parameterization
provides reasonable scaling of the contribution of actions versus states
in $Q$.
Our goal is for $V_\theta$ to approximate $V^\pi_\deltat$ and for
$A_{\psi}$ to approximate $A^\pi_\deltat$.


%\TODO{CT and LB are fighting each other about the importance which should be given to the next paragraph. A impartial judge should give his opinion. (CT is wrong)}
Still, this reparameterization does not, on its own, guarantee that $A$
correctly approximates $A^\pi_\deltat$ if
$Q$ approximates $Q^\pi_\deltat$% \footnote{
% 	Ensuring that $A$ does not differ from $A^\pi_\deltat$ by a state dependent function only affects
% 	$A$'s interpretability. Indeed, for any function $f$, the ranking of actions given by
% 	$A^\pi_\deltat(s, \cdot)$ and $A^\pi_\deltat(s, \cdot) + f(s)$ is
% 	the same. \TODO{still unclear why we'd care about identifiability}
%}
.
Indeed, for any given pair $(V,\,A)$, the pair $(V(s) -
f(s),\,A(s,a)+f(s)/\deltat)$ (for an arbitrary $f$)
% \begin{align}
% 	\tilde{V}(s) &= V(s) - f(s),\qquad
% 	\tilde{A}(s, a) = A(s, a) + \frac{f(s)}{\deltat}
% \end{align}
yields the exact same $Q$ function. This new $A$ still defines %the same $Q$ and
the same ranking of actions, yet this phenomenon might cause
numerical problems or instability of $A$ when $\deltat\to 0$, and prevents direct
interpretation of the learned $A$.
% Consequently, $Q$ can correctly approximate
% $Q^\pi$ even if $A$ is arbitrarily far from $A^\pi$.
To enforce identifiability of $A$, one must enforce the consistency equation
\begin{equation}
	Q^\pi_\deltat(s, \pi(s)) - V^\pi_\deltat(s) = 0
\end{equation}
on the approximate $A$ and $V$. This translates to
\begin{equation}
	A(s, \pi(s)) = 0.
\end{equation}
With this additional constraint, if $Q = Q^\pi_\deltat$, then $A =
A^\pi_\deltat$ and $V = V^\pi_\deltat$: indeed \TODO{keep or  leave as
an exercise?}
\begin{align}
	A^\pi_\deltat(s, a) &= \frac{Q^\pi_\deltat(s,a) - V^\pi_\deltat(s)}{\deltat}\\
		    &= \frac{Q(s,a) - Q(s, \pi(s))}{\deltat}\\
		    &= A(s, a).
\end{align}
In the spirit of~\cite{dueling_nets}, this condition is enforced by writing $A$ as
\begin{equation}
\label{eq:Aparam}
	A(s, a) \deq \bar{A}(s, a) - \bar{A}(s, \pi(s)).
\end{equation}
With this parameterization, one can hope to learn an approximation of $A^\pi$,
from which a proper ranking of actions can be derived for any $\deltat$.

\subsection{Timestep-Invariant Exploration}
\label{subsec:explo}

To obtain a timestep-invariant RL algorithm, a timestep-invariant
exploration scheme is required. 
For continuous actions, \cite{ddpg} already introduced a time discretization invariant exploration scheme, which consists in adding an
\emph{Ornstein--Uhlenbeck} \cite{orn-uhl} (OU) process to actions. Formally, it is defined as
\begin{equation}
	\pi^\text{explore}(s^k_\deltat, z^k_\deltat) \deq \pi(s^k_\deltat) + z^k_\deltat
\end{equation}
with $z^k_\deltat$ the discretization of a continuous-time OU process,
\begin{equation}
	dz_t = - z_t \,\kappa\, dt + \sigma \,\dbt.
	\label{eq:orn_uhl}
\end{equation}
where $B_t$ is a brownian motion, $\kappa$ a stiffness parameter and
$\sigma$ a noise scaling parameter. The discretized trajectories converge
to non trivial continuous time trajectories, exhibiting a Brownian
behavior with a recall force towards $0$.

This exploration can be extended to schemes of the form
\begin{equation}
  \label{eq:explore}
	a^k_\deltat = \pi^\text{explore}_\deltat(s^k_\deltat, z^k_\deltat)
\end{equation}
with $(z^k_\deltat)_{k\geq 0}$ a sequence of random variables independent from the $a$'s and $s$'s.
A sufficient condition for this policy to admit a continuous-time
limit is for the sequence
$z_\deltat$ to converge in law to a
well-defined continuous stochastic process $z$ as $\deltat$ goes to $0$.
% For instance, $\varepsilon$-greedy exploration naturally falls in this framework, \TODO{[[CT would like to give these details, LB thinks it is not necessary:]] with each $z^k_\deltat$
% being a pair composed of a bernoulli random variables $b^k_\deltat$ with probability of being $1$ equal to $\varepsilon$,
% and of a uniform categorical random variable $\xi^k_\deltat$ on actions. Exploratory actions are then selected as
% \begin{equation}
% 	a^k_\deltat = (1 - b^k_\deltat) \text{argmax}_{a'} Q^\pi_\deltat(s, a') + b^k_\deltat \xi^k_\deltat.
% \end{equation}}

Thus, for discrete actions we can obtain a temporally consistent
exploration scheme by taking
$z_\deltat$ to be a discretization of an $|\mathcal{A}|$-dimensional
continuous OU process, and setting
\begin{equation}
  \pi^\text{explore}(s_t, z_t)\deq \text{argmax}_{a'} \left(A(s, a') + z_t[a']\right)
\end{equation}
where $z_t[a']$ denotes the $a'$-th component of $z_t$. Namely, we
perturb advantage values by a random process before selecting an action. The resulting scheme
converges in continuous time to a nontrivial exploration scheme.

On the other hand,
$\varepsilon$-greedy
exploration is likely \emph{not} to explore, i.e. to collapse to a deterministic
policy, when $\deltat$ goes to $0$.
% Indeed, in a
% near-continuous deterministic environment, %the exploration policy resulting from
% applying $\varepsilon$-greedy exploration to a given exploitation policy
% converges to a determinist policy as $\deltat$ goes to $0$.
Intuitively,
with very small $\deltat$, changing the action at random every $\deltat$ time
step just averages out the randomness due to the law of large numbers.
More precisely, let
$(a_\deltat)$ be a sequence of i.i.d.\ actions drawn from a distribution $p_A$, (for instance
the sequence of actions generated by $\varepsilon$-greedy exploration on a
fixed policy) in a near-continuous environment. Then, as $\deltat$ goes
to $0$, the trajectories generated by
this policy tend to solutions of the deterministic equation
\begin{equation}
	ds_t/dt  = \E_{a\sim p_A}\left[F(s_t, a)\right]
\end{equation}
where the expectation cancels the exploration noise. The proof is given in the
supplementary material. \TODO{Is there a stronger theorm? Should we talk more about that?}

\subsection{Algorithms for Deep Advantage Updating}
\label{subsec:algorithm}
\begin{algorithm}[ht]
  \caption{Deep Advantage Updating (Discrete actions)}
  \label{alg:dau}
	\input{dauc.tex}
\end{algorithm}

$V_{\theta}$ and $A_{\psi}$ are learned by a variants of $Q$-learning for continuous and
discrete action spaces are used. Namely, 
the true $A^\pi_\deltat$
and $V^\pi_\deltat$ of a near-continuous MDP with greedy exploitation
policy $\pi(s) \deq
\text{argmax}_{a'}A^\pi_\deltat(s, a')$
are the unique solution to the Bellman and identifiability equations
\begin{align}
	V^{\pi}_\deltat(s) + \deltat\, A^{\pi}_\deltat(s, a) &=
	r\,\deltat + \gamma^{\deltat}  \,\E_{s'} V^{\pi}_\deltat(s')\label{eq:bellman_A}\\
	A^{\pi}_\deltat(s, \pi(s)) &= 0\label{eq:max_A}.
\end{align}
as seen in \ref{subsec:reparam}. Then $V_{\theta}$
and $A_{\psi}$ are trained to approximately solve this equation.

Maximization over actions in $\pi$ is implemented exactly for discrete actions,
and approximated by a policy neural network $\pi_\phi(s)$, trained to maximize $A(s,
\pi_\phi(s))$ for continuous actions. This point is similar to
\cite{ddpg}.

Eq.~\eqref{eq:max_A}
is directly verified by $A_{\psi}$, owing to the reparametrization
$A_\psi(s, a) = \bar{A}_\psi(s, a) - \bar{A}_\psi(s, \pi(s))$, described
in~\ref{subsec:reparam}.  To approximately verify~\eqref{eq:bellman_A}, the
corresponding squared residual is minimized by an approximate gradient descent.
The update equations when learning from a transition $(s, a, r, s')$, either from
an exploratory trajectory or from a replay buffer~\cite{dqn}, are
\begin{align}
	\delta Q_\deltat &\leftarrow A(s, a)\, \deltat - \left(r
	\,\deltat + \gamma^{\deltat}\, V(s') - V(s)\right)\\
	\theta_\deltat &\leftarrow \theta_\deltat + \eta^V_\deltat
	\,\partial_{\theta} V(s) \,\frac{\delta Q_\deltat}{\deltat}\\
	\psi_\deltat &\leftarrow \psi_\deltat + \eta^A_\deltat
	\,\partial_{\psi} A(s, a) \,\frac{\delta Q_\deltat}{\deltat}.
	\label{eq:approx_bellman_A}
\end{align} 
where the $\eta$'s are learning rates.
Appropriate scalings for the learning rates $\eta^V_\deltat$ and
$\eta^A_\deltat$ in term of $\deltat$ to obtain a well defined continuous
limit are derived next.
% As we will see in the following section,
% to obtain a well defined continuous time limit, the learning rates can be rewritten as
% $\eta^i_\deltat = \alpha^i \deltat$, where $\alpha^1$ and $\alpha^2$ are independant
% of $\deltat$.

\TODO{say somewhere around here that we assume $\deltat$ is known}

\subsection{Scaling the Learning Rates}
\label{subsec:lr}
\input{pendulum_fig.tex}
For the discrete algorithm to admit a continuous time limit, the trajectories
of parameters must converge to well defined trajectories as $\deltat$ goes to
$0$.  Intuitively, this means that optimization steps should not be too large,
in which case parameter trajectories could diverge in a single gradient step,
or too small, in which case the trajectories would converge to single points as
$\deltat$ goes to $0$.  This in turn imposes conditions on the scalings of the
learning rates of algorithms robust to changes of the tine discretization.

The updates equations on $\theta$ and $\psi$ are described in
Eq.~\eqref{eq:approx_bellman_A}.  To have parameter steps that do not explode or shrink to $0$ too
fast as $\deltat \rightarrow 0$, both $\delta \theta$ and
$\delta \psi$ should verify stochastic differential equations, and notably
have a deterministic component of order $\deltat$. As $\deltat$ goes to $0$,
updates are rephrased as
\begin{align}
	\delta Q_\deltat &= A(s, a)\deltat - r\deltat - \log(\gamma) V(s)\deltat - \partial_s V(s) \delta s\nonumber \\&- \frac{(\delta s)^T \partial_{s^2}^2 V(s) \delta s}{2} + \smallo\left(\deltat\right)\\
	\delta \theta_\deltat &= \eta^V_\deltat \partial_{\theta} V(s) \frac{\delta Q_\deltat}{\deltat} + \smallo\left(\eta^V_\deltat\right)\\
	\delta \psi_\deltat &= \eta^A_\deltat \partial_{\psi} A(s) \frac{\delta Q_\deltat}{\deltat} + \smallo\left(\eta^A_\deltat\right).
\end{align}


For parameter updates to follow a SDE, one must
have $\eta^V_\deltat = \alpha^V \deltat$ and $\eta^A_\deltat = \alpha^A
\deltat$. More precisely
\begin{theorem}
	\TODO{We need a bit more details: we probably want to assume that states and actions are
	drawn from a single fixed trajectory, and that we are learning for a fixed, constant policy.}
	Let $\eta^V_\deltat = \alpha^V \deltat^\beta$ (resp. $\eta^A \deltat^\beta$) be the learning rate
	of $V$ (resp. $A$), when training on a single exploratory trajectory. Then
	\begin{itemize}
		\item If $\beta = 1$ the discrete parameter trajectories converge to continuous parameter
			trajectories as $\deltat$ goes to $0$.
		\item If $\beta < 1$, parameters grow arbitrarily large in a single timestep as $\deltat$
			goes to $0$.
		\item If $\beta > 1$, parameter trajectories shrink to a single point as 
			$\deltat$ goes to $0$. \TODO{Is that clear}
	\end{itemize}
	\label{th:cont-params}
\end{theorem}

% With this choice of learning rates, the limit SDEs are
% \begin{align}
% 	F &= A(s_t, a_t) - r_t - \log(\gamma) V(s_t) - \partial_s V(s_t) f(s_t, a_t)\nonumber\\
% 	G &= - \partial_s V(s_t, a_t) \Sigma(s_t, a_t)\nonumber\\
% 	d\theta_t &= (\alpha^V Fdt  + D \dbt)\partial_{\theta} V(s_t)\nonumber\\
% 	d\psi_t &= (\alpha^A Fdt  + D \dbt)\partial_{\psi} A(s_t, a_t)\nonumber.
% \end{align}
The full algorithm, that is referred to as \emph{Deep Advantage Updating} (DAU), can now be derived.
Pseudocode for the discrete actions algorithm is given in Alg.~\ref{alg:dau}.
Pseudocode for the continuous actions algorithm is given in the supplementary
material.


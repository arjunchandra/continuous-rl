%! TEX root = icml_drau.tex
\section{Near continuous time reinforcement learning}
\label{sec:continous}
% In this section, a continuous reinforcement learning framework is introduced,
% and it is shown that under this framework, the state-action value function does
% not depend on the action. We further show that in near continous domains, when
% the discretization timestep $\deltat$ is small, the difference in state-action
% value between two different actions in a fixed state is of order $\bigO(\deltat)$.
\TODO{. Main ideas: near continuous time environments (control, mujoco, video games, ...)
There is a time discretization $\deltat$ given by the problem. The uderlying process does not depend of $\deltat$. Need to see what is happening when changing $\deltat$. The problem is invariant to $\deltat$. The quantities we are looking at should be invariant to. The learning procedure should be invariant. Looking at the limits when $\deltat \rightarrow 0$. In simulated environments, changing $\deltat$ only means to change the FPS. In real environments, for example with robots, this could mean changing the quality of the sensors and motors.}

\subsection{Framework}

Let assume that we have a Markov Decision Process \TODO{Cite?} $\langle {\cal S}, {\cal A}, T_{\deltat}, r_\deltat, \gamma_\deltat\rangle$. ${\cal S}$ is the state-space, which is supposed to be continuous \TODO{not the good formulation}. The action space is \TODO{todo}.

We assume that the transition is continuous. This means that ... \TODO{todo}. Discussion on the hypothesis, in which environments it is true. Is the condition $\forall \deltat, \forall s, \E[\|s_{k+1} - s_k \||s_k = s] \leq ?$. 

In many situations, we can assume that there is an uderlying process:
\begin{equation}
  \label{eq:diffusion}
  ds = F(s, a) + \Sigma(s, a)\dbt
\end{equation}
$F$ is the physics, and $\dbt$ is the noise.

There is convergence to the continuous process when $\deltat \rightarrow 0$.

What we want is an algorithm invariant to $\deltat$, so we need to build a discrete algorithm such that when $\deltat \rightarrow 0$, the algorithm converge to a limit algorithm doing what we want.

TODO: Explain $a_t = \pi(s_t)$ deterministic.

\subsection{Return, $\gamma$ and Value function in near continuous time}
\label{sec:ret-gamma-v}

For a continuous MDP we define the return as:
\begin{equation}
  \label{eq:continuous-return}
  R(\tau) = \int_{t=0}^\infty\gamma^tr(s_t, a_t)dt
\end{equation}

The natural discretization of this, if we want to define a discretize return $R_\delta$ for each $\deltat$ is:
\begin{equation}
  \label{eq:discretized-return}
  R_\deltat(\tau) = \sum_{k=0}^\infty\gamma^{k\deltat}r(s^\deltat_k, a^\deltat_k)\deltat
\end{equation}

We see that the good \TODO{rewrite} definitions of $\gamma_\deltat$ and $r_\deltat$ are:
\begin{align}
  \gamma_\deltat =& \gamma^\deltat\\
  r_\deltat =& \deltat . r
\end{align}
Thus, we see that the natural discretization of a continuous MDP ${\cal M} = \langle {\cal S}, {\cal A}, T, r, \gamma\rangle$ is ${\cal M}_\deltat = \langle {\cal S}, {\cal A}, T_\deltat, r\deltat, \gamma^\deltat\rangle$.

Thus, we should define $\gamma_\deltat = \gamma^\deltat$, so we have $\lim_{\deltat \rightarrow 0} R_\deltat(\tau)  = R(\tau)$. \TODO{Should we state that more formally?}


This is consistent with what we expect for the time horizon. We know \TODO{Cite?} that for a MDP with discount factor $\gamma$, the time horizon is $\frac{1}{1-\gamma}$ steps. But if the duration of a step is $\deltat$, this means that the time horizon in \emph{physical time} (independant of $\deltat$) is $\frac{\deltat}{1-\gamma}$, which goes to 0 if $\deltat \rightarrow 0$. This means that if $\gamma$ is not adapted to $\deltat$, the return $R_\deltat(\tau) = \sum_{k=0}^\infty\gamma^{k}r(s^\deltat_k, a^\deltat_k)\deltat \rightarrow_{\deltat\rightarrow 0} r(s_0, a_0)$. \TODO{A changer pour ne pas ecrire de chose fausse} But in practice, for example on a control problem, we do not care about the time horizon as a number of steps but as a \emph{physical quantity}, independant of $\deltat$. With $\gamma_\deltat = \gamma^\deltat$, the time horizon in \emph{physical time} is:
\begin{equation}
  \label{eq:time-horizon}
  \frac{\deltat}{1-\gamma^\deltat} \rightarrow_{\deltat \rightarrow 0} - \frac{1}{\log \gamma} \approx \frac{1}{1-\gamma}
\end{equation}
\TODO{Explain again that we have a limit so it is better}



The Value Function for a policy $\pi$ with a discretization $\deltat$ is
\begin{equation}
  V^\pi_\deltat(s) = \E_{\tau\sim\pi}\left[\sum\limits_{k=0}^\infty \gamma^{k\deltat}\deltat r_{k\deltat} \mid s_0 = s\right]
\end{equation}
and we have, for all $\sigma \in {\cal S}$, $\lim_{\deltat \rightarrow 0} V^\pi_\deltat(s) = V^\pi(s)$. \TODO{Theorem?}

It follows the Bellman equation:
\begin{equation}
  \label{eq:bellman}
  V^\pi_\deltat(s) = \reward(s, \pi(s))\deltat + \gamma^{\dt} \E_{s_{t+\deltat} | s_t = s} V^\pi(s_{t+\deltat})
\end{equation}

If the MDP is following the equation (\ref{eq:diffusion}), rhe limit of the Bellman equation for $V^\pi_\deltat$ when $\deltat \rightarrow 0$ is the Hamilton-Jacobi-Bellman equation on $V^\pi$:
\begin{equation}
  \label{eq:hamilton-jacobi-bellman}
  r + \partial_s V^\pi \cdot F + \frac{1}{2} \text{tr}\left(\Sigma^T\partial^2_{s^2} V^\pi\Sigma\right) = (1 - \gamma) V^\pi(s)
\end{equation}

% In what follows, $\dt$ denotes a truly infinitesimal time interval while $\deltat$
% denotes a small, but non infinitesimal time interval. Similarily, $\dbt$ denotes
% an infinitesimal (multi-dimensional) brownian step, while $\deltabt$ denotes its
% discretized equivalent. Informally, $\dbt$ is to be thought of as $\gauss(0, \sqrt{\dt}^2)$,
% and $\deltabt$ as $\gauss(0, \sqrt{\deltat}^2)$, where $\gauss(\mu, \sigma)$ denotes a gaussian
% random variable of mean $\mu$ and standard deviation (or covariance matrix in the multi-dimensional case)
% $\sigma$.

% Consider a fully observable \emph{Markov Decision Process} defined by the following
% transitions and rewards
% \begin{align}
% 	ds(s, a) &= F(s, a) dt + \Sigma(s, a) \dbt\\
% 	dr(s, a) &= \reward(s, a) dt + \Sigma_r(s, a) \dbt.
% \end{align}
% The equation on $s$ is that of a diffusion process, with drift coeficient $F$
% and diffusion matrix $\Sigma$.  This framework is only moderately restrictive.
% For example, deterministic continuous control environments fall in this
% framework, with $\Sigma = 0$ and $\Sigma_r = 0$. Limitations of the framework
% are discussed in Sec.~\ref{sec:limitations}.
% This framework is naturally discretized to arbitrary $\deltat$ by replacing all
% $d$'s with $\delta$'s.

% A continuous policy $\pi$ is defined as a deterministic mapping from states to
% actions.  In continuous time, defining stochastic policies with well defined
% state-value functions requires incorporating information from previous action into
% the state to obtain a temporally coherent policy. To avoid such inconvenience, the
% framework is restricted to deterministic policies. This is a mild constraint, as this
% only enforces determinism of the exploitation policy and not of exploration policies.
% In what follows, $\tau\sim\pi$ denotes a trajectory of states and actions sampled
% according to policy $\pi$, with $s_0$ sampled according to an arbitrary initial distribution
% on states $\rho_0$, and $dr_{t} = dr(s_t, a_t)$.

% Under a deterministic policy $\pi$, one can define the $\gamma$ discounted
% \emph{state value function} $V^\pi(s)$ and \emph{state action value function}
% $Q^\pi(s, a)$ as
% \begin{align}
% 	V^\pi(s) &= \E_{\tau\sim\pi}\left[
% 		\int\limits_{t=0}^\infty \gamma^{t}
% 		dr_{t} \mid s_0 = s
% 	\right]\\
% 	Q^\pi(s, a) &= \E_{\tau\sim\pi}\left[
% 		\int\limits_{t=0}^\infty \gamma^{t}
% 		dr_{t} \mid s_0 = s, a_0 = a
% 	\right]
% \end{align}
% which are naturally discretized as
% \begin{align}
% 	V^\pi(s) &= \E_{\tau\sim\pi}\left[
% 		\sum\limits_{k=0}^\infty \gamma^{k\deltat}
% 		\delta r_{k\deltat} \mid s_0 = s
% 	\right]\\
% 	Q^\pi(s, a) &= \E_{\tau\sim\pi}\left[
% 		\sum\limits_{k=0}^\infty \gamma^{k\deltat}
% 		\delta r_{k\deltat} \mid s_0 = s, a_0 = a
% 	\right].
% \end{align}
% Both $V^\pi$ and $Q^\pi$ verify Bellman equations, informally
% \begin{align}
% 	V^\pi(s) &= \reward(s, \pi(s))\dt + \gamma^{\dt} \E_{ds \sim ds(s, \pi(s))} V^\pi(s + ds)\nonumber\\
% 	Q^\pi(s, a) &= \reward(s, a)\dt + \gamma^{\dt} \E_{ds(s, a)} V^\pi(s + ds(s, a))
% 	\label{eq:cont_bell}
% \end{align}
% which in continuous time translates, using Ito's lemma, into a Hamilton Jacobi equation
% \begin{align}
% 	\reward + \partial_s V^\pi \cdot f + \frac{1}{2} \text{tr}\left(\Sigma^T\partial^2_{s^2} V^\pi\Sigma\right) = (1 - \gamma) V^\pi(s).
% \end{align}

\subsection{There is no Q-function in continuous time}

Contrary to the Value function, the Action-Value function $Q$ is ill-defined for a continuous MDP. Moreover, this means that the discretized $Q$ function \TODO{explain}. This can be stated formally.
\begin{theorem}
  For all $s, a$, we have
  \begin{equation}
    \label{eq:q-discr}
    Q^\pi_\deltat(s, a) = V^\pi(s) + \bigO\left(\sqrt\deltat\right)
  \end{equation}
  
  If we consider the continuous action-value function $Q^\pi$, we have:
  \begin{equation}
    \label{eq:q-cont}
    Q^\pi(s, a) = V^\pi
  \end{equation}
  \label{th:q-cont}
\end{theorem}
% Consider a discretization of Eq.~\eqref{eq:cont_bell} with time interval
% $\deltat \ll~1$, Ito's lemma yield
% \begin{align}
% 	Q^\pi(s, a) &= \reward(s, a) \deltat + \gamma^{\deltat} \E_{\delta s \sim \delta s(s, a)}V^\pi(s + \delta s)\nonumber\\
% 		    &= \reward(s, a) \deltat + \gamma^{\deltat} V^\pi(s) +
% 		     \gamma^{\deltat} \partial_s V^\pi(s)f(s, a) \deltat\nonumber\\
% 		    &+ \frac{1}{2}\text{tr}\left(\Sigma^T\partial^2_{s^2}V^\pi(s)\Sigma\right)\deltat + \bigO(\deltat^2)\nonumber\\
% 		    &\xrightarrow[\deltat \to 0]{} V^\pi(s).
% 		    \label{eq:Q_cont}
                      %   \end{align}

A consequence of Theorem \ref{th:q-cont} is that in exact continuous time,
$Q^\pi$ does not bear any information on the hierarchy of actions, and
thus cannot be used to select actions that yields higher future returns.

\begin{proof}
    \TODO{TODO}
  \end{proof}

%! TEX root = icml_drau.tex
\section{Near continuous time reinforcement learning}
\label{sec:framework}
% In this section, a continuous reinforcement learning framework is introduced,
% and it is shown that under this framework, the state-action value function does
% not depend on the action. We further show that in near continous domains, when
% the discretization timestep $\deltat$ is small, the difference in state-action
% value between two different actions in a fixed state is of order $\bigO(\deltat)$.


Many reinforcement learning environments are discretizations or approximations of continuous environments. We say that these environments are \emph{near continuous}. This includes usual control environment (Mujoco, Gym, ...) \TODO{Cite}. In practice, these environments are considered as discrete, and a time discretization $\deltat$ is fixed by the simulator. In real environments, like in robotics, the time discretization $\deltat$ is fixed by the sensors or motors. Still, in these environments, there is an uderlying continuous process which do not depend on $\deltat$. Therefore, our learning methods should be invariant to $\deltat$, as well as all the meaningfull quantities we are using.

In this paper, we try to analyze \emph{near-continuous} environments when  modifying $\deltat$, especially when $\deltat \rightarrow 0$. In order to design algorithms more robust to the choice of $\deltat$, we want that when $\deltat \rightarrow 0$, the quantities we are using and our algorithm do converge to meaningful limit quantities and algorithms.

\TODO{We should explain that, if not invariant, harder to find good hyperparameters}

In this section, we first introduce our framework. Then we show how we can define the return, discount factor and Value function such that it has a limit when $\deltat \rightarrow 0$. Finally, we show that there is no Action-Value function in continuous MDPs.

\subsection{Framework}

We say that ${\cal M}_\deltat = \langle {\cal S}, {\cal A}, T_{\deltat}, r_\deltat, \gamma_\deltat\rangle$ is a \emph{near-continuous} Markov Decision Process with trajectories $(s_\deltat^k)_{k\in\mathbb{N}}$ if there is a \emph{physical time} $\deltat$ between two observations, and if $s_\deltat^{k+1}$ is close to $s_\deltat^k$. We assume that the state space ${\cal S}$ is continuous \TODO{True? Ok?}. The action space ${\cal A}$ can be either continuous or discrete \TODO{except in the theorems}.

% Many practical Reinforcement Learning are near-continuous: For example, in a simulated physical environment in which the $\deltat$ is fixed by the simulator, or in a real environments, if the agent is a robot.

% , the $\deltat$ is fixed by the sensitivity of the sensors and the reactivity of the motors. We assume that the state space ${\cal S}$ is continuous.

If the \emph{near continuous} MDP is the discretization, or a good approximation, of continuous MDP, Iwe can assume that the agent is actually is following a diffusion process:
\begin{equation}
  \label{eq:diffusion}
  ds = F(s, a)dt + \Sigma(s, a)\dbt
\end{equation}
where $F: {\cal S}\times{\cal A}\rightarrow {\cal S}$ describes the physics of the environment, and $\dbt$ is the noise of the environment. In that case, the transition function is: $T_\deltat(s, a, s')$ is a probability distribution over $s'$, which is the distribution of $s_\deltat$ if $(s_t)_{t\in\mathbb{R_+}}$ is following equation (\ref{eq:diffusion}), $s_0 = s$ and the action is $a$ during the interval $[0, \deltat]$. If the environment is deterministic as in classical control theory, or in many usual simulated environments (Mujoco, Gym, ...), we have $\Sigma = 0$.

In this work we only consider deterministic policies $\pi: {\cal S} \rightarrow {\cal A}$. Therefore, the diffusion equation becomes:
\begin{equation}
  \label{eq:diffusion-policy}
  ds = F(s, \pi(s))dt + \Sigma(s, \pi(s))\dbt
\end{equation}

\TODO{Les deux paragraphes suivants sont redondants avec l'intro de la section}
If we assume that the \emph{near continuous} MDP we are observing is a discretization of a continuous MDP \TODO{for example which could follow a diffusion process as in Equation (\ref{eq:diffusion})}, then we know that the time discretization $\deltat$ is not intrisically related to the problem we are tackling. Therefore, our algorithm should be robust to different choices of $\deltat$. For example, in a simulated environment where the $\deltat$ only depends on the FPS, or in a real environment in which the $\deltat$ is defined by the reactivity of the sensors and motors, we would like our algorithms to be robust if the FPS in changed, or if we use more reactive sensors.

In this paper, we try to design RL methods for \emph{near continuous} MDP robust to the choice of $\deltat$. Our method is to analyse the different quantities we are looking at when $\deltat$ is changing, and especially when $\deltat \rightarrow 0$, and make sure that the quantities we are interested in and our algorithms do converge to meaningful limit quantities and algorithms. \TODO{OK?} The first formal statement we can make on discretized MDP with a time discretization $\deltat$ is that if $\deltat \rightarrow 0$, the distribution of trajectories do converge to the continuous trajectories.

\begin{theorem}
  Assume that $\pi: {\cal S} \rightarrow {\cal A}$ is a continuous and deterministic policy. Let $(s_t)_{t\in\mathbb{R}_+}$ be the continuous process by following \eqref{eq:diffusion} and $(s_\deltat^k)$ be the discrete process obtained if the agent only get observations every $\deltat$ \TODO{explain better}.

  We define the time process $\tilde s_\deltat^t = s^{\lfloor \frac{t}{\deltat}\rfloor}_\deltat$ for $t\in\mathbb{R_+}$.\TODO{Explain}. Then:
  \begin{equation}
    \label{eq:conv-traj}
    (\tilde s_\deltat^t)_{t\in\mathbb{R}_+} \rightarrow_{\deltat \rightarrow 0} (s_t)_{t\in\mathbb{R}_+}
  \end{equation}
  as probability distributions.
\end{theorem}




\subsection{Return, discount factor  and Value function in near continuous time}
\label{sec:ret-gamma-v}

In this section, we show how to define the discount factor $\gamma_\deltat$ as a function of $\deltat$, such that the return and Value function do converge to the desired limit when $\deltat \rightarrow 0$. 

\paragraph{Return and Discount Factor} For a continuous MDP the return is defined as \TODO{Cite}:
\begin{equation}
  \label{eq:continuous-return}
  R(\tau) = \int_{t=0}^\infty\gamma^tr(s_t, a_t)dt
\end{equation}

The natural discretization of this equation, if we want to define a discretize return $R_\delta$ for each $\deltat$ is:
\begin{equation}
  \label{eq:discretized-return}
  R_\deltat(\tau) = \sum_{k=0}^\infty\gamma^{k\deltat}r(s^\deltat_k, a^\deltat_k)\deltat
\end{equation}

We see that the natural definitions of $\gamma_\deltat$ and $r_\deltat$ such that the return defined in equation (\ref{eq:discretized-return}) corresponds to the discrete return for the MDP ${\cal M}_\deltat$ are:
\begin{align}
  \label{eq:def-gamma}
  \gamma_\deltat =& \gamma^\deltat\\
  \label{eq:def-reward}
  r_\deltat =& \deltat . r
\end{align}
% Thus, we see that the natural discretization of a continuous MDP ${\cal M} = \langle {\cal S}, {\cal A}, T, r, \gamma\rangle$ is ${\cal M}_\deltat = \langle {\cal S}, {\cal A}, T_\deltat, r\deltat, \gamma^\deltat\rangle$.

% Thus, we should define $\gamma_\deltat = \gamma^\deltat$, so we have $\lim_{\deltat \rightarrow 0} R_\deltat(\tau)  = R(\tau)$.





\paragraph{Time horizon} The definition of $\gamma_\deltat$ in equation (\ref{eq:def-gamma}) is consistent with what we expect for the time horizon. We know \TODO{Cite?} that for a MDP with discount factor $\gamma$, the time horizon is $\frac{1}{1-\gamma}$ steps. But if the duration of a step is $\deltat$, the time horizon in \emph{physical time} is $\frac{\deltat}{1-\gamma}$, which goes to 0 if $\deltat \rightarrow 0$.
% This means that if $\gamma$ is not adapted to $\deltat$, the return $R_\deltat(\tau) = \sum_{k=0}^\infty\gamma^{k}r(s^\deltat_k, a^\deltat_k)\deltat \rightarrow_{\deltat\rightarrow 0} r(s_0, a_0)$. \TODO{A changer pour ne pas ecrire de chose fausse}
But in practice, for example on a control problem, we do not care about the time horizon as a number of steps but as a \emph{physical} quantity independant of $\deltat$. With $\gamma_\deltat = \gamma^\deltat$, the time horizon in \emph{physical time} is:
\begin{equation}
  \label{eq:time-horizon}
  \frac{\deltat}{1-\gamma^\deltat} \rightarrow_{\deltat \rightarrow 0} - \frac{1}{\log \gamma} \approx \frac{1}{1-\gamma}
\end{equation}


\paragraph{Value function}

With the definition of $\gamma_\deltat$ and $r_\deltat$ in equations \eqref{eq:def-gamma} and \eqref{eq:def-reward}, the Value Function for a policy $\pi$ with a discretization $\deltat$ becomes:
\begin{equation}
  V^\pi_\deltat(s) = \E_{\tau\sim\pi}\left[\sum\limits_{k=0}^\infty \gamma^{k\deltat}\deltat r_{k\deltat} \mid s_0 = s\right]
\end{equation}
We have the following convergence theorem of the discretized value function to the continuous value function:
  \begin{theorem}
    We have, for all $s \in {\cal S}$:
    \begin{equation}
      \label{eq:conv-value}
      \lim_{\deltat \rightarrow 0} V^\pi_\deltat(s) = V^\pi(s)
    \end{equation}
  \end{theorem}

The discretized value function follows the Bellman equation:
\begin{equation}
  \label{eq:bellman}
  V^\pi_\deltat(s) = \reward(s, \pi(s))\deltat + \gamma^{\dt} \E_{s_{t+\deltat} | s_t = s} V^\pi(s_{t+\deltat})
\end{equation}

If the MDP is following the equation (\ref{eq:diffusion}), the limit of the Bellman equation for $V^\pi_\deltat$ when $\deltat \rightarrow 0$ is the Hamilton-Jacobi-Bellman equation on $V^\pi$:
\begin{equation}
  \label{eq:hamilton-jacobi-bellman}
  r + \partial_s V^\pi \cdot F + \frac{1}{2} \text{tr}\left(\Sigma^T\partial^2_{s^2} V^\pi\Sigma\right) = (1 - \gamma) V^\pi(s)
\end{equation}


\subsection{There is no Q-function in continuous time}

Contrary to the Value function, the Action-Value function $Q$ is ill-defined for a continuous MDP. More precisely, for a continuous MDP, $Q^\pi(s, a) = V^\pi(s)$, so the continuous $Q$-function is independant of the actions. For a \emph{near-continuous} MDP, this means that the $Q$ is very close  to the $V$-function.\TODO{Say more ?} Moreover, this means that the discretized $Q$ function \TODO{explain}. This can be stated formally.
\begin{theorem}
  For all $s, a$, we have
  \begin{equation}
    \label{eq:q-discr}
    Q^\pi_\deltat(s, a) = V^\pi(s) + \bigO\left(\deltat\right)
  \end{equation}
  
  If we consider the continuous action-value function $Q^\pi$, we have:
  \begin{equation}
    \label{eq:q-cont}
    Q^\pi(s, a) = V^\pi
  \end{equation}
  \label{th:q-cont}
\end{theorem}
% Consider a discretization of Eq.~\eqref{eq:cont_bell} with time interval
% $\deltat \ll~1$, Ito's lemma yield
% \begin{align}
% 	Q^\pi(s, a) &= \reward(s, a) \deltat + \gamma^{\deltat} \E_{\delta s \sim \delta s(s, a)}V^\pi(s + \delta s)\nonumber\\
% 		    &= \reward(s, a) \deltat + \gamma^{\deltat} V^\pi(s) +
% 		     \gamma^{\deltat} \partial_s V^\pi(s)f(s, a) \deltat\nonumber\\
% 		    &+ \frac{1}{2}\text{tr}\left(\Sigma^T\partial^2_{s^2}V^\pi(s)\Sigma\right)\deltat + \bigO(\deltat^2)\nonumber\\
% 		    &\xrightarrow[\deltat \to 0]{} V^\pi(s).
% 		    \label{eq:Q_cont}
                      %   \end{align}

A consequence of Theorem \ref{th:q-cont} is that in exact continuous time,
$Q^\pi$ does not bear any information on the hierarchy of actions, and
thus cannot be used to select actions that yields higher future returns.

\begin{proof}
  \begin{align}
    Q^\pi(s, a) &= r(s, a)\deltat + \gamma^\deltat \E_{s'|s, a}\left[V(s')\right] \\
  \end{align}
  But we know that $s'$ is close to $s$. If we define $\delta s = s' - s$, then:
  \begin{align}
    \|\E_{s'|s, a}\left[\delta s\right]\| &= \bigO(\deltat) \\
    \E_{s'|s, a}\left[\|\delta s\|^2\right] &= \bigO(\deltat)
  \end{align}
  \TODO{Explain.} Therefore:
  \begin{align}
    |\E_{s'|s, a}\left[V(s')\right] - V(s)| &= \E_{\delta s|s, a}\left[\partial_s V\cdot \delta s +\bigO(\|\delta s\|^2)\right]\\
                                            &\leq \|\partial_s V\|\cdot \|\E_{\delta s|s, a}\left[\delta s\right]|\| +\bigO(\delta t)\\
    &= \bigO(\deltat)
  \end{align}
  Finally:
  \begin{align}
    Q^\pi(s, a) &= r(s, a)\deltat + \gamma^\deltat \E_{s'|s, a}\left[V(s')\right] \\
                &= r(s, a)\deltat + \left(1 + \bigO(\deltat)\right)(V(s) + \bigO(\deltat))\\
    &= V(s) + \bigO(\deltat)
  \end{align}
  \end{proof}

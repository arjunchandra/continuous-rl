%! TEX root = icml_drau.tex
\section{Near continuous-time reinforcement learning}
\label{sec:framework}

Many reinforcement learning environments are not intrinsically
time-discrete, but discretizations of an underlying continuous-time
environment. For instance, many simulated control environments, such as
the Mujoco environments~\cite{ddpg} or OpenAI Gym classic control
environments~\cite{gym}, are discretization of continuous-time control
problems.  In simulated environments, the time discretization is fixed by
the simulator, and is often used to approximate an underlying
differential equation.  For example, the timestep may correspond to the
number of frames generated by second.  In real world environments,
sensors and actuators have a fixed time precision: cameras can only
capture a fixed amount of frames per second, and physical limitations
prevent actuators from responding instantaneously. The quality of these
components thus imposes a lower bound on the discretization timestep. As
the timestep $\deltat$ is not an intrinsic parameter of the
environment, but a constraint imposed by computational ressources, we
would expect that decreasing $\deltat$ could only improve the performance
of reinforcement learning agents (though it might make optimization
harder).  Reinforcement
learning algorithms should, at least, be resilient to a change of
$\deltat$, and should remain viable when $\deltat \rightarrow 0$.
Besides, designing a time discretization invariant algorithm could
alleviate tedious hyperparameterization by providing better defaults for
time-horizon-related parameters.
\TODO{Move some of this to the intro??}

We are thus interested in the behavior of reinforcement learning
algorithms in discretized environments, when the discretization timestep
is small. We will refer to such environments as \emph{near-continuous
environments}.

In what follows, a formalized view of near continuous environments is
given, along with $\deltat$ dependant definitions of return, discount
factor and value functions that are shown to converge to well defined
continous time limits. Finally, it is shown that the state-action value
function collapses to the value function as $\deltat$ goes to $0$.
Consequently there is no Q-learning in continuous time, foreshadowing
problematic behaviors of near continuous time Q-learning.
%      \TODO{We should explain somwhere here that, if not invariant,
%      harder to find good hyperparameters} DONE

\subsection{Framework}

% \VC{
%   Consider ${\cal S} = \mathbb{R}^d$ a set of states, and ${\cal A}$ a set of
% 	actions. A familly of \emph{Markov decision processes} ${\cal
% 	M}_\deltat = \langle {\cal S}, {\cal A}, T_{\deltat}, r_\deltat,
% 	\gamma_\deltat\rangle$ is \emph{near continous} if for any
% 	deterministic policy $\pi:{\cal S} \rightarrow {\cal A}$, trajectories
% 	of ${\cal M}_\deltat$ converge to stochastic processes that satisfy the
% 	Stochastic differential equation
% 	\begin{equation}
% 		\label{eq:diffusion}
% 		ds = F(s, a)dt + \Sigma(s, a)\dbt.
% 	\end{equation}
% 	Informally $F: {\cal S}\times{\cal A}\rightarrow {\cal S}$ describes
% 	the deterministic dynamic of the environment, while $\Sigma(s, a)\dbt$
% 	describes its stochasticity. Notably, $\Sigma = 0$ if the environment is
% 	deterministic, as in classical control theory, or in many usual simulated
% 	environments (Mujoco, Gym, ...).

% 	In what follows, ${\cal M}_\deltat$ is taken to be a direct discretization of
% 	Eq.~\ref{eq:diffusion} with discretization timestep $\deltat$. The
% 	corresponding transition function $T_\deltat(., s, a)$ is the distribution of
% 	$s'$ when starting from point $s$, and following Eq.~\ref{eq:diffusion} while
% 	maintaining $a$ constant on an interval of time $\deltat$.

	
%       }

      
% TODO: For now, differential equation is the default, SDE is a footnote.
        Let ${\cal S} = \mathbb{R}^d$ be a set of states, and ${\cal A}$ be a set of
	actions. Consider the continuous \emph{Markov Decision Process} (MDP) defined by the \emph{Differential Equation}: %\TODO{Explain the intuition of Brownian in SDE}:
        \begin{equation}
		\label{eq:diffusion}
		ds = F(s, a)dt. % + \Sigma(s, a)\dbt.
              \end{equation}
        For any $\deltat$, the discrete MDP ${\cal
	M}_\deltat = \langle {\cal S}, {\cal A}, T_{\deltat}, r_\deltat,
      \gamma_\deltat\rangle$ is a discretization of the continuous MDP with
      \emph{time discretization} $\deltat$. The transition probability
      $T_\deltat(s, a, s')$ is a dirac on $s_\deltat$, where $s_\deltat$ is the
      state obtained
      when starting at $s_0 = s$ and maintaining $a_t=a$ constant for a time
      $\deltat$.
      This corresponds to an agent evolving in the continuous
      environment described by Eq. \eqref{eq:diffusion}, but who is
      only making observations and choosing actions every $\deltat$. We
      say the the MDP ${\cal M}_\deltat$ is near-continuous.\footnote{
	      We believe the results presented here to be true in a more general framework, where states
	      follow a \emph{Stochastic Differential Equation} $ds = F(s, a)dt.  + \Sigma(s, a)\dbt$, with
	      $B_t$ a multidimensional brownian motion and $\Sigma$ its associated covariance matrix. However,
      formal treatment of SDEs is generally tedious and out of the scope of this paper.}
      Informally $F: {\cal S}\times{\cal A}\rightarrow {\cal S}$ describes
	the dynamic of the environment. The agent interacts with the environment through a deterministic policy function
	$\pi \colon {\cal S} \rightarrow {\cal A}$. Determinism of the policy is assumed to prevent problematic definitions
	of the value function in continuous time. This is only a mild requirement, as we introduce an off-policy algorithm,
	and only impose determinism of the exploitation policy.
	% This framework covers classical control theory, as well as many usual simulated environments. TODO: not convinced that this is relevant here.

%      \footnote{Informally, $T_\deltat(s, a, s')$ is approximately
%	      the probability distribution of $s + F(s, a)\deltat +
%	      \sqrt{\deltat}\Sigma(s, a){\cal N}(0, I)$, which is the
%	      discretization of Eq. \eqref{eq:diffusion} \TODO{CITE}}.
        In this paper, we try to design RL methods for near continuous MDP
	robust to the choice of $\deltat$. For robustness, a necessary condition is to remain viable when $\deltat \rightarrow 0$. Our method is thus to analyse the different
	quantities involved when $\deltat \rightarrow 0$, and make sure that they converge to meaningful limits.

        The first step in that direction is to check that the discretized
          trajectories converge to the continuous ones. 
	\begin{theorem}
		Assume that $\pi: {\cal S} \rightarrow {\cal A}$ is a continuous and
		deterministic policy. Let $(s_t)_{t\in\mathbb{R}_+}$ be the
		continuous process obtained by following \eqref{eq:diffusion} and
		$(s_\deltat^k)$ be the discrete process obtained if the agent only
		get observations and interacts with the environment every $\deltat$ \TODO{explain better}.

		We define the time process $\tilde s_\deltat^t = s^{\lfloor
		\frac{t}{\deltat}\rfloor}_\deltat$ for
		$t\in\mathbb{R_+}$.\TODO{Explain}. Then:
		\begin{equation}
			\label{eq:conv-traj}
			(\tilde s_\deltat^t)_{t\in\mathbb{R}_+} 
			\xrightarrow[\deltat \rightarrow 0]{} (s_t)_{t\in\mathbb{R}_+}.
		\end{equation}
	\end{theorem}
      

\subsection{Return, discount factor and value function in near continuous time}
\label{sec:ret-gamma-v}



	In what follows, $\deltat$-dependant scalings of the discount factor $\gamma_\deltat$
	and the reward $r_\deltat$ are given. These definitions fit the discrete case
	scenario when $\deltat=1$, and yields well defined, non trivial returns
	and value functions when $\deltat$ goes to $0$.

\paragraph{Return and Discount Factor}
For a continuous MDP and a continuous trajectory $\tau = (s_t, a_t)_t$,
the return is defined as~\cite{cont_rl}
\begin{equation}
	\label{eq:continuous-return}
	R(\tau) = \int_{t=0}^\infty\gamma^tr(s_t, a_t)dt.
\end{equation}

The natural discretization of this equation, if we want to define a discretized
return $R_\delta$ for each $\deltat$ is:
\begin{equation}
	\label{eq:discretized-return}
	R_\deltat(\tau) = \sum_{k=0}^\infty\gamma^{k\deltat}
	r(s_\deltat^k, a_\deltat^k)\deltat
\end{equation}

Natural definitions of $\gamma_\deltat$ and $r_\deltat$ such
that the return defined in equation (\ref{eq:discretized-return}) corresponds
to the discrete return for the MDP ${\cal M}_\deltat$ follow:
\begin{align}
	\label{eq:def-gamma}
	\gamma_\deltat =& \gamma^\deltat\\
	\label{eq:def-reward}
	r_\deltat =& \deltat . r.
\end{align}
% Thus, we see that the natural discretization of a continuous MDP ${\cal M} = \langle {\cal S}, {\cal A}, T, r, \gamma\rangle$ is ${\cal M}_\deltat = \langle {\cal S}, {\cal A}, T_\deltat, r\deltat, \gamma^\deltat\rangle$.

% Thus, we should define $\gamma_\deltat = \gamma^\deltat$, so we have $\lim_{\deltat \rightarrow 0} R_\deltat(\tau)  = R(\tau)$.

% \paragraph{Microscopic/Macroscopic time, Time horizon}
% \VC{In near continuous environments, there are two notions of time,
% 	the number of observation received, or the number of actions taken,
% 	which we call the microscopic time, and the actual amount of time spent
% 	in the underlying continuous time environment, which we call the macroscopic
% 	time. The two are related through
% 	\begin{equation}
% 		\tau^\text{macro} = \tau^\text{micro} \deltat.
% 	\end{equation}

% 	The notion of time horizon, which is, informally, the amount of time on
% 	which the agent optimizes its return, is only meaningful in macroscopic
% 	time. Having a time horizon of $10$ steps means something very different
% 	if $\deltat = 0.1$ and if $\deltat = 0.0001$. A rule of thumb
% 	is that the time horizon of an agent with discount factor $\gamma$ is
% 	of order $\frac{1}{1 - \gamma}$. If $\gamma_\deltat$ was left constant
% 	as $\deltat$ goes to $0$, the corresponding macroscopic time horizon 
% 	would write as $\frac{\deltat}{1 - \gamma}$ which goes to $0$ when
% 	$\deltat$ goes to $0$.

% 	On the contrary, the definition of $\gamma_\deltat$ in
% 	Eq.~\eqref{eq:def-gamma} yields an approximately constant macroscopic
% 	time horizon when $\deltat$ goes to $0$. Indeed,
% 	\begin{equation}
% 		\label{eq:time-horizon}
% 		\frac{\deltat}{1-\gamma^\deltat}
% 		= - \frac{1}{\log \gamma} + \bigO(\deltat)
% 		\approx \frac{1}{1-\gamma}
% 	\end{equation}
%       }

      \paragraph{Number of steps / Physical time, Time horizon}
        % [[I prefer to talk about ``number of steps'' VS ``Physical time'' than microscopic/Macroscopic]]
        In near continuous environments, there are two notions of time,
	the number of steps, or the number of actions taken, and the actual amount of time spent
	in the underlying continuous time environment, which we call the physical
	time. If $t$ is the real time and $k$ the number of steps, the two are related through $t = k \deltat$.

	The notion of time horizon, which is, informally, the amount of time on
	which the agent optimizes its return, is only meaningful in physical
	time.
        %Having a time horizon of $10$ steps means something very different
	%if $\deltat = 0.1$ and if $\deltat = 0.0001$.
        A rule of thumb
	is that the time horizon of an agent with discount factor $\gamma$ is
	of order $\frac{1}{1 - \gamma}$ steps. If $\gamma_\deltat$ was left constant
	as $\deltat$ goes to $0$, the corresponding time horizon in physical time
	would write as $\frac{\deltat}{1 - \gamma}$ which goes to $0$ when
	$\deltat$ goes to $0$.
        On the contrary, the definition of $\gamma_\deltat$ in
	Eq.~\eqref{eq:def-gamma} yields an approximately constant
	time horizon in physical time when $\deltat$ goes to $0$. Indeed,
	\begin{equation}
          \label{eq:time-horizon}
          \frac{\deltat}{1-\gamma^\deltat}
          = - \frac{1}{\log \gamma} + \bigO(\deltat)
          \approx \frac{1}{1-\gamma},
        \end{equation}
        approximately invariant to $\deltat$ when $\deltat$ goes to 0.
      
        


\paragraph{Value function}


The definition of the return Eq.~\eqref{eq:continuous-return}, yields the
following definition of the continuous value function
\begin{align}
  V^\pi(s) =& \E_{\tau\sim\pi}\left[R(\tau) \mid s_0 = s\right]\\
  =& \E_{\tau\sim\pi}\left[\int\limits_{0}^\infty \gamma^{t} r(s_t, a_t) dt \mid s_0 = s\right].
\end{align}
The corresponding discretization for time discretization $\deltat$ is
\begin{align}
  V^\pi_\deltat(s) =& \E_{\tau\sim\pi}\left[R_\deltat(\tau) \mid s_0 = s\right]\\
  =& \E_{\tau\sim\pi}\left[\sum\limits_{k=0}^\infty \gamma^{k\deltat}r(s^k_{\deltat}, a^k_{\deltat})\deltat \mid s_0 = s\right]
\end{align}
which fits the usual definition of the discrete V-function for $\gamma_\deltat$ and
$r_\deltat$ defined in~(\ref{eq:def-gamma}, \ref{eq:def-reward}).
The discretized value function follows the Bellman equation\footnote{
	If the continuous MDP dynamic follows~\eqref{eq:diffusion}, the limit of the Bellman equation for $V^\pi_\deltat$ when $\deltat \rightarrow 0$ is the Hamilton-Jacobi-Bellman equation on $V^\pi$ \cite{cont_rl}
\begin{equation}
  \label{eq:hamilton-jacobi-bellman}
  r + \partial_s V^\pi \cdot F = - \log(\gamma) V^\pi
\end{equation}}

\begin{equation}
  \label{eq:bellman}
  V^\pi_\deltat(s) = \reward(s, \pi(s))\deltat + \gamma^{\deltat} \E_{s_{\deltat}^{k+1} | s_{\deltat}^{k} = s} V^\pi_\deltat(s_{\deltat}^{k+1})
\end{equation}

We have the following convergence theorem of the discretized value function to the continuous value function:
  \begin{theorem}
   For all $s \in {\cal S}$:
    \begin{equation}
      \label{eq:conv-value}
      V^\pi_\deltat(s) = V^\pi(s) + \bigO(\deltat)
    \end{equation}
    \label{th:conv-value}
  \end{theorem}


\subsection{There is no Q-function in continuous time}

Contrary to the Value function, the Action-Value function $Q$ is ill-defined
for a continuous MDP. More precisely, the Q-function  collapses to the
V-function when $\deltat \rightarrow 0$. In near continuous time, the effect of
individual actions on the Q-function is of order $\bigO(\deltat)$. This makes
deriving a ranking of action from an approximate Q-function difficult.
Formally:
\begin{theorem}
	Let $\pi$ be a deterministic policy such that for all $\deltat$, $V^\pi_\deltat$ is continuously
	differentiable, then, for all $s, a$,
	\begin{equation}
		\label{eq:q-discr}
		Q^\pi_\deltat(s, a) = V^\pi(s) + \bigO\left(\deltat\right)
	\end{equation}

  \label{th:q-cont}
\end{theorem}
% Consider a discretization of Eq.~\eqref{eq:cont_bell} with time interval
% $\deltat \ll~1$, Ito's lemma yield
% \begin{align}
% 	Q^\pi(s, a) &= \reward(s, a) \deltat + \gamma^{\deltat} \E_{\delta s \sim \delta s(s, a)}V^\pi(s + \delta s)\nonumber\\
% 		    &= \reward(s, a) \deltat + \gamma^{\deltat} V^\pi(s) +
% 		     \gamma^{\deltat} \partial_s V^\pi(s)f(s, a) \deltat\nonumber\\
% 		    &+ \frac{1}{2}\text{tr}\left(\Sigma^T\partial^2_{s^2}V^\pi(s)\Sigma\right)\deltat + \bigO(\deltat^2)\nonumber\\
% 		    &\xrightarrow[\deltat \to 0]{} V^\pi(s).
% 		    \label{eq:Q_cont}
                      %   \end{align}

A consequence of Theorem \ref{th:q-cont} is that in exact continuous time,
$Q^\pi$ does not bear any information on the ranking of actions, and
thus cannot be used to select actions that yields higher future returns. There is no time continuous Q-learning.

\begin{proof}
	Bellman equation for the discretized Q-function yields
  \begin{align}
    Q^\pi_\deltat(s, a) &= r(s, a)\deltat + \gamma^\deltat \E_{s'|s, a}\left[V^\pi_\deltat(s')\right] 
  \end{align}
  From the environment dynamic equation, 
  \begin{equation}
	  s' = s + f(s, a) \deltat + \smallo(\deltat).
  \end{equation}As $V^\pi$ is assumed to be continuously differentiable,
  \begin{align}
	  V^\pi_\deltat(s') &= V^\pi_\deltat(s) + \partial_s V^\pi_\deltat(s) F(s, a)\deltat + \smallo(\deltat)\\
			    &= V^\pi_\deltat(s) + \bigO(\deltat)
  \end{align}
  Rewriting $Q^\pi_\deltat$ yields
  \begin{align}
	  Q^\pi_\deltat(s, a) &= r(s, a)\deltat + \gamma^\deltat (V^\pi_\deltat(s) + \bigO(\deltat))\\
			      &= (1 + \bigO(\deltat)) (V^\pi_\deltat(s) + \bigO(\deltat)) + \bigO(\deltat)\\
			      &= V^\pi_\deltat(s) + \bigO(\deltat).
  \end{align}
  Now, using Thm.~\ref{th:conv-value},
  \begin{align}
	  Q^\pi_\deltat(s, a) = V^\pi(s) + \bigO(\deltat)
  \end{align}
  which ends the proof.
%  But we know that $s'$ is close to $s$. If we define $\delta s = s' - s$, then:
%  \begin{equation}
%    \E_{s'|s, a}\left[\delta s\right] = \bigO(\deltat) \quad \text{Var}(\delta s) = \bigO(\deltat)
%  \end{equation}
%  \TODO{Explain. And this is not exactly the Variance.} Therefore:
%  \begin{align}
%    |\E_{s'|s, a}\left[V^\pi_\deltat(s')\right] - V^\pi_\deltat(s)| &= \E_{\delta s|s, a}\left[\partial_s V^\pi_\deltat\cdot \delta s +\bigO(\|\delta s\|^2)\right]\nonumber\\
%                                            &\leq \|\partial_s V_\deltat^\pi\|\cdot \|\E_{\delta s|s, a}\left[\delta s\right]\| +\bigO(\delta t)\nonumber\\
%    &= \bigO(\deltat)\nonumber
%  \end{align}
%  Finally:
%  \begin{align}
%    Q^\pi_\deltat(s, a) &= r(s, a)\deltat + \gamma^\deltat \E_{s'|s, a}\left[V_\deltat^\pi(s')\right] \nonumber\\
%                &= r(s, a)\deltat + \left(1 + \bigO(\deltat)\right)(V^\pi(s) + \bigO(\deltat))\nonumber\\
%    &= V^\pi(s) + \bigO(\deltat)\nonumber
%  \end{align}
  \end{proof}

%! TEX root = icml_drau.tex
\section{Near continuous time reinforcement learning}
\label{sec:framework}
% In this section, a continuous reinforcement learning framework is introduced,
% and it is shown that under this framework, the state-action value function does
% not depend on the action. We further show that in near continous domains, when
% the discretization timestep $\deltat$ is small, the difference in state-action
% value between two different actions in a fixed state is of order $\bigO(\deltat)$.
TODO. Main ideas: near continuous time environments (control, mujoco, video games, ...)
There is a time discretization $\deltat$ given by the problem. The uderlying process does not depend of $\deltat$. Need to see what is happening when changing $\deltat$. The problem is invariant to $\deltat$. The quantities we are looking at should be invariant to. The learning procedure should be invariant. Looking at the limits when $\deltat \rightarrow 0$. In simulated environments, changing $\deltat$ only means to change the FPS. In real environments, for example with robots, this could mean changing the quality of the sensors and motors.

\subsection{Framework}

Let assume that we have a Markov Decision Process (TODO:Cite?) $\langle {\cal S}, {\cal A}, T_{\deltat}, r, \gamma\rangle$. ${\cal S}$ is the state-space, which is supposed to be continuous (TODO:not the good formulation). The action space is TODO.

We assume that the transition is continuous. This means that ... TODO. Discussion on the hypothesis, in which environments it is true. Is the condition $\forall \deltat, \forall s, \E[\|s_{k+1} - s_k \||s_k = s] \leq ?$. 

In what follows, $\dt$ denotes a truly infinitesimal time interval while $\deltat$
denotes a small, but non infinitesimal time interval. Similarily, $\dbt$ denotes
an infinitesimal (multi-dimensional) brownian step, while $\deltabt$ denotes its
discretized equivalent. Informally, $\dbt$ is to be thought of as $\gauss(0, \sqrt{\dt}^2)$,
and $\deltabt$ as $\gauss(0, \sqrt{\deltat}^2)$, where $\gauss(\mu, \sigma)$ denotes a gaussian
random variable of mean $\mu$ and standard deviation (or covariance matrix in the multi-dimensional case)
$\sigma$.

Consider a fully observable \emph{Markov Decision Process} defined by the following
transitions and rewards
\begin{align}
	ds(s, a) &= F(s, a) dt + \Sigma(s, a) \dbt\\
	dr(s, a) &= \reward(s, a) dt + \Sigma_r(s, a) \dbt.
\end{align}
The equation on $s$ is that of a diffusion process, with drift coeficient $F$
and diffusion matrix $\Sigma$.  This framework is only moderately restrictive.
For example, deterministic continuous control environments fall in this
framework, with $\Sigma = 0$ and $\Sigma_r = 0$. Limitations of the framework
are discussed in Sec.~\ref{sec:limitations}.
This framework is naturally discretized to arbitrary $\deltat$ by replacing all
$d$'s with $\delta$'s.

A continuous policy $\pi$ is defined as a deterministic mapping from states to
actions.  In continuous time, defining stochastic policies with well defined
state-value functions requires incorporating information from previous action into
the state to obtain a temporally coherent policy. To avoid such inconvenience, the
framework is restricted to deterministic policies. This is a mild constraint, as this
only enforces determinism of the exploitation policy and not of exploration policies.
In what follows, $\tau\sim\pi$ denotes a trajectory of states and actions sampled
according to policy $\pi$, with $s_0$ sampled according to an arbitrary initial distribution
on states $\rho_0$, and $dr_{t} = dr(s_t, a_t)$.

Under a deterministic policy $\pi$, one can define the $\gamma$ discounted
\emph{state value function} $V^\pi(s)$ and \emph{state action value function}
$Q^\pi(s, a)$ as
\begin{align}
	V^\pi(s) &= \E_{\tau\sim\pi}\left[
		\int\limits_{t=0}^\infty \gamma^{t}
		dr_{t} \mid s_0 = s
	\right]\\
	Q^\pi(s, a) &= \E_{\tau\sim\pi}\left[
		\int\limits_{t=0}^\infty \gamma^{t}
		dr_{t} \mid s_0 = s, a_0 = a
	\right]
\end{align}
which are naturally discretized as
\begin{align}
	V^\pi(s) &= \E_{\tau\sim\pi}\left[
		\sum\limits_{k=0}^\infty \gamma^{k\deltat}
		\delta r_{k\deltat} \mid s_0 = s
	\right]\\
	Q^\pi(s, a) &= \E_{\tau\sim\pi}\left[
		\sum\limits_{k=0}^\infty \gamma^{k\deltat}
		\delta r_{k\deltat} \mid s_0 = s, a_0 = a
	\right].
\end{align}
Both $V^\pi$ and $Q^\pi$ verify Bellman equations, informally
\begin{align}
	V^\pi(s) &= \reward(s, \pi(s))\dt + \gamma^{\dt} \E_{ds \sim ds(s, \pi(s))} V^\pi(s + ds)\nonumber\\
	Q^\pi(s, a) &= \reward(s, a)\dt + \gamma^{\dt} \E_{ds(s, a)} V^\pi(s + ds(s, a))
	\label{eq:cont_bell}
\end{align}
which in continuous time translates, using Ito's lemma, into a Hamilton Jacobi equation
\begin{align}
	\reward + \partial_s V^\pi \cdot f + \frac{1}{2} \text{tr}\left(\Sigma^T\partial^2_{s^2} V^\pi\Sigma\right) = (1 - \gamma) V^\pi(s).
\end{align}

\subsection{Independence of $Q^\pi$ on $a$ in continuous time}
Consider a discretization of Eq.~\eqref{eq:cont_bell} with time interval
$\deltat \ll~1$, Ito's lemma yield
\begin{align}
	Q^\pi(s, a) &= \reward(s, a) \deltat + \gamma^{\deltat} \E_{\delta s \sim \delta s(s, a)}V^\pi(s + \delta s)\nonumber\\
		    &= \reward(s, a) \deltat + \gamma^{\deltat} V^\pi(s) +
		     \gamma^{\deltat} \partial_s V^\pi(s)f(s, a) \deltat\nonumber\\
		    &+ \frac{1}{2}\text{tr}\left(\Sigma^T\partial^2_{s^2}V^\pi(s)\Sigma\right)\deltat + \bigO(\deltat^2)\nonumber\\
		    &\xrightarrow[\deltat \to 0]{} V^\pi(s).
		    \label{eq:Q_cont}
\end{align}
A consequence of Eq.~\eqref{eq:Q_cont} is that in exact continuous time,
$Q^\pi$ does not bear any information on the hierarchy of actions, and
thus cannot be used to select actions that yields higher future returns.

As the difference between between $Q^\pi(s, a)$ and $V^\pi(s)$ is of order
$\bigO(\deltat)$, it is natural to define a rescaled version of the advantage
function, namely
\begin{align}
	A^\pi(s, a) &= \frac{Q^\pi(s,a) - V^\pi(s)}{\deltat}\\
		    &= \reward(s, a) +
		    \frac{\gamma^{\deltat} \E_{\delta s \sim \delta s(s, a)} V^\pi(s + \delta s) - V^\pi(s)}{\deltat}.
    \label{eq:adv}
\end{align}
Contrary to the action state value function, the rescaled advantage function converges
to an action dependent quantity when $\deltat$ goes to $0$
\begin{align}
	A^\pi(s, a) &\xrightarrow[\deltat \to 0]{} \reward(s, a) +
	(\gamma - 1) V^\pi(s) + \partial_s V^\pi(s) f(s, a)\nonumber\\
		    &+
	\frac{\text{tr}\left(\Sigma^T(s, a)\partial^2_{s^2}V^\pi(s) \Sigma(s,a)\right)}{2},
\end{align}
and $Q^\pi$ can then be rewritten as
\begin{equation}
	Q^\pi(s, a) = V^\pi(s) + \deltat A^\pi(s, a).
	\label{eq:reparam_q_pi}
\end{equation}


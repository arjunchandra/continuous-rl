%! TEX root = icml_drau.tex
\section{Near continuous time reinforcement learning}
\label{sec:framework}

  Many reinforcement learning environments are not intrinsically time
	discrete, but discretization of an underlying continuous time
	environment. For instance, many control environments, such as the
	Mujoco~\cite{ddpg} environments or OpenAI Gym classic~!CITE! control
	environments are discretization of continuous time control problems,
        \LB{with a discretization timestep $\deltat$ fixed by the simulator}.
	For real world environments, the discretization timestep, $\deltat$ is
	most often imposed by the quality of sensors and actuators. For visual
	simulated environments, it often corresponds to the number of frames
	that the simulator generates per second. As $\deltat$ is not an
	intrinsic parameter of the environment, but a constraint imposed by
	computational ressources, we would expect that decreasing $\deltat$
	could  only improve the performance of reinforcement learning agents.
	To achieve this goal, reinforcement learning algorithms should be
	resilient to a change of $\deltat$, and should remain viable when
	$\deltat \rightarrow 0$. We are thus interested in the behavior of
	reinforcement learning algorithms in discretized environments, when the
	discretization timestep is small. We will refer to such environments as
	\emph{near continuous environments}.

	In what follows, a formalized view of near continuous environments is given,
	along with $\deltat$ dependant definitions of return, discount factor and
	value functions. With these definitions, each of these quantities is shown
	to converge to a well defined continous time limit. Finally, it is shown
	that the state-action value function collapses to the value function as
	$\deltat$ goes to $0$, and consequently that there is no Q-learning in
	continuous time.
      \TODO{We should explain somwhere here that, if not invariant, harder to find good hyperparameters}
\subsection{Framework}

\VC{
  Consider ${\cal S} = \mathbb{R}^d$ a set of states, and ${\cal A}$ a set of
	actions. A familly of \emph{Markov decision processes} ${\cal
	M}_\deltat = \langle {\cal S}, {\cal A}, T_{\deltat}, r_\deltat,
	\gamma_\deltat\rangle$ is \emph{near continous} if for any
	deterministic policy $\pi:{\cal S} \rightarrow {\cal A}$, trajectories
	of ${\cal M}_\deltat$ converge to stochastic processes that satisfy the
	Stochastic differential equation
	\begin{equation}
		\label{eq:diffusion}
		ds = F(s, a)dt + \Sigma(s, a)\dbt.
	\end{equation}
	Informally $F: {\cal S}\times{\cal A}\rightarrow {\cal S}$ describes
	the deterministic dynamic of the environment, while $\Sigma(s, a)\dbt$
	describes its stochasticity. Notably, $\Sigma = 0$ if the environment is
	deterministic, as in classical control theory, or in many usual simulated
	environments (Mujoco, Gym, ...).

	In what follows, ${\cal M}_\deltat$ is taken to be a direct discretization of
	Eq.~\ref{eq:diffusion} with discretization timestep $\deltat$. The
	corresponding transition function $T_\deltat(., s, a)$ is the distribution of
	$s'$ when starting from point $s$, and following Eq.~\ref{eq:diffusion} while
	maintaining $a$ constant on an interval of time $\deltat$.

	
      }

      \LB{
        [[What I don't like in that version is that it is too formal ...]]
        Consider ${\cal S} = \mathbb{R}^d$ a set of states, and ${\cal A}$ a set of
	actions. Assume that an agent is following the continuous Markov Decision Process defined by the Stochastic Differential Equation:
        \begin{equation}
		\label{eq:diffusion}
		ds = F(s, a)dt + \Sigma(s, a)\dbt.
              \end{equation}
        Then, for any $\deltat$, we say that the discrete \emph{Markov decision processes} ${\cal
	M}_\deltat = \langle {\cal S}, {\cal A}, T_{\deltat}, r_\deltat,
      \gamma_\deltat\rangle$ is a discretization of the continuous MDP with \emph{time discretization} $\deltat$ if the transition probability $T_\deltat(s, a, s')$ is the probability distribution of $s_\deltat$, with $s_0 = s$ and the action is $a_t=a$ during the interval $[0, \deltat]$. We say the the MDP ${\cal M}_\deltat$ is \emph{near-continuous}. [[Pb here: we require an uderlying continuous process: there is no intrinsic near continuous process]]

      Informally $F: {\cal S}\times{\cal A}\rightarrow {\cal S}$ describes
	the deterministic dynamic of the environment, while $\Sigma(s, a)\dbt$
	describes its stochasticity. Notably, $\Sigma = 0$ if the environment is
	deterministic, as in classical control theory, or in many usual simulated
	environments (Mujoco, Gym, ...).
      }
      
        In this paper, we try to design RL methods for \emph{near continuous} MDP
	robust to the choice of $\deltat$. Our method is to analyse the different
	quantities we are looking at when $\deltat$ is changing, and especially when
	$\deltat \rightarrow 0$, and make sure that the quantities we are interested in
	and our algorithms do converge to meaningful limit quantities and algorithms.\TODO{Better formulation maybe?}

        \LB{In that direction, the first thing to do is to check that the discretized
          trajectory do converge to the continuous ones. This is stated in the following theorem. [[Tres tres mal dit, mais c'est l'idee]]
        }
        % The first formal statement we can make on discretized MDP with a
	% time discretization $\deltat$ is that if $\deltat \rightarrow 0$, the
	% distribution of trajectories do converge to the continuous trajectories.
      
	\begin{theorem}
		Assume that $\pi: {\cal S} \rightarrow {\cal A}$ is a continuous and
		deterministic policy. Let $(s_t)_{t\in\mathbb{R}_+}$ be the
		continuous process by following \eqref{eq:diffusion} and
		$(s_\deltat^k)$ be the discrete process obtained if the agent only
		get observations every $\deltat$ !explain better!.

		We define the time process $\tilde s_\deltat^t = s^{\lfloor
		\frac{t}{\deltat}\rfloor}_\deltat$ for
		$t\in\mathbb{R_+}$.!Explain!. Then:
		\begin{equation}
			\label{eq:conv-traj}
			(\tilde s_\deltat^t)_{t\in\mathbb{R}_+} 
			\xrightarrow[\deltat \rightarrow 0]{} (s_t)_{t\in\mathbb{R}_+}
		\end{equation}
		in distribution.
	\end{theorem}
      

\subsection{Return, discount factor  and Value function in near continuous time}
\label{sec:ret-gamma-v}

In this section, we show how to define the discount factor $\gamma_\deltat$ as a function of $\deltat$, such that the return and Value function do converge to the desired limit when $\deltat \rightarrow 0$. 

\VC{
	In what follows, $\deltat$-dependant definitions of the discount factor
	and the reward are given. These definitions fit the discrete case
	scenario when $\deltat=1$, and yields well defined, non trivial returns
	and value functions when $\deltat$ goes to $0$.
}

\paragraph{Return and Discount Factor}
For a continuous MDP and a continuous trajectory $\tau = (s_t, a_t)_t$,
the return is defined as~\cite{cont_rl}
\begin{equation}
	\label{eq:continuous-return}
	R(\tau) = \int_{t=0}^\infty\gamma^tr(s_t, a_t)dt
\end{equation}

The natural discretization of this equation, if we want to define a discretized
return $R_\delta$ for each $\deltat$ is:
\begin{equation}
	\label{eq:discretized-return}
	R_\deltat(\tau) = \sum_{k=0}^\infty\gamma^{k\deltat}
	r(s^\deltat_k, a^\deltat_k)\deltat
\end{equation}

Natural definitions of $\gamma_\deltat$ and $r_\deltat$ such
that the return defined in equation (\ref{eq:discretized-return}) corresponds
to the discrete return for the MDP ${\cal M}_\deltat$ follow:
\begin{align}
	\label{eq:def-gamma}
	\gamma_\deltat =& \gamma^\deltat\\
	\label{eq:def-reward}
	r_\deltat =& \deltat . r.
\end{align}
% Thus, we see that the natural discretization of a continuous MDP ${\cal M} = \langle {\cal S}, {\cal A}, T, r, \gamma\rangle$ is ${\cal M}_\deltat = \langle {\cal S}, {\cal A}, T_\deltat, r\deltat, \gamma^\deltat\rangle$.

% Thus, we should define $\gamma_\deltat = \gamma^\deltat$, so we have $\lim_{\deltat \rightarrow 0} R_\deltat(\tau)  = R(\tau)$.

\paragraph{Microscopic/Macroscopic time, Time horizon}
\VC{In near continuous environments, there are two notions of time,
	the number of observation received, or the number of actions taken,
	which we call the microscopic time, and the actual amount of time spent
	in the underlying continuous time environment, which we call the macroscopic
	time. The two are related through
	\begin{equation}
		\tau^\text{macro} = \tau^\text{micro} \deltat.
	\end{equation}

	The notion of time horizon, which is, informally, the amount of time on
	which the agent optimizes its return, is only meaningful in macroscopic
	time. Having a time horizon of $10$ steps means something very different
	if $\deltat = 0.1$ and if $\deltat = 0.0001$. A rule of thumb
	is that the time horizon of an agent with discount factor $\gamma$ is
	of order $\frac{1}{1 - \gamma}$. If $\gamma_\deltat$ was left constant
	as $\deltat$ goes to $0$, the corresponding macroscopic time horizon 
	would write as $\frac{\deltat}{1 - \gamma}$ which goes to $0$ when
	$\deltat$ goes to $0$.

	On the contrary, the definition of $\gamma_\deltat$ in
	Eq.~\eqref{eq:def-gamma} yields an approximately constant macroscopic
	time horizon when $\deltat$ goes to $0$. Indeed,
	\begin{equation}
		\label{eq:time-horizon}
		\frac{\deltat}{1-\gamma^\deltat}
		= - \frac{1}{\log \gamma} + \bigO(\deltat)
		\approx \frac{1}{1-\gamma}
	\end{equation}
      }

      \LB{ \paragraph{Number of steps / Physical time: Time horizon}
        % [[I prefer to talk about ``number of steps'' VS ``Physical time'' than microscopic/Macroscopic]]
        In near continuous environments, there are two notions of time,
	the number of steps, or the number of actions taken, and the actual amount of time spent
	in the underlying continuous time environment, which we call the physical
	time. If $t$ is the real time and $k$ the number of steps, the two are related through $t = k \deltat$.

	The notion of time horizon, which is, informally, the amount of time on
	which the agent optimizes its return, is only meaningful in physical
	time.
        
        %Having a time horizon of $10$ steps means something very different
	%if $\deltat = 0.1$ and if $\deltat = 0.0001$.
        
        A rule of thumb
	is that the time horizon of an agent with discount factor $\gamma$ is
	of order $\frac{1}{1 - \gamma}$. If $\gamma_\deltat$ was left constant
	as $\deltat$ goes to $0$, the corresponding macroscopic time horizon 
	would write as $\frac{\deltat}{1 - \gamma}$ which goes to $0$ when
	$\deltat$ goes to $0$.

	On the contrary, the definition of $\gamma_\deltat$ in
	Eq.~\eqref{eq:def-gamma} yields an approximately constant macroscopic
	time horizon when $\deltat$ goes to $0$. Indeed,
	\begin{equation}
          \label{eq:time-horizon}
          \frac{\deltat}{1-\gamma^\deltat}
          = - \frac{1}{\log \gamma} + \bigO(\deltat)
          \approx \frac{1}{1-\gamma}
        \end{equation}
              
      }
        


\paragraph{Value function}

\LB{I did not change this part}
The definition of the return Eq.~\eqref{eq:continuous-return}, yields the
following definition of the continuous value function
\begin{equation}
  V^\pi(s) = \E_{\tau\sim\pi}\left[\int\limits_{0}^\infty \gamma^{t}\deltat r_{t} \mid s_0 = s\right].
\end{equation}
The corresponding discretization for time discretization $\deltat$ is
\begin{equation}
  V^\pi_\deltat(s) = \E_{\tau\sim\pi}\left[\sum\limits_{k=0}^\infty \gamma^{k\deltat}\deltat r_{k\deltat} \mid s_0 = s\right]
\end{equation}
which fits the usual definition of the discrete V-function for $\gamma_\deltat$ and
$r_\deltat$ defined in~(\ref{eq:def-gamma}, \ref{eq:def-reward}).
We have the following convergence theorem of the discretized value function to the continuous value function:
  \begin{theorem}
    We have, for all $s \in {\cal S}$:
    \begin{equation}
      \label{eq:conv-value}
      \lim_{\deltat \rightarrow 0} V^\pi_\deltat(s) = V^\pi(s)
    \end{equation}
  \end{theorem}

The discretized value function follows the Bellman equation:
\begin{equation}
  \label{eq:bellman}
  V^\pi_\deltat(s) = \reward(s, \pi(s))\deltat + \gamma^{\dt} \E_{s_{t+\deltat} | s_t = s} V^\pi(s_{t+\deltat})
\end{equation}

If the MDP is following the equation (\ref{eq:diffusion}), the limit of the Bellman equation for $V^\pi_\deltat$ when $\deltat \rightarrow 0$ is the Hamilton-Jacobi-Bellman equation on $V^\pi$:
\begin{equation}
  \label{eq:hamilton-jacobi-bellman}
  r + \partial_s V^\pi \cdot F + \frac{1}{2} \text{tr}\left(\Sigma^T\partial^2_{s^2} V^\pi\Sigma\right) = - \log(\gamma) V^\pi(s)
\end{equation}


\subsection{There is no Q-function in continuous time}

Contrary to the Value function, the Action-Value function $Q$ is ill-defined
for a continuous MDP. More precisely, for a continuous MDP, $Q^\pi(s, a) =
V^\pi(s)$, so the continuous $Q$-function is independant of the actions. For a
\emph{near-continuous} MDP, this means that the $Q$-function is very close  to the
$V$-function.\TODO{Say more ?} Moreover, this means that the discretized $Q$
function \TODO{explain}. This can be stated formally.
\begin{theorem}
  For all $s, a$, we have
  \begin{equation}
    \label{eq:q-discr}
    Q^\pi_\deltat(s, a) = V^\pi(s) + \bigO\left(\deltat\right)
  \end{equation}
  
  If we consider the continuous action-value function $Q^\pi$, we have:
  \begin{equation}
    \label{eq:q-cont}
    Q^\pi(s, a) = V^\pi
  \end{equation}
  \label{th:q-cont}
\end{theorem}
% Consider a discretization of Eq.~\eqref{eq:cont_bell} with time interval
% $\deltat \ll~1$, Ito's lemma yield
% \begin{align}
% 	Q^\pi(s, a) &= \reward(s, a) \deltat + \gamma^{\deltat} \E_{\delta s \sim \delta s(s, a)}V^\pi(s + \delta s)\nonumber\\
% 		    &= \reward(s, a) \deltat + \gamma^{\deltat} V^\pi(s) +
% 		     \gamma^{\deltat} \partial_s V^\pi(s)f(s, a) \deltat\nonumber\\
% 		    &+ \frac{1}{2}\text{tr}\left(\Sigma^T\partial^2_{s^2}V^\pi(s)\Sigma\right)\deltat + \bigO(\deltat^2)\nonumber\\
% 		    &\xrightarrow[\deltat \to 0]{} V^\pi(s).
% 		    \label{eq:Q_cont}
                      %   \end{align}

A consequence of Theorem \ref{th:q-cont} is that in exact continuous time,
$Q^\pi$ does not bear any information on the hierarchy of actions, and
thus cannot be used to select actions that yields higher future returns.

\begin{proof}
  \begin{align}
    Q^\pi(s, a) &= r(s, a)\deltat + \gamma^\deltat \E_{s'|s, a}\left[V(s')\right] \\
  \end{align}
  But we know that $s'$ is close to $s$. If we define $\delta s = s' - s$, then:
  \begin{align}
    \|\E_{s'|s, a}\left[\delta s\right]\| &= \bigO(\deltat) \\
    \E_{s'|s, a}\left[\|\delta s\|^2\right] &= \bigO(\deltat)
  \end{align}
  \TODO{Explain.} Therefore:
  \begin{align}
    |\E_{s'|s, a}\left[V(s')\right] - V(s)| &= \E_{\delta s|s, a}\left[\partial_s V\cdot \delta s +\bigO(\|\delta s\|^2)\right]\\
                                            &\leq \|\partial_s V\|\cdot \|\E_{\delta s|s, a}\left[\delta s\right]|\| +\bigO(\delta t)\\
    &= \bigO(\deltat)
  \end{align}
  Finally:
  \begin{align}
    Q^\pi(s, a) &= r(s, a)\deltat + \gamma^\deltat \E_{s'|s, a}\left[V(s')\right] \\
                &= r(s, a)\deltat + \left(1 + \bigO(\deltat)\right)(V(s) + \bigO(\deltat))\\
    &= V(s) + \bigO(\deltat)
  \end{align}
  \end{proof}

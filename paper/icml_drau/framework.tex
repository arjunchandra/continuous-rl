%! TEX root = icml_drau.tex
\section{Near continuous-time reinforcement learning}
\label{sec:framework}

Many reinforcement learning environments are not intrinsically
time-discrete, but discretizations of an underlying continuous-time
environment. For instance, many simulated control environments, such as
the Mujoco environments~\cite{ddpg} or OpenAI Gym classic control
environments~\cite{gym}, are discretization of continuous-time control
problems.  In simulated environments, the time discretization is fixed by
the simulator, and is often used to approximate an underlying
differential equation.  For example, the timestep may correspond to the
number of frames generated by second.  In real world environments,
sensors and actuators have a fixed time precision: cameras can only
capture a fixed amount of frames per second, and physical limitations
prevent actuators from responding instantaneously. The quality of these
components thus imposes a lower bound on the discretization timestep. As
the timestep $\deltat$ is not an intrinsic parameter of the
environment, but a constraint imposed by computational ressources, we
would expect that decreasing $\deltat$ could only improve the performance
of reinforcement learning agents (though it might make optimization
harder).  Reinforcement
learning algorithms should, at least, be resilient to a change of
$\deltat$, and should remain viable when $\deltat \rightarrow 0$.
Besides, designing a time discretization invariant algorithm could
alleviate tedious hyperparameterization by providing better defaults for
time-horizon-related parameters.

We are thus interested in the behavior of reinforcement learning
algorithms in discretized environments, when the discretization timestep
is small. We will refer to such environments as \emph{near-continuous
environments}.
A formalized view of near-continuous environments is
given below, along with $\deltat$-dependent definitions of return, discount
factor and value functions, that converge to well defined
continous-time limits. The state-action value
function is shown to collapse to the value function as $\deltat$ goes to $0$.
Consequently there is no $Q$-learning in continuous time, foreshadowing
problematic behaviors of near continuous time $Q$-learning.
%      \TODO{We should explain somwhere here that, if not invariant,
%      harder to find good hyperparameters} DONE

\subsection{Framework}

% \VC{
%   Consider ${\cal S} = \mathbb{R}^d$ a set of states, and ${\cal A}$ a set of
% 	actions. A familly of \emph{Markov decision processes} ${\cal
% 	M}_\deltat = \langle {\cal S}, {\cal A}, T_{\deltat}, r_\deltat,
% 	\gamma_\deltat\rangle$ is \emph{near continous} if for any
% 	deterministic policy $\pi:{\cal S} \rightarrow {\cal A}$, trajectories
% 	of ${\cal M}_\deltat$ converge to stochastic processes that satisfy the
% 	Stochastic differential equation
% 	\begin{equation}
% 		\label{eq:diffusion}
% 		ds = F(s, a)dt + \Sigma(s, a)\dbt.
% 	\end{equation}
% 	Informally $F: {\cal S}\times{\cal A}\rightarrow {\cal S}$ describes
% 	the deterministic dynamic of the environment, while $\Sigma(s, a)\dbt$
% 	describes its stochasticity. Notably, $\Sigma = 0$ if the environment is
% 	deterministic, as in classical control theory, or in many usual simulated
% 	environments (Mujoco, Gym, ...).

% 	In what follows, ${\cal M}_\deltat$ is taken to be a direct discretization of
% 	Eq.~\ref{eq:diffusion} with discretization timestep $\deltat$. The
% 	corresponding transition function $T_\deltat(., s, a)$ is the distribution of
% 	$s'$ when starting from point $s$, and following Eq.~\ref{eq:diffusion} while
% 	maintaining $a$ constant on an interval of time $\deltat$.


%       }

      
% TODO: For now, differential equation is the default, SDE is a footnote.
Let ${\cal S} = \mathbb{R}^d$ be a set of states, and ${\cal A}$ be a set of
actions. Consider the continuous \emph{Markov Decision Process} (MDP) defined by the differential equation %\TODO{Explain the intuition of Brownian in SDE}:
        \begin{equation}
	\label{eq:diffusion}
	ds_t/dt = F(s_t, a_t) % + \Sigma(s, a)\dbt.
              \end{equation}
where $F\colon {\cal S}\times{\cal A}\rightarrow {\cal S}$ describes
the dynamics of the environment. The agent interacts with the environment through a deterministic policy function
$\pi \colon {\cal S} \rightarrow {\cal A}$, so that $a_t=\pi(s_t)$. 
For simplicity we assume here that both the dynamics and exploitation policy are
deterministic; \footnote{
      We believe the results presented here hold more
      generally, assuming states
      follow a \emph{stochastic} differential equation \begin{equation}
	      ds = F(s, a)dt  + \Sigma(s, a)\dbt
	      \label{eq:sde}
      \end{equation} with
      $B_t$ a multidimensional Brownian motion and $\Sigma$ a covariance matrix. A
      formal treatment of SDEs is beyond the scope of this paper.}
 the exploration policy will be random, but care must be
taken to properly define random policies with
discrete actions in continuous time, see below.

For any $\deltat>0$, we can define an MDP ${\cal
M}_\deltat = \langle {\cal S}, {\cal A}, T_{\deltat}, r_\deltat,
      \gamma_\deltat\rangle$ as a discretization of the continuous MDP with
      \emph{time discretization} $\deltat$. The %deterministic
      transition function
      %$T_\deltat(s, a, s')$ 
      is %a dirac on $s_\deltat$, where $s_\deltat$ is 
      the
      state obtained
      when starting at $s_0 = s$ and maintaining $a_t=a$ constant for a time
      $\deltat$.
      This corresponds to an agent evolving in the continuous
      environment \eqref{eq:diffusion}, but 
      only making observations and choosing actions every $\deltat$. The
      rewards and decay factor are specified below. We
      call such an  MDP ${\cal M}_\deltat$
      \emph{near-continuous}.      % This framework covers classical control theory, as well as many usual simulated environments. TODO: not convinced that this is relevant here.

%      \footnote{Informally, $T_\deltat(s, a, s')$ is approximately
%	      the probability distribution of $s + F(s, a)\deltat +
%	      \sqrt{\deltat}\Sigma(s, a){\cal N}(0, I)$, which is the
%	      discretization of Eq. \eqref{eq:diffusion} \TODO{CITE}}.
        %In this paper, we try to design RL methods for near continuous MDP
%robust to the choice of $\deltat$. 

By standard arguments for time discretizations of differential
equations, for a given policy the near-continuous states converge to the
continuous-time states when $\deltat\to 0$. \TODO{ref to appendix+formal
statement in appendix}% We now turn to the RL-related quantities.

A necessary condition for robustness of an algorithm for
near-continuous time MDPs is 
to remain viable when $\deltat \rightarrow 0$. Thus we will try to
make sure the various quantities involved converge to meaningful
limits when $\deltat\to 0$.

%         The first step in that direction is to check that the discretized
%           trajectories converge to the continuous ones. 
% \begin{theorem}
% 	Assume that $\pi: {\cal S} \rightarrow {\cal A}$ is a continuous and
% 	deterministic policy. Let $(s_t)_{t\in\mathbb{R}_+}$ be the
% 	continuous process obtained by following \eqref{eq:diffusion} and
% 	$(s_\deltat^k)$ be the discrete process obtained if the agent only
% 	get observations and interacts with the environment every $\deltat$ \TODO{explain better}.
% 
% 	We define the time process $\tilde s_\deltat^t = s^{\lfloor
% 	\frac{t}{\deltat}\rfloor}_\deltat$ for
% 	$t\in\mathbb{R_+}$.\TODO{Explain}. Then:
% 	\begin{equation}
% 		\label{eq:conv-traj}
% 		(\tilde s_\deltat^t)_{t\in\mathbb{R}_+} 
% 		\xrightarrow[\deltat \rightarrow 0]{} (s_t)_{t\in\mathbb{R}_+}.
% 	\end{equation}
% \end{theorem}
      

% \subsection{Return, discount factor and value function in near continuous time}
% \label{sec:ret-gamma-v}




\paragraph{Return and Discount Factor.}
Suitable 
$\deltat$-dependent scalings of the discount factor $\gamma_\deltat$
and the reward $r_\deltat$ are as follows. These definitions fit the discrete case
when $\deltat=1$, and provide well defined, non-trivial returns
and value functions when $\deltat$ goes to $0$.

For a continuous MDP and a continuous trajectory $\tau = (s_t, a_t)_t$,
the return is defined as~\cite{cont_rl}
\begin{equation}
\label{eq:continuous-return}
R(\tau) \deq \int_{t=0}^\infty\gamma^tr(s_t, a_t)dt.
\end{equation}

A natural time discretization is obtained  by defining the discretized return
return $R_\delta$ for each $\deltat$ as
\begin{equation}
\label{eq:discretized-return}
R_\deltat(\tau) \deq \sum_{k=0}^\infty\gamma^{k\deltat}
r(s_\deltat^k, a_\deltat^k)\deltat
\end{equation}
and the discretized return will correspond to the continuous-time return
if we set the decay factor $\gamma_\deltat$ and rewards $r_\deltat$ of
the discretized MDP ${\cal M}_\deltat$ to
% Natural definitions of $\gamma_\deltat$ and $r_\deltat$ such
% that the return defined in equation (\ref{eq:discretized-return}) corresponds
% to the discrete return for the MDP ${\cal M}_\deltat$ follow:
\begin{align}
\label{eq:def-gamma}
%\label{eq:def-reward}
\gamma_\deltat \deq \gamma^\deltat,\qquad
r_\deltat \deq \deltat . r.
\end{align}
% Thus, we see that the natural discretization of a continuous MDP ${\cal M} = \langle {\cal S}, {\cal A}, T, r, \gamma\rangle$ is ${\cal M}_\deltat = \langle {\cal S}, {\cal A}, T_\deltat, r\deltat, \gamma^\deltat\rangle$.

% Thus, we should define $\gamma_\deltat = \gamma^\deltat$, so we have $\lim_{\deltat \rightarrow 0} R_\deltat(\tau)  = R(\tau)$.

% \paragraph{Microscopic/Macroscopic time, Time horizon}
% \VC{In near continuous environments, there are two notions of time,
% 	the number of observation received, or the number of actions taken,
% 	which we call the microscopic time, and the actual amount of time spent
% 	in the underlying continuous time environment, which we call the macroscopic
% 	time. The two are related through
% 	\begin{equation}
% 		\tau^\text{macro} = \tau^\text{micro} \deltat.
% 	\end{equation}

% 	The notion of time horizon, which is, informally, the amount of time on
% 	which the agent optimizes its return, is only meaningful in macroscopic
% 	time. Having a time horizon of $10$ steps means something very different
% 	if $\deltat = 0.1$ and if $\deltat = 0.0001$. A rule of thumb
% 	is that the time horizon of an agent with discount factor $\gamma$ is
% 	of order $\frac{1}{1 - \gamma}$. If $\gamma_\deltat$ was left constant
% 	as $\deltat$ goes to $0$, the corresponding macroscopic time horizon 
% 	would write as $\frac{\deltat}{1 - \gamma}$ which goes to $0$ when
% 	$\deltat$ goes to $0$.

% 	On the contrary, the definition of $\gamma_\deltat$ in
% 	Eq.~\eqref{eq:def-gamma} yields an approximately constant macroscopic
% 	time horizon when $\deltat$ goes to $0$. Indeed,
% 	\begin{equation}
% 		\label{eq:time-horizon}
% 		\frac{\deltat}{1-\gamma^\deltat}
% 		= - \frac{1}{\log \gamma} + \bigO(\deltat)
% 		\approx \frac{1}{1-\gamma}
% 	\end{equation}
%       }

      \paragraph{Physical time vs algorithmic time, time horizon.}
        % [[I prefer to talk about ``number of steps'' VS ``Physical time'' than microscopic/Macroscopic]]
        In near continuous environments, there are two notions of time:
the algorithmic time $k$ (number of steps or actions taken), and the
physical time $t$ (time spent
in the underlying continuous time environment), related via
$t=k.\deltat$.

The time horizon is, informally, the time range over
which the agent optimizes its return.
        %Having a time horizon of $10$ steps means something very different
%if $\deltat = 0.1$ and if $\deltat = 0.0001$.
As a rule of thumb,
the time horizon of an agent with discount factor $\gamma$ is
of order $\frac{1}{1 - \gamma}$ steps; beyond that, the decay factor
kicks in and the influence of further rewards becomes small.

The definition~\eqref{eq:def-gamma} of the decay factor $\gamma_\deltat$ in
near-continuous environments keeps the time horizon constant in
\emph{physical} time, by making $\gamma_\deltat$ close to $1$ in
algorithmic time. Indeed, physical time horizon is $\deltat$ times the
algorithmic time horizon, namely
\begin{equation}
          \label{eq:time-horizon}
          \frac{\deltat}{1-\gamma^\deltat}
          %= - \frac{1}{\log \gamma} + \bigO(\deltat)
          \approx \frac{1}{1-\gamma},
        \end{equation}
which is thus stable when $\deltat\to 0$.
On the other hand,
if $\gamma_\deltat$ was left constant
as $\deltat$ goes to $0$, the corresponding time horizon in physical time
would be $\approx \frac{\deltat}{1 - \gamma}$ which goes to $0$ when
$\deltat$ goes to $0$: such an agent would be increasingly short-sighted
as $\deltat\to 0$.

In the following, we use the suitably-scaled decay factor
\eqref{eq:def-gamma} both for Deep Advantage Updating and for the
classical deep $Q$-learning baselines.
%         On the contrary, the definition of $\gamma_\deltat$ in
% Eq.~\eqref{eq:def-gamma} yields an approximately constant
% time horizon in physical time when $\deltat$ goes to $0$. Indeed,
%         approximately invariant to $\deltat$ when $\deltat$ goes to 0.
%       
        


\paragraph{Value function.}
The return \eqref{eq:continuous-return} yields the
following continuous-time value function
\begin{align}
  V^\pi(s) &= \E_{\tau\sim\pi}\left[R(\tau) \mid s_0 = s\right]\\
  &= \E_{\tau\sim\pi}\left[\int\limits_{0}^\infty \gamma^{t} r(s_t, a_t) dt \mid s_0 = s\right].
\end{align}
Meanwhile, the value function (in the ordinary sense) of the discrete MDP ${\cal
M}_\deltat$ is
\begin{align}
  V^\pi_\deltat(s) &= \E_{\tau\sim\pi}\left[R_\deltat(\tau) \mid s_0 = s\right]\\
  &= \E_{\tau\sim\pi}\left[\sum\limits_{k=0}^\infty \gamma^{k\deltat}r(s^k_{\deltat}, a^k_{\deltat})\deltat \mid s_0 = s\right]
\end{align}
% which fits the usual definition of the discrete $V$-function for $\gamma_\deltat$ and
% $r_\deltat$ defined in \eqref{eq:def-gamma}.
which obeys the Bellman equation
\footnote{
If the continuous MDP dynamic follows~\eqref{eq:diffusion}, the limit of the Bellman equation for $V^\pi_\deltat$ when $\deltat \rightarrow 0$ is the Hamilton-Jacobi-Bellman equation on $V^\pi$ \cite{cont_rl}
\begin{equation}
  \label{eq:hamilton-jacobi-bellman}
  r + \partial_s V^\pi \cdot F = - \log(\gamma) V^\pi
\end{equation}
}
\begin{equation}
  \label{eq:bellman}
  V^\pi_\deltat(s) = \reward(s, \pi(s))\deltat + \gamma^{\deltat} \E_{s_{\deltat}^{k+1} | s_{\deltat}^{k} = s} V^\pi_\deltat(s_{\deltat}^{k+1})
\end{equation}

When the timestep tends to $0$, one converges to the other.
  \begin{theorem}
    \label{th:conv-value}
   Under suitable assumptions \TODO{ref appendix}, for all $s \in {\cal
   S}$, one has
    %\begin{equation}
      %\label{eq:conv-value}
      $V^\pi_\deltat(s) = V^\pi(s) + \bigO(\deltat)$
    %\end{equation}
    when $\deltat\to 0$.
  \end{theorem}

\subsection{There is no $Q$-function in continuous time}

Contrary to the value function, the action-value function $Q$ is ill-defined
for continuous-time MDPs. More precisely, the $Q$-function  collapses to the
$V$-function when $\deltat \rightarrow 0$. In near continuous time, the effect of
individual actions on the $Q$-function is of order $\bigO(\deltat)$. This makes
deriving a ranking of action from an approximate $Q$-function difficult.
Formally:
\begin{theorem}
Let $\pi$ be a deterministic policy such that for all $\deltat$, $V^\pi_\deltat$ is continuously
differentiable, then, for all $s, a$,
\begin{equation}
	\label{eq:q-discr}
	Q^\pi_\deltat(s, a) = V^\pi(s) + \bigO\left(\deltat\right)
\end{equation}

  \label{th:q-cont}
\end{theorem}
% Consider a discretization of Eq.~\eqref{eq:cont_bell} with time interval
% $\deltat \ll~1$, Ito's lemma yield
% \begin{align}
% 	Q^\pi(s, a) &= \reward(s, a) \deltat + \gamma^{\deltat} \E_{\delta s \sim \delta s(s, a)}V^\pi(s + \delta s)\nonumber\\
% 		    &= \reward(s, a) \deltat + \gamma^{\deltat} V^\pi(s) +
% 		     \gamma^{\deltat} \partial_s V^\pi(s)f(s, a) \deltat\nonumber\\
% 		    &+ \frac{1}{2}\text{tr}\left(\Sigma^T\partial^2_{s^2}V^\pi(s)\Sigma\right)\deltat + \bigO(\deltat^2)\nonumber\\
% 		    &\xrightarrow[\deltat \to 0]{} V^\pi(s).
% 		    \label{eq:Q_cont}
                      %   \end{align}

A consequence of Theorem \ref{th:q-cont} is that in exact continuous time,
$Q^\pi$ does not bear any information on the ranking of actions, and
thus cannot be used to select actions that yields higher future returns. There is no time continuous $Q$-learning.

\begin{proof}
Bellman equation for the discretized $Q$-function yields
  \begin{align}
    Q^\pi_\deltat(s, a) &= r(s, a)\deltat + \gamma^\deltat \E_{s'|s, a}\left[V^\pi_\deltat(s')\right] 
  \end{align}
  From the environment dynamic equation, 
  \begin{equation}
  s' = s + f(s, a) \deltat + \smallo(\deltat).
  \end{equation}As $V^\pi$ is assumed to be continuously differentiable,
  \begin{align}
  V^\pi_\deltat(s') &= V^\pi_\deltat(s) + \partial_s V^\pi_\deltat(s) F(s, a)\deltat + \smallo(\deltat)\\
		    &= V^\pi_\deltat(s) + \bigO(\deltat)
  \end{align}
  Rewriting $Q^\pi_\deltat$ yields
  \begin{align}
  Q^\pi_\deltat(s, a) &= r(s, a)\deltat + \gamma^\deltat (V^\pi_\deltat(s) + \bigO(\deltat))\\
		      &= (1 + \bigO(\deltat)) (V^\pi_\deltat(s) + \bigO(\deltat)) + \bigO(\deltat)\\
		      &= V^\pi_\deltat(s) + \bigO(\deltat).
  \end{align}
  Now, using Thm.~\ref{th:conv-value},
  \begin{align}
  Q^\pi_\deltat(s, a) = V^\pi(s) + \bigO(\deltat)
  \end{align}
  which ends the proof.
%  But we know that $s'$ is close to $s$. If we define $\delta s = s' - s$, then:
%  \begin{equation}
%    \E_{s'|s, a}\left[\delta s\right] = \bigO(\deltat) \quad \text{Var}(\delta s) = \bigO(\deltat)
%  \end{equation}
%  \TODO{Explain. And this is not exactly the Variance.} Therefore:
%  \begin{align}
%    |\E_{s'|s, a}\left[V^\pi_\deltat(s')\right] - V^\pi_\deltat(s)| &= \E_{\delta s|s, a}\left[\partial_s V^\pi_\deltat\cdot \delta s +\bigO(\|\delta s\|^2)\right]\nonumber\\
%                                            &\leq \|\partial_s V_\deltat^\pi\|\cdot \|\E_{\delta s|s, a}\left[\delta s\right]\| +\bigO(\delta t)\nonumber\\
%    &= \bigO(\deltat)\nonumber
%  \end{align}
%  Finally:
%  \begin{align}
%    Q^\pi_\deltat(s, a) &= r(s, a)\deltat + \gamma^\deltat \E_{s'|s, a}\left[V_\deltat^\pi(s')\right] \nonumber\\
%                &= r(s, a)\deltat + \left(1 + \bigO(\deltat)\right)(V^\pi(s) + \bigO(\deltat))\nonumber\\
%    &= V^\pi(s) + \bigO(\deltat)\nonumber
%  \end{align}
  \end{proof}

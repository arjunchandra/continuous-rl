%! TEX root = icml_drau.tex
\section{Conclusion}



From the observation that standard Q-learning based methods fail to learn with small time discretization,
we derived a formal explanation for their sensitiviy.
% Empirically, we find that
% Q-learning-based approaches such as
% \emph{Deep Q-learning}~\cite{dqn} and \emph{Deep Deterministic
% Policy Gradient}~\cite{ddpg} 
% are highly sensitive to variations of time discretization and
% collapse with small time steps. Formally, we prove that
% $Q$-learning does not exist in
% continuous time.
We defined an off-policy algorithm based on this analysis which yields better robustness properties.


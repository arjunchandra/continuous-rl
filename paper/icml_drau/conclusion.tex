%! TEX root = icml_drau.tex
\section{Conclusion}
$Q$-learning methods have been found to fail to learn with small time
steps, both theoretically and empirically. A theoretical analysis
help in building a practical off-policy deep RL algorithm with better robustness to time
discretization. This robustness is confirmed empirically.
% 
% From the observation that standard Q-learning based methods fail to learn with small time discretization,
% we derived a formal explanation for their sensitiviy.
% % Empirically, we find that
% % Q-learning-based approaches such as
% % \emph{Deep Q-learning}~\cite{dqn} and \emph{Deep Deterministic
% % Policy Gradient}~\cite{ddpg} 
% % are highly sensitive to variations of time discretization and
% % collapse with small time steps. Formally, we prove that
% % $Q$-learning does not exist in
% % continuous time.
% We defined an off-policy algorithm based on this analysis which yields better robustness properties.

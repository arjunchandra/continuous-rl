%! TEX root = icml_drau.tex
\section{Related work}
\label{sec:related}

Our approach builds on \cite{adv_upd}, who identified the collapse of
Q-learning for small time steps and suggested as a solution the Advantage
Updating algorithm with proper $\deltat$ scalings for the $V$ part and
the advantage part; testing was only done on a quadratic-linear problem.
We expand on \cite{adv_upd} in several directions. First, we modify the
algorithm by
using a different normalization step for $A$, which forgoes the need to
learn the normalization itself, thanks to the parameterization
\eqref{eq:Aparam}. Second, we test Advantage Updating for the first time on a
variety on RL environments using deep networks, establishing Deep
Advantage Updating as a viable algorithm in this setting.  Third,
we provide formal proofs in a general setting for the collapse of Q-learning when $\deltat\to
0$, and for the non-collapse of Advantage Updating with the proper scalings.
Fourth, we also discuss how to obtain $\deltat$-invariant exploration. Fifth, we
provide stringent experimental tests of the actual robustness to changing
$\deltat$.
%  \cite{adv_upd} already separated the learning procedure of the
% value and advantage functions, and introduced proper scalings between the two
% components. We extend on \cite{adv_upd} in three directions. We provide a
% formal argument on why the advantage contribution of the Q-function is of order
% $\bigO(\deltat)$, irrelevant of the stochasticity of the environment, in a quite
% general framework. We show that, provided with well suited learning rates and
% exploration method, Advantage updating provide near invariance to change of time
% discretization. We show that Advantage updating is viable in the context of deep
% reinforcement learning, while not using the true gradient of the residual error.

\TODO{Say that Actor-critic has no problem when $\deltat\to 0$?}

More recently, \cite{dueling_nets} also introduced a parameterization separating
the value and the advantage components of the Q-function. Contrary to
\cite{advup}'s Advantage Updating, learning is still done in a standard
way on the $Q$-function obtained from adding these two components. Thus this
approach reparameterizes $Q$ but does not introduce the proper scalings
and does not result in an invariant algorithm for small $\deltat$.%We additionally
%introduce a natural scaling coeficient between the two, that strongly improves
%the time discretization invariance properties.

Continuous and near continuous reinforcement learning, which is at the core of
our approach has been thoroughly studied in the deterministic case
in~\cite{adv_upd} or \cite{cont_rl}, but has not been applied in the context of
deep reinforcement learning.

Reinforcement learning has been studied from a mathematical perspective
when time and space are both continuous, in connection with optimal
control and the Hamilton--Jacobi--Bellman equation (a PDE which
characterizes the value function for continuous space-time). We refer to
\cite{cont_rl} and the references therein. In particular \TODO{cite
Munos, convergence of sol to HJB.}  The corresponding algorithms are
often impractical \cite{cont_rl} for larger-dimensional problems.
\cite{cont_rl} focusses on practical algorithms for the continuous
space-time case, including advantage updating.

Here on the other hand we focus on generic deep RL algorithms that can handle
both discrete and continuous time and space, without collapsing in
continuous time, thus being robust to arbitrary timesteps.

%The approach presented in this paper is closely related to \cite{adv_upd} and \cite{cont_rl}.


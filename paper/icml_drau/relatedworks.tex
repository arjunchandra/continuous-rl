%! TEX root = icml_drau.tex
\section{Related Work}
\label{sec:related}

Our approach builds on \cite{adv_upd},  who identified the collapse of
$Q$-learning for small time steps and, as a solution, suggested the Advantage
Updating algorithm, with proper $\deltat$ scalings for the $V$ part and
the advantage part; testing was only done on a quadratic-linear problem.
\TODO{mention that Baird did not experimentally study robustness. NDY:
fifth point below}

We expand on \cite{adv_upd} in several directions. First, we modify the
algorithm by
using a different normalization step for $A$, which forgoes the need to
learn the normalization itself, thanks to the parameterization
\eqref{eq:Aparam}. Second, we test Advantage Updating for the first time on a
variety on RL environments using deep networks, establishing Deep
Advantage Updating as a viable algorithm in this setting.  Third,
we provide formal proofs in a general setting for the collapse of $Q$-learning when $\deltat\to
0$, and for the non-collapse of Advantage Updating with the proper scalings.
Fourth, we also discuss how to obtain $\deltat$-invariant exploration. Fifth, we
provide stringent experimental tests of the actual robustness to changing
$\deltat$.
%  \cite{adv_upd} already separated the learning procedure of the
% value and advantage functions, and introduced proper scalings between the two
% components. We extend on \cite{adv_upd} in three directions. We provide a
% formal argument on why the advantage contribution of the $Q$-function is of order
% $\bigO(\deltat)$, irrelevant of the stochasticity of the environment, in a quite
% general framework. We show that, provided with well suited learning rates and
% exploration method, Advantage updating provide near invariance to change of time
% discretization. We show that Advantage updating is viable in the context of deep
% reinforcement learning, while not using the true gradient of the residual error.

\TODO{Somewhere else: Say that Actor-critic has no problem when $\deltat\to 0$? With SGD
but not with RMSProp, and needs robust exploration, and limited to
on-policy}

\cite{dueling_nets} also use a parameterization separating
the value and advantage components of the $Q$-function. But contrary to
\cite{adv_upd}'s Advantage Updating, learning is still done in a standard
way on the $Q$-function obtained from adding these two components. Thus this
approach reparameterizes $Q$ but does not change scalings
and does not result in an invariant algorithm for small $\deltat$.%We additionally
%introduce a natural scaling coefficient between the two, that strongly improves
%the time discretization invariance properties.

% Continuous and near continuous reinforcement learning, which is at the core of
% our approach has been thoroughly studied in the deterministic case
% in~\cite{adv_upd} or \cite{cont_rl}, but has not been applied in the context of
% deep reinforcement learning.

The problem studied here is a continuity effect quite distinct from
multiscale RL approaches: indeed the issue arises even if there
is only one timescale in the environment. Arguably, a small
$\deltat$ can be seen as a mismatch between the algorithm's
timescale and the physical system's timescale, but the collapse of the
$Q$ function to the $V$ function is an intrinsic mathematical phenomenon \TODO{arising} from time
continuity, and cannot obviously be addressed via existing multiscale
techniques. \TODO{cite something!}

Reinforcement learning has been studied from a mathematical perspective
when time and space are both continuous, in connection with optimal
control and the Hamilton--Jacobi--Bellman (HJB) equation (a PDE which
characterizes the value function for continuous space-time). Explicit
algorithms for continuous
space-time can be found in
\cite{cont_rl,MunosBourgines98} (see also the references therein).
\cite{MunosBourgines98} use a grid approach to provably solve the HJB
equation when discretization tends to $0$ (assuming every state in the
grid is visited a large number of times). However,
the resulting algorithms are
impractical \cite{cont_rl} for larger-dimensional problems.
\cite{cont_rl} focusses on algorithms specific to the continuous
space-time case, including advantage updating and modelling
the time derivative of the environment.

Here on the other hand we focus on generic deep RL algorithms that can handle
both discrete and continuous time and space, without collapsing in
continuous time, thus being robust to arbitrary timesteps.

%The approach presented in this paper is closely related to \cite{adv_upd} and \cite{cont_rl}.


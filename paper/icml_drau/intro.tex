%! TEX root = icml_drau.tex

\section{Introduction}
\label{sec:intro}
In recent years, \emph{Deep Reinforcement Learning} (DRL) approaches have
provided impressive results in a variety of domains, achieving superhuman
performance with no expert knowledge in perfect information zero-sum
games~\cite{alphazero}, reaching level of top players in video
games~(\citealt{openai_five}, \citealt{dqn}), or learning dexterous manipulation
from scratch without demonstrations~\cite{hand_control}. 
In spite of those successes, DRL approaches are
sensitive to a number of factors, including hyperparameterization,
implementation details or small changes in the environment
parameters~(\citealt{drl_matter}, \citealt{drl_matter_bis}). This sensitivity,
along with sample inefficiency, largely prevents DRL from being applied in real
world settings. Notably, high sensitivity to environment parameters prevents
transfer from imperfect simulators to real world scenarios.

In this paper we focus on the sensitivity to time discretization of DRL
approaches, such as what happens when an agent receives $50$ observations
and is expected to take $50$ actions per second instead of $10$. One
would expect that decreasing time discretization, or equivalently
shortening reaction time, should only improve agent performance.
Robustness to time discretization is especially relevant in
\emph{near-continuous} environments, which includes most continuous
control environments, robotics, and many video games.  (We focus on
near-continuous time because this leads to a clear framework, but the
approach below would be relevant in any setting in which the advantage of
any single individual action is small compared to the total return.)%, for instance, long time horizon problems.

In practice, standard approaches based on estimation of state action value functions, e.g.
\emph{Deep Q-learning} (DQN~\citep{dqn} and \emph{Deep deterministic policy
gradient} (DDPG~\citep{ddpg}) are not robust to changes in time discretization. This is shown experimentally in Sec.~\ref{sec:exp}. 
Intuitively, as the discretization timestep
decreases, the effect of individual actions on the total return decreases
too: $Q(s,a)$ is the value of playing action $a$ then playing optimally,
and if $a$ is only maintained for a very short time its advantage over
other actions will be accordingly small. (This continuity effect occurs even with a
well-adjusted decay factor $\gamma$.)

The natural framework to study this phenomenon is Continuous time
reinforcement learning (Sec.~\ref{sec:framework}) \cite{cont_rl,
adv_upd}. Formally, the effect of a single action vanishes when the
discretization timestep becomes infinitesimal: there is no
continuous-time $Q$-function. 

A time discretization invariant algorithm should in particular admit a
continuous-time
limit when the discretization timestep goes to $0$.  This
leads to precise design choices in term of agent architecture, exploration
policy, and learning rates scalings.  The resulting algorithm is shown to
provide better invariance to time discretization than
vanilla DQN or DDPG, on many environments (Sec.~\ref{sec:exp}).  On a new environment, as soon as the
order of magnitude of the time discretization is known, our analysis readily
provides relevant scalings for a number of hyperparameters.


Our contribution is threefold:
\begin{itemize} 
\item Building on \cite{adv_upd}, we formally show that the $Q$-function collapses to the $V$-function in near-continuous time, and thus that
    standard $Q$-learning is ill-behaved in this setting.
  \item Our analysis of limit properties in near continuous environments leads to a robust off-policy algorithm. In particular,
    it provides insights on how to design architectures, constrain exploration, and scale learning rates.
  \item We empirically show that standard Q-learning methods are not
  robust to changes in time discretization, while our algorithm provides
  substantial robustness in that domain.
\end{itemize}



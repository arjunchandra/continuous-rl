%! TEX root = icml_drau.tex

\section{Introduction}
\label{sec:intro}
In recent years, deep learning based approaches to reinforcement learning have
thrived and provided impressive results in a variety of domains, achieving superhuman
performances with no expert knowledge in perfect information zero-sum
games~\cite{alphazero}, reaching level of top players in video
games~(\citealt{openai_five}, \citealt{dqn}), or learning dexterous manipulation
from scratch without demonstrations~\cite{hand_control}. \LB{I would merge these two paragraphs.}

In spite of those successes, \emph{Deep reinforcement learning} approaches are
sensitive to a number of factors, including hyperparameterization,
implementation details or small changes in the environment
parameters~(\citealt{drl_matter}, \citealt{drl_matter_bis}). This sensitivity,
along with sample inefficiency, prevents DRL from being applied in most real
world settings. High sensitivity to environment parameters notably prevents
transfering abilities from imperfect simulators to real world scenarios.

In this paper we focus our attention on the sensitivity to time discretization
of DRL approaches \LB{in \emph{near-continuous} environments (control, ...},
i.e. what happens when an agent receives $50$ observations
and is expected to take $50$ actions per second instead of $10$. One expects
that decreasing time discretization, or equivalently shortening reaction time,
should only improve agent performances. This is not what happens in practice
for approaches based on estimation of state action value functions, e.g.
\emph{Deep Q-learning} (DQN~\citep{dqn} and \emph{Deep deterministic policy
gradient} (DDPG~\citep{ddpg}). This is shown experimentally in Sec.~\ref{sec:exp}.

We relate this sensitivity to the fact that, as the discretization timestep
decreases, the effect of individual actions on the total return decreases, and
can vanish when the discretization timestep becomes infinitesimal. The natural
framework to study this phenomenon is Continuous time reinforcement learning 
(Sec.~\ref{sec:framework}). 

To cope for this effect, we introduce a
parameterization of the Q-function as a sum of a state value term and a
\emph{small} action dependent advantage term (Sec.~\ref{subsec:reparam}). This
reparameterized Q approximation can be trained with deep variants of Q-learning
(Sec.~\ref{subsec:algorithm}).  To properly scale w.r.t. time discretization,
particular attention has to be taken regarding both the exploration method, and
the scaling of learning rates (Sec.~\ref{subsec:lr}, Sec.~\ref{subsec:explo}).
The resulting algorithm is shown to provide near perfect invariance to time
discretization on simple environments, and much better invariance properties
than vanilla DQN or DDPG on more complex environments (Sec.~\ref{sec:exp}).
\LB{I think I would give less details, especially at the beginning, but insist on the methodology
  (all quantities and algorithms should have meaningful limits when $\deltat \rightarrow 0$)}

\LB{We should say somewhere that more invariance should mean less time spent to find hyperparameters}

\LB{Our main contributions are:
  \begin{itemize}
  \item We show that the Q-function is ill-defined in continuous environments, and thus that
    RL methods based on the Q-function (Q-learning, DDPG) are not robust to variations of time discretization.
  \item We introduce an algorithm (NAME?) resilient to time discretization \TODO{say more}
  \end{itemize}
}


\documentclass[11pt]{article}
\usepackage{amssymb, amsmath}

\newcommand{\deltat}{\delta \hspace{-.1em}t}
\newcommand{\gauss}{\mathcal{N}}
\begin{document}
One can parameterize the advantage function as
\begin{equation}
	A_\phi(s, a) = (\sigma_\phi(s, a) A^m_\phi(s, a) + (1 - \sigma_\phi(s, a)) A^M_\phi(s, a)) \deltat^{\sigma_\phi(s, a)}.
\end{equation}
This way, $A_\phi$ takes macroscopic values when $\sigma_\phi = 0$ and microscopic values when $\sigma_\phi = 1$. The $A^m$ part of the function is used to model \emph{microscopic} values, while $A^M$ is used to model
\emph{macroscopic} values.

The full probabilistic model used is
\begin{equation}
	p(A(s, a) \mid \phi) = \gauss\left(A(s, a)\mid A_\phi(s, a), \sqrt{\deltat^{\sigma_\phi(s, a)}}^2\right).
\end{equation}
Given a dataset of triplets $(s, a, \tilde{A}(s, a))$, the loss on a single datapoint is,
up to an additive constant independant of $\phi$,
\begin{equation}
	\mathcal{L}_\phi = \log{p(\tilde{A}(s, a) \mid \phi)} = -\frac{\left(A_\phi(s, a) - \tilde{A}(s, a)\right)^2}{2 \deltat^{\sigma_\phi(s, a)}} - \frac{\sigma_\phi}{2} \log{\deltat}.
\end{equation}
Splitting $\phi$ into $(\phi^m, \phi^M, \theta)$, with $A^m$ (resp. $A^M$, $\sigma$) depending only on
$\phi^m$ (resp. $\phi^M$, $\theta$), gradients of the loss w.r.t. these parameters are
\begin{align}
	\partial_{\phi^m} \mathcal{L} =& - \sigma_\phi(s, a) \deltat^{\sigma_\phi(s,a)}
	\frac{\delta A_\phi(s, a)}{\deltat^{\sigma_\phi(s, a)}}
	\partial_{\phi^m} A^m_{\phi^m}(s, a)\\
	\partial_{\phi^M} \mathcal{L} =& - (1 - \sigma_\phi(s, a)) \deltat^{\sigma_\phi(s,a)}
	\frac{\delta A_\phi(s, a)}{\deltat^{\sigma_\phi(s, a)}}
	\partial_{\phi^M} A^M_{\phi^M}(s, a)\\
	\partial_{\theta} \mathcal{L} =&  \frac{1}{2}\left(\left(\left(\frac{\delta A_\phi(s, a)\log{\deltat}}{\deltat^{\sigma_\phi(s,a)}} - \left(A^m_\phi(s, a) - A^M_\phi(s, a)\right)\right)\delta A_\phi(s, a) - \log{\deltat}\right)\partial_\theta \sigma_\theta(s, a)\\
\end{align}
with $\delta A_\phi(s, a) = A_\phi(s, a) - \tilde{A}(s, a)$.

Now assume that $\tilde{A}(s, a)$ is or order $\deltat$ with probability roughly $1 - \deltat$, and
of order $1$ with probability $\deltat$. Further assume that $\sigma$ is close to optimal, in the sense
that $\sigma$ is close to $1$ when $\tilde{A}$ is of order $\deltat$, and close to $0$ when $\tilde{A}$
is of order $1$. Under these assumptions, $\delta A_\phi$ is of order $\deltat^\sigma(s, a)$, and the
following orders of magnitudes for the gradients are obtained for $\phi^m$ and $\phi^M$
\begin{align}
	\delta_{\phi^m} \mathcal{L} \approx& \deltat\\
	\delta_{\phi^M} \mathcal{L} \approx& \deltat.
\end{align}
The gradient with respect to $\theta$ could be problematic. When $\deltat$ is small, the recall term
$\log{\deltat} \partial_\theta \sigma_\theta$ dominates, meaning that $\sigma wi

\end{document}

\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage{xcolor}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\newcommand{\deltat}{{\delta t}}
\newcommand{\gauss}{\mathcal{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\TODO}[1]{\textcolor{red}{#1}}
\newcommand{\bigO}[1]{O(#1)}
\newcommand{\Tdo}{TD($0$)}

\title{Scaling continuous reinforcement learning up}
\begin{document}
\maketitle
\begin{abstract}
	Most reinforcement learning algorithms are not resilient to a change of
	time discretization. Even worse, their time continuous limit is often
	ill defined.  While continuous reinforcement methods have been
	introduced in the past, they have been dismissed when scaling up from
	reinforcement learning to deep reinforcement learning. We introduce the
	first deep reinforcement learning that scales when faced with
	diminishing discretization steps.
\end{abstract}
\paragraph{Problem at hand}
Assume a stochastic continuous environment of the form
\begin{align}
	s_{t+\deltat} &= s_t + F(s_t, a_t) \deltat + \gauss(0, C(s_t, a_t)) \sqrt{\deltat}\\
	r(s_t, a_t) &= \tilde{r}(s_t, a_t) \deltat
\end{align}
with $F$ a deterministic drift, and $C$ a noise covariance matrix
associated to each $(s, a)$ pair. Our aim is to find, a policy $\pi$, that is a
(potentially stochastic) mapping from states to actions that maximizes the
average return, i.e.\ the expected average of rewards
\begin{align}
	R^\pi &= \lim\limits_{T\to+\infty}
	\frac{1}{T}
	\E_{s_0, a_0, \ldots, s_T, a_T\sim\pi}\left[
		\sum\limits_{t = 0}^T \tilde{r}(s_t, a_t)\deltat
	\right]
\end{align}
or the discounted sum of future rewards for a physical discount factor of $\gamma$\footnote{
	Having a physical discount factor of $\gamma$ induces a practical discretized discount
	factors of $\gamma^\deltat$. Indeed, \eqref{eq:discounted} is the discretization of
	\begin{equation}
		R^\pi_\gamma = \int_{0}^{+\infty} \gamma^{t} r(t) dt.
		\label{eq:discounted}
	\end{equation}
}
\begin{align}
	R^\pi_\gamma &= \E_{s_0, a_0, \ldots, s_T, a_T \sim \pi}
	\left[
		\sum\limits_{t=0}^{+\infty}
		(\gamma^\deltat)^t \tilde{r}(s_t, a_t)\deltat 
	\right].
\end{align}
We have the following desiderata for our algorithms:
\begin{itemize}
	\item To be resilient to a change in discretization. Precisely, this
		means that increasing the sampling rate should not decrease the
		performances of the algorithm. Notably, this means that if we consider
		two discretization steps $\deltat$ and $\deltat'$, and two associated
		runs of the learning algorithm, the first run should learn in $\frac{1}{\deltat}$
		samples approximately the same thing as the second in $\frac{1}{\deltat'}$ samples.
	\item To be able to handle $\gamma$ coeficients arbitrarily close to $1$.
		Such a condition turns out to be necessary if we require to
		be able to handle constant physical discount factors.
	\item To be offline, i.e.\ to learn a policy that is different from
		the policy used for sampling trajectories. This is useful in
		two ways. First it decorrelates the problem of learning a good
		policy from the problem of exploring the world. Second it
		allows for learning from a fixed dataset of trajectories, or more
		precisely, in the fully observable case, from a dataset of
		states, actions, rewards, next states quadruplets.
\end{itemize}

\paragraph{Assumptions and notations}
Value, value-state and advantage functions as well as there
optimal counterparts are defined as usual, e.g.
\begin{align}
	V^\pi(s) &= \E_{a_0, s_1, \ldots \sim \pi}
	\left[
		\sum\limits_{t=0}^{+\infty}
		(\gamma^\deltat)^t \tilde{r}(s_t, a_t) \deltat
		\mid s=s_0
	\right]\\
	Q^\pi(s, a) &= \E_{a_1, s_1\ldots \sim \pi}
	\left[
		\sum\limits_{t=0}^{+\infty}
		(\gamma^\deltat)^t \tilde{r}(s_t, a_t) \deltat
		\mid s_0=s, a_0=a
	\right]\\
	A^\pi(s,a ) &= Q^\pi(s, a) - V^\pi(s)
\end{align}
and verify Bellman equations, e.g.
\begin{align}
	V^\pi(s) = \E_{a\sim\pi(\cdot\mid s)}
	\left[
		\tilde{r}(s, a) \deltat +
		\gamma^\deltat \E_{s'\mid a} V(s')
	\right].
\end{align}
We assume that for any policy, the environment admits an invariant
measure, that is that there exists a distribution on states $\rho^\pi$ such that for
any regular function $F$ and any infinite sequence $s_0, \ldots$ sampled using policy
$\pi$,
\begin{equation}
	\lim\limits_{T\to+\infty}
	\frac{1}{T}
	\sum\limits_{t=0}^T F(s_t) =
	\E_{s\sim \rho^\pi}\left[
		F(s)
	\right].
	\label{eq:ergodic}
\end{equation}

\paragraph{The Q-function action independant when $\deltat \to 0$}
Q-learning, which uses the optimal Bellman equation on the state value function
to approximate the optimal Q-function is a natural candidate for off-policy
continuous time learning. However, as stated in \TODO{refs}, the state value function
becomes independant of its action parameter as the discretization step decreases towards
$0$. Indeed, approximating to first order in $\deltat$ in the Bellman equation defining
$Q^\pi$, 
\begin{align}
	Q^\pi(s, a) &= \tilde{r}(s, a)\deltat + \gamma^\deltat \E_{s_{t+\deltat} \mid a, s} 
	V^\pi(s_{t+\deltat})\nonumber\\
	&= \tilde{r}(s, a)\deltat + \gamma^\deltat V^\pi(s) + \gamma^\deltat \partial_s V(s) F(s, a) \deltat
	+ \frac{\gamma^\deltat}{2} \text{tr}\left(C(s, a)\partial^2_{s} V(s)\right) \deltat\nonumber\\
	&\to_{\deltat \to 0} V^\pi(s).
\end{align}

Consequently Q-learning is extremely sensitive both to noise and to approximation errors.
To extract the optimal action from an estimate of the optimal Q-function, you need an estimate
whose approximation error is of order $\bigO{\deltat}$.
Typically, when learning the state-action value function only from monte-carlo trajectory samples, 
an optimistic number of timesteps required to learn an estimate with precision $\bigO{\deltat}$ is
$\bigO{\frac{1}{\deltat^3}}$.\footnote{
	Monte-carlo estimates of the Q function have typical variance of order
	$\frac{1}{\deltat\sqrt{2(1 - \gamma)}}$. To obtain an estimate with precision $1$ of the mean
	of a quantity with variance $\sigma^2$ by averaging independant samples, you
	require at least $\sigma^2$ samples.}
This notably means that when $\deltat \to 0$, you would require an infinite number of samples
per infinitesimal timestep to actually learn correct actions. A more formal statement in a simple case
is given in appendix.

\paragraph{The average reward dominates when $\gamma$ gets close to $1$}
\TODO{Make this formal and clear.}\\
Under the ergodic condition eq.~\eqref{eq:ergodic}, $V^\pi(s)$ can be rewritten as
\begin{align}
	V^\pi(s) = \frac{\E_{s\sim\rho^\pi}\tilde{r}}{1 - \gamma} +
	\sigma^\pi(s) + \varepsilon^\pi(s, \gamma)
	\label{eq:dominate}
\end{align}
with $\lim\limits_{\gamma\to 1}\varepsilon^\pi(s, \gamma) = 0$. This means that
when $\gamma$ gets close to $1$, if the average reward under the stationary
policy is non zero, the value function, as well as the state-action value
function, are dominated by a constant term, independant of both the state and
the action. When $\gamma$ is exactly equal to $1$, the constant term becomes
infinite, making all standard algorithms that direcetly learn the value
function or the state-action value function impractical. 

Substracting arbitrary constants to all rewards does not change the relative
performances of policies, and consequently does not change the optimal
policy.\footnote{
	This is only true for non episodic MDPs. For episodic MDPs, substracting
	a constant factor to all rewards only leaves the relative performance of
	policies unchanged if the agent if properly penalized (or rewarded) when
	reaching a terminal state. More details are given in
	\TODO{Probably in the appendix}.
} If one is able to estimate the average reward of the current policy, substracting
the average reward to all rewards negates the first term in eq.~\eqref{eq:dominate}.
The average reward of the current policy is rarely readily available. When learning
on-policy, the average reward can be estimated using running averages
over trajectories\footnote{One must be aware that you cannot use TD(0) on the 
	same trajectory as the one on which you are learning the average reward.
	\TODO{Explain in more details: this creates a dependency of the average
	reward on the current state, which biased learning.}
}. When learning off policy, the average reward cannot be directly learnt from reward
samples along the trajectories.

\paragraph{Centralized advantage updating}
The algorithm introduced hereby attends to both problematics in the following ways.
It uses the advantage updating formulation introduced in~\TODO{ref} to learn
in an offline fashion the advantage function. Contrary to the state-action value
function, the difference of advantage between two actions does not vanish as the
discretization step decreases. Combined with correctly scaled learning rates, this
algorithm proves to be both efficient for high discretization steps, and resilient
to wide changes in the discretization step, even for complex function approximators.
The optimal centralized advantage function and the optimal centralized value
Second, by introducing a proper baseline parameter and adding a normalization term
to the loss, the algorithm provide resilience to gamma values close to $1$, and
equals to $1$ when eq.~\eqref{eq:ergodic} is verified.
\subparagraph{Advantage updating}
Advantage updating, as introduced in~\TODO{ref} relies on the fact that the advantage
function and value function are the only functions that verify
\begin{gather}
	A^*(s, a) = \max\limits_{a'} A^*(s, a') + 
		\tilde{r}(s, a)
		 + 
		\frac{\gamma^\deltat \E_{s'\mid a}\left[
			V^*(s')
		\right] - V^*(s)
		}{
		\deltat
	}\\
	\max\limits_{a} A^*(s, a) = 0.
\end{gather}
These equations can be used to learn approximations of both $A^*$ and $V^*$,
$A_\theta$ and $V_\phi$. To that end, define the following residuals
\begin{gather}
	\delta^A_{\theta, \phi}(s, a) = A_\theta(s, a) -
	\max\limits_{a'} A_\theta(s, a') + 
	\tilde{r}(s, a)
	+ 
	\frac{\gamma^\deltat \E_{s'\mid a}\left[
			V_\phi(s')
		\right] - V_\phi(s)
		}{
		\deltat
	}\\
	\delta^\text{norm}_\theta(s, a) = \max\limits_{a} A_\theta(s, a).
\end{gather}
The mean squared residuals
\begin{align}
	\E_{s \sim \rho^\pi, a \sim \pi}\left[\delta^A_{\theta, \phi}(s, a)^2\right]\\
	\E_{s \sim \rho^\pi, a \sim \pi}\left[\delta^\text{norm}_{\theta}(s, a)^2\right]
\end{align}
admit $V^*$ and $A^*$ as sole global minimizers.

Estimates of the gradients of the first residual with respect to $\phi$ computed only
from trajectories are not readily availables. Indeed, this would require computing two
next states from each state, and this in turn requires a model of the world. Instead,
the approximate \Tdo trick can be used, yielding gradient estimates of the form
\begin{equation}
	g^\phi_{\delta^A_{\theta, \phi}} =
	\frac{1}{\deltat}
	\partial_\phi V_\phi(s) \left(A_\theta(s, a) -
		\max\limits_{a'} A_\theta(s, a') + 
		\tilde{r}(s, a)
		+ 
		\frac{\gamma^\deltat 
			V_\phi(s')
			- V_\phi(s)
			}{
			\deltat
	}\right).
\end{equation}

\subparagraph{Baseline centralization}
As previously mentionned, using a non centralized reward may hurt learning, as the
value function is bound to learn a very large constant which is not useful in the
action selection process.

To centralize rewards, we introduce a constant baseline $b$ the learning process.
Consider the following set of equations on $A^*$, $V^*$ and $b$
\begin{gather}
	A^*(s, a) = \max\limits_{a'} A^*(s, a') + 
		(\tilde{r}(s, a) - b)
		 + 
		\frac{\gamma^\deltat \E_{s'\mid a}\left[
			V^*(s')
		\right] - V^*(s)
		}{
		\deltat
	}\\
	\max\limits_{a} A^*(s, a) = 0\\
	\E_{s\sim \rho^\pi}\left[V^*(s)\right] = 0
	.
\end{gather}
It is unclear at first wether this system of equations admit a solution.
Consider the two first equations. As we only substract a constant $b$ to all
rewards, the only $V^*$ that verifies those equations takes the form
\begin{equation}
	V^*(s) = \frac{\E_{s\sim\rho^*}\tilde{r} - b}{1 - \gamma} +
	\sigma^*(s) + \varepsilon^*(s, \gamma).
\end{equation}
Averaging under $\rho^\pi$, the following is obtained
\begin{equation}
	b = \E_{s\sim\rho^*}\tilde{r} +
	(1 - \gamma) \E_{s\sim\rho^\pi}\left[\sigma^*(s) +
		\varepsilon^*(s, \gamma) +
		V^*(s)
	\right].
\end{equation}
Consequently, for the third equation to hold, $b$ takes the value
\begin{equation}
	b = \E_{s\sim\rho^*}\tilde{r} +
	(1 - \gamma) \E_{s\sim\rho^\pi}\left[\sigma^*(s) +
		\varepsilon^*(s, \gamma)
	\right]
\end{equation}
and notably, as $\gamma$ gets closer to $1$, $b$ gets closer to the
average reward under the optimal policy.

\TODO{More on that later}
To approximately verify the three equations, the corresponding residuals are
minimized over $b$, $\phi$ and $\theta$. Caution should be taken to minimize
the third residual, as the gradient requires sampling of two different states.

\paragraph{Learning rates scalings}
\TODO{More on that later. I am not sure if we should have a $\frac{1}{\deltat}$
	or a $\frac{1}{\deltat^\frac{3}{2}}$ in front of the mean normalization
term. My bet is $\frac{1}{\deltat}$}.
Actual loss we minimize on both $\theta$ and $\phi$
\begin{equation}
	\E_{s \sim \rho^\pi, a \sim \pi}\left[\delta^A_{\theta, \phi}(s, a)^2\right] +
	\E_{s \sim \rho^\pi, a \sim \pi}\left[\delta^\text{norm}_{\theta}(s, a)^2\right] +
	+ \frac{1}{\deltat}\E_{s \sim \rho^\pi}\left[\delta^\text{mean}_{\theta}(s)^2\right].
\end{equation}
Learning rates on $\phi$ and $b$ are of order $\deltat$, learning rate on
$\theta$ is of order $\deltat^2$. This is to have parameters moving by a
distance of order $\deltat$, with at most a $\sqrt{\deltat}$ centered noise per
iteration.

\end{document}

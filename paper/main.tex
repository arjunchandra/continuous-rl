\documentclass[11pt]{article}

\usepackage{amsmath,amssymb}
\usepackage{xcolor}

\newcommand{\deltat}{{\delta t}}
\newcommand{\gauss}{\mathcal{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\TODO}[1]{\textcolor{red}{#1}}
\newcommand{\bigO}[1]{O(#1)}

\title{Scaling continuous reinforcement learning up}
\begin{document}
\maketitle
\begin{abstract}
	Most reinforcement learning algorithms are not resilient to a change of
	time discretization. Even worse, their time continuous limit is often
	ill defined.  While continuous reinforcement methods have been
	introduced in the past, they have been dismissed when scaling up from
	reinforcement learning to deep reinforcement learning. We introduce the
	first deep reinforcement learning that scales when faced with
	diminishing discretization steps.
\end{abstract}
\paragraph{Problem at hand}
Assume a stochastic continuous environment of the form
\begin{align}
	s_{t+\deltat} &= s_t + F(s_t, a_t) \deltat + \gauss(0, C(s_t, a_t)) \sqrt{\deltat}\\
	r(s_t, a_t) &= \tilde{r}(s_t, a_t) \deltat
\end{align}
with $F$ a deterministic drift, and $C$ a noise covariance matrix
associated to each $(s, a)$ pair. Our aim is to find, a policy $\pi$, that is a
(potentially stochastic) mapping from states to actions that maximizes the
average return, i.e.\ the expected average of rewards
\begin{align}
	R^\pi &= \lim\limits_{T\to+\infty}
	\frac{1}{T}
	\E_{s_0, a_0, \ldots, s_T, a_T\sim\pi}\left[
		\sum\limits_{t = 0}^T \tilde{r}(s_t, a_t)\deltat
	\right]
\end{align}
or the discounted sum of future rewards for a physical discount factor of $\gamma$\footnote{
	Having a physical discount factor of $\gamma$ induces a practical discretized discount
	factors of $\gamma^\deltat$. Indeed, \eqref{eq:discounted} is the discretization of
	\begin{equation}
		R^\pi_\gamma = \int_{0}^{+\infty} \gamma^{t} r(t) dt.
		\label{eq:discounted}
	\end{equation}
}
\begin{align}
	R^\pi_\gamma &= \E_{s_0, a_0, \ldots, s_T, a_T \sim \pi}
	\left[
		\sum\limits_{t=0}^{+\infty}
		(\gamma^\deltat)^t \tilde{r}(s_t, a_t)\deltat 
	\right].
\end{align}
We have the following desiderata for our algorithms:
\begin{itemize}
	\item To be resilient to a change in discretization. Precisely, this
		means that increasing the sampling rate should not decrease the
		performances of the algorithm. Notably, this means that if we consider
		two discretization steps $\deltat$ and $\deltat'$, and two associated
		runs of the learning algorithm, the first run should learn in $\frac{1}{\deltat}$
		samples approximately the same thing as the second in $\frac{1}{\deltat'}$ samples.
	\item To be able to handle $\gamma$ coeficients arbitrarily close to $1$.
		Such a condition turns out to be necessary if we require to
		be able to handle constant physical discount factors.
	\item To be offline, i.e.\ to learn a policy that is different from
		the policy used for sampling trajectories. This is useful in
		two ways. First it decorrelates the problem of learning a good
		policy from the problem of exploring the world. Second it
		allows for learning from a fixed dataset of trajectories, or more
		precisely, in the fully observable case, from a dataset of
		states, actions, rewards, next states quadruplets.
\end{itemize}

\paragraph{Assumptions and notations}
Value, value-state and advantage functions as well as there
optimal counterparts are defined as usual, e.g.
\begin{align}
	V^\pi(s) &= \E_{a_0, s_1, \ldots \sim \pi}
	\left[
		\sum\limits_{t=0}^{+\infty}
		(\gamma^\deltat)^t \tilde{r}(s_t, a_t) \deltat
		\mid s=s_0
	\right]\\
	Q^\pi(s, a) &= \E_{a_1, s_1\ldots \sim \pi}
	\left[
		\sum\limits_{t=0}^{+\infty}
		(\gamma^\deltat)^t \tilde{r}(s_t, a_t) \deltat
		\mid s_0=s, a_0=a
	\right]\\
	A^\pi(s,a ) &= Q^\pi(s, a) - V^\pi(s)
\end{align}
and verify Bellman equations, e.g.
\begin{align}
	V^\pi(s) = \E_{a\sim\pi(\cdot\mid s)}
	\left[
		\tilde{r}(s, a) \deltat +
		\gamma^\deltat \E_{s'\mid a} V(s')
	\right].
\end{align}
We assume that for any policy, the environment admits an invariant
measure, that is that there exists a distribution on states $\rho^\pi$ such that for
any regular function $F$ and any infinite sequence $s_0, \ldots$ sampled using policy
$\pi$,
\begin{equation}
	\lim\limits_{T\to+\infty}
	\frac{1}{T}
	\sum\limits_{t=0}^T F(s_t) =
	\E_{s\sim \rho^\pi}\left[
		F(s)
	\right].
\end{equation}

\paragraph{The Q-function action independant when $\deltat \to 0$}
Q-learning, which uses the optimal Bellman equation on the state value function
to approximate the optimal Q-function is a natural candidate for off-policy
continuous time learning. However, as stated in \TODO{refs}, the state value function
becomes independant of its action parameter as the discretization step decreases towards
$0$. Indeed, approximating to first order in $\deltat$ in the Bellman equation defining
$Q^\pi$, 
\begin{align}
	Q^\pi(s, a) &= \tilde{r}(s, a)\deltat + \gamma^\deltat \E_{s_{t+\deltat} \mid a, s} 
	V^\pi(s_{t+\deltat})\nonumber\\
	&= \tilde{r}(s, a)\deltat + \gamma^\deltat V^\pi(s) + \gamma^\deltat \partial_s V(s) F(s, a) \deltat
	+ \frac{\gamma^\deltat}{2} \text{tr}\left(C(s, a)\partial^2_{s} V(s)\right) \deltat\nonumber\\
	&\to\limits_{\deltat \to 0} V^\pi(s).
\end{align}

Consequently Q-learning is extremely sensitive both to noise and to approximation errors.
To extract the optimal action from an estimate of the optimal Q-function, you need an estimate
whose approximation error is of order $\bigO{\deltat}$.
Typically, when learning the state-value function only from monte-carlo trajectory samples, 
an optimistic number of timesteps required to learn an estimate with precision $\bigO{\deltat}$ is
$\bigO{\frac{1}{\deltat^2}}$.\footnote{
	Monte-carlo estimates of the Q function have typical variance of order
	$\frac{1}{\deltat\sqrt{2(1 - \gamma)}}$. To obtain an estimate with precision $1$ of the mean
	of a quantity with variance $\sigma^2$ by averaging independant samples, you
	require at least $\sigma^2$ samples.}
This notably means that when $\deltat \to 0$, you would require an infinite number of samples
per infinitesimal timestep to actually learn correct actions.

\paragraph{The average reward dominates when $\gamma$ gets close to $1$}
\TODO{Make this formal and clear.}\\
When the time horizon is unbounded, $V^\pi(s)$ can be rewritten as
\begin{align}
	V^\pi(s) = \frac{\E_{s\sim\rho^\pi}\tilde{r}}{1 - \gamma} +
	\sigma^\pi(s) + \varepsilon^\pi(s, \gamma)
\end{align}
with $\lim\limits_{\gamma\to 1} = 0$. This means that when $\gamma$ gets close to $1$,
if $\rho^\pi(s)$ is non-zero, the 
\paragraph{Centralized advantage updating}
The optimal centralized advantage function and the optimal centralized value
function verify the following Bellman equation
\begin{gather}
	A^*(s, a) = \max\limits_{a'} A^*(s, a') + 
		(\tilde{r}(s, a) - \E^*\left[
			\tilde{r}
		\right]
		) + 
		\frac{\gamma \E_{s'\mid a}\left[
			V^*(s')
		\right] - V^*(s)
		}{
		\deltat
	}\\
	\max\limits_{a} A^*(s, a) = 0
\end{gather}

\paragraph{Approximate advantage updating and learning rates scalings}
\end{document}

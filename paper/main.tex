\documentclass[11pt]{article}

\usepackage{amsmath,amssymb}

\newcommand{\deltat}{{\delta t}}
\newcommand{\gauss}{\mathcal{N}}
\newcommand{\E}{\mathbb{E}}

\title{Scaling continuous reinforcement learning up}
\begin{document}
\maketitle
\begin{abstract}
	Most reinforcement learning algorithms are not resilient to a change of
	time discretization. Even worse, their time continuous limit is often
	ill defined.  While continuous reinforcement methods have been
	introduced in the past, they have been dismissed when scaling up from
	reinforcement learning to deep reinforcement learning. We introduce the
	first deep reinforcement learning that scales when faced with
	diminishing discretization steps.
\end{abstract}
\paragraph{Problem at hand}
Assume a stochastic continuous environment of the form
\begin{align}
	s_{t+\deltat} &= s_t + F(s_t, a_t) \deltat + \gauss(0, C(s_t, a_t)) \sqrt{\deltat}\\
	r(s_t, a_t) &= \tilde{r}(s_t, a_t) \deltat
\end{align}
with $F$ a deterministic drift, and $C$ a noise covariance matrix
associated to each $(s, a)$ pair. Our aim is to find, a policy $\pi$, that is a
(potentially stochastic) mapping from states to actions that maximizes the
average return, i.e.\ the expected average of rewards
\begin{align}
	R^\pi &= \lim\limits_{T\to+\infty}
	\frac{1}{T}
	\E_{s_0, a_0, \ldots, s_T, a_T\sim\pi}\left[
		\sum\limits_{t = 0}^T \tilde{r}(s_t, a_t)\deltat
	\right]
\end{align}
or the discounted sum of future rewards for a physical discount factor of $\gamma$\footnote{
	Having a physical discount factor of $\gamma$ induces a practical discretized discount
	factors of $\gamma^\deltat$. Indeed, \eqref{eq:discounted} is the discretization of
	\begin{equation}
		R^\pi_\gamma = \int_{0}^{+\infty} \gamma^{t} r(t) dt.
		\label{eq:discounted}
	\end{equation}
}
\begin{align}
	R^\pi_\gamma &= \E_{s_0, a_0, \ldots, s_T, a_T \sim \pi}
	\left[
		\sum\limits_{t=0}^{+\infty}
		(\gamma^\deltat)^t \tilde{r}(s_t, a_t)\deltat 
	\right].
\end{align}
We have the following desiderata for our algorithms:
\begin{itemize}
	\item To be resilient to a change in discretization. Precisely, this
		means that increasing the sampling rate should not decrease the
		performances of the algorithm. Notably, this means that if we consider
		two discretization steps $\deltat$ and $\deltat'$, and two associated
		runs of the learning algorithm, the first run should learn in $\frac{1}{\deltat}$
		samples approximately the same thing as the second in $\frac{1}{\deltat'}$ samples.
	\item To be able to handle $\gamma$ coeficients arbitrarily close to $1$.
		Such a condition turns out to be necessary if we require to
		be able to handle constant physical discount factors.
	\item To be offline, i.e.\ to learn a policy that is different from
		the policy used for sampling trajectories. This is useful in
		two ways. First it decorrelates the problem of learning a good
		policy from the problem of exploring the world. Second it
		allows for learning from a fixed dataset of trajectories, or more
		precisely, in the fully observable case, from a dataset of
		states, actions, rewards, next states quadruplets.
\end{itemize}

\paragraph{Assumptions and notations}
Value, value-state and advantage functions as well as there
optimal counterparts are defined as usual, e.g.
\begin{align}
	V^\pi(s) &= \E_{a_0, s_1, \ldots \sim \pi}
	\left[
		\sum\limits_{t=0}^{+\infty}
		(\gamma^\deltat)^t \tilde{r}(s_t, a_t) \deltat
		\mid s=s_0
	\right]\\
	Q^\pi(s, a) &= \E_{a_1, s_1\ldots \sim \pi}
	\left[
		\sum\limits_{t=0}^{+\infty}
		(\gamma^\deltat)^t \tilde{r}(s_t, a_t) \deltat
		\mid s_0=s, a_0=a
	\right]
\end{align}
and verify Bellman equations, e.g.
\begin{align}
	V^\pi(s) = \E_{a\sim\pi(\cdot\mid s)}
	\left[
		\tilde{r}(s, a) \deltat +
		\gamma^\deltat \E_{s'\mid a} V(s')
	\right]
\end{align}
We assume that for any policy, the environment admits an invariant
measure, that is that there exists a distribution on states $\rho^\pi$ such that for
any regular function $F$ and any infinite sequence $s_0, \ldots$ sampled using policy
$\pi$,
\begin{equation}
	\lim\limits_{T\to+\infty}
	\frac{1}{T}
	\sum\limits_{t=0}^T F(s_t) =
	\E_{s\sim \rho^\pi}\left[
		F(s)
	\right].
\end{equation}
\paragraph{Centralized advantage updating}
The optimal centralized advantage function and the optimal centralized value
function verify the following Bellman equation
\begin{gather}
	A^*(s, a) = \max\limits_{a'} A^*(s, a') + 
		(\tilde{r}(s, a) - \E^*\left[
			\tilde{r}
		\right]
		) + 
		\frac{\gamma \E_{s'\mid a}\left[
			V^*(s')
		\right] - V^*(s)
		}{
		\deltat
	}\\
	\max\limits_{a} A^*(s, a) = 0
\end{gather}

\paragraph{Approximate advantage updating and learning rates scalings}
\end{document}

\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage{xcolor}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\newcommand{\deltat}{{\delta t}}
\newcommand{\gauss}{\mathcal{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\TODO}[1]{\textcolor{red}{#1}}
\newcommand{\bigO}[1]{O(#1)}
\newcommand{\Tdo}{TD($0$)}
\newcommand{\tdres}{\delta_\text{TD}}

\newcommand{\deq}{\mathrel{\mathop{:}}=}
\newcommand{\eqd}{=\mathrel{\mathop{:}}}


\title{Baseline centering problem}
\begin{document}
\maketitle

Let $\pi$ be any policy (or set of observed transitions) and let
$\rho^\pi$ be the associated distribution on states.

Let us start again from  the equations on $A$, $V$ and $b$:
\begin{gather}
	A(s, a) = \max\limits_{a'} A(s, a') + 
		(r(s, a) - b)
		 + 
		\gamma \E_{s'\mid s,a}\left[
			V(s')
		\right] - V(s)
	\\
	\max\limits_{a} A(s, a) = 0\\
	\E_{s\sim \rho^\pi}\left[V(s)\right] = 0
	.
\end{gather}

Let us prove existence and uniqueness of the solution over $(V,b)$ for a
given function $A$. Moreover, let us prove that when $\gamma\to 1$, $b$
tends to the average reward under the policy derived from $A$.

\paragraph{Existence of a solution.}

Let $\pi_A$ be the (or any in case of ties) policy described by the advantage function $A$,
namely, that returns an action $a^\ast$ that maximizes $A(s,a)$.

Applying the equation to $a^\ast$ we find
\begin{equation}
\label{eq:optbellman}
0=r(s,a^\ast)-b+\gamma \E_{s'|s,a^\ast} V(s')-V(s)
\end{equation}

This is the Bellman equation for the policy $\pi_A$ with rewards $r-b$.
Shifting rewards by $b$ shifts value functions by $b/(1-\gamma)$. So
the unique solution is
\begin{equation}
V(s)=V^{\pi_A}(s)-\frac{b}{1-\gamma}
\end{equation}
and averaging under the current empirical policy $\pi$, the only $b$ that
satisfies
$\E_{\rho^\pi} V=0$ is
\begin{equation}
b=(1-\gamma)\E_{s\sim \rho^\pi} V^{\pi_A}(s)
\end{equation}

Conversely, defining 
\begin{equation}
V(s)\deq V^{\pi_A}(s)-\E_{s\sim \rho^\pi} V^{\pi_A}(s),\qquad
b\deq (1-\gamma)\E_{s\sim \rho^\pi} V^{\pi_A}(s)
\end{equation}
these functions satisfy
both the equation $\E_{\rho^\pi} V=0$ (by construction) and the Bellman equation for
every pair $(s,a^\ast)$, that is, the Bellman equation for $\pi_A$.

Thus, interestingly, the system provides a well-defined solution for $V$
and $b$ even if $A$ is not perfect (namely, the value function associated
with $\pi_A$). This provides some numerical protection.

[TODO: this assumes we only apply Bellman to $(s,a^\ast)$ not every
$(s,a)$! SHOULD WE RESTRICT UPDATES OF $V$ and $b$ TO SUCH PAIRS? Using
Bellman for $a\neq a^\ast$ might just pollute $V$ and $b$. Eg, if using
eps-greedy, don't update $V$ when exploring, only update $A$?]

TODO Satisfying Bellman for every pair $(s,a)$ instead of just $(s,a^\ast)$
tells that $A$ is indeed the advantage function of $\pi_A$.

TODO this still works if we have any heuristic procedure that returns a
would-be argmax of $A$ instead of the true argmax!

TODO Should we multiply the learning rate of $b$ by $1-\gamma$ if we want
a well-defined algorithm when $\gamma\to 1$? I don't think it's needed: in
Bellman, thanks to $\gamma V(s')-V(s)$, the homogeneity of $V$ is
$(1-\gamma)$. So even if $V$ is initialized to a large value such as
$V^{\pi_A}$ it should be fine.

\paragraph{The value of $b$ when $\gamma\to 1$.}
We know that when $\gamma\to 1$, the value function for $\pi_A$ takes the form
\begin{equation}
\label{eq:reladv}
	V^{\pi_A}(s) = \frac{\E_{(s,a^\ast)\sim\rho^{\pi_A}}\,r(s,a^\ast) }{1 - \gamma} +
	U(s) + O(1-\gamma)
\end{equation}
where $U(s)$ is the relative advantage function of the policy $\pi^A$.
The function $U$
is bounded when $\gamma\to 1$.

Since $b=(1-\gamma)\E_{\rho^\pi} V^{\pi_A}$ we just find
\begin{equation}
b=\E_{(s,a^\ast)\sim\rho^{\pi_A}}\,r(s,a^\ast)+O(1-\gamma)
\end{equation}
namely, $b$ is the average reward under policy $\pi^A$.

\paragraph{Convergence of $V$ given $b$.} For robustness analysis, we now
derive what happens to one variable when the other is wrong. Let us first
study convergence of $V$ given $b$. There is a subtlety with the
respective strength of the Bellman objective and the $\E V=0$ objective.
This is because TD is not a gradient and behaves badly when added to the
gradient of another loss.

The loss corresponding to $\E_{\rho^\pi}V=0$ is $\frac12
(\E_{\rho^\pi}V)^2$. We can estimate its gradient as $(\E_{\rho^\pi}\partial_\theta
V)(\E_{\rho^\pi}V)$ where the two expectations require two independent
samples from the current policy, eg, sampling twice from the training
dataset.

Assume that we learn $V$ via TD, to which we add $\alpha$ times the
centering objective on $V$. Adding the TD step and the centering gradient
step, the average step is
\begin{equation}
\E_{(s,a)\sim\rho^\pi}\partial_\theta
V(s)(A(s,a^\ast)-A(s,a)+r(s,a)-b+\gamma \E_{s'|s,a}V(s')-V(s) - \alpha
\E_{\rho^\pi}V)
\end{equation}
which corresponds to using TD to solve the modified Bellman equation
with rewards $A(s,a^\ast)-A(s,a)+r(s,a)-b-\alpha \E_{\rho^\pi}V$. The
solution is... TODO

\paragraph{Only $\alpha=1$, i.e. penalty $\frac12
(\E_{\rho^\pi}V)^2$, works for balancing the constraint on $\E V$
and TD on $V$.}

Let us consider the case when we just train $V$ on pairs $(s,a^\ast)$.
Then the combined TD+centering step on $V$, with centering loss
$\frac{\alpha}{2} (\E_{\rho^\pi}V)^2$, is
\begin{equation}
\E_{s\sim\rho^\pi}\partial_\theta
V(s)(r(s,a^\ast)-b+\gamma \E_{s'|s,a^\ast}V(s')-V(s) - \alpha
\E_{\rho^\pi}V)
\end{equation}

This is equivalent to using TD
to solve the modified Bellman equation with
rewards $r(s,a)-b-\alpha \E_{\rho^\pi}V$. Thus we are trying to solve
$r(s,a)-b-\alpha \E_{\rho^\pi}V+\gamma PV-V=0$, and the only fixed point
of this gradient step is
\begin{equation}
V(s)=V^{\pi_A}(s)-\frac{b+\E_{\rho^\pi} V^{\pi_A}}{1+\alpha-\gamma}
\end{equation}
% which looks different from the solution
% \begin{equation}
% V(s)=V^{\pi_A}(s)-\frac{b}{1-\gamma}
% \end{equation}
% considered above.
Meanwhile, $b$ is trained to minimize $\E_{s\sim \rho^\pi}
(r(s,a^\ast)-b+\gamma \E_{s'|s,a^\ast}V(s')-V(s))^2$ whose unique
solution given $V$ is TODO WRONG I THINK WE HAVE A DOUBLE SAMPLING
PROBLEM FOR $b$!!!
\begin{equation}
b=\E_{s\sim \rho^\pi}(r(s,a^\ast)+\gamma \E_{s'|s,a^\ast}V(s')-V(s))
\end{equation}
and substituting the fixed point
$V(s)=V^{\pi_A}(s)-\frac{b+\E V^{\pi_A}}{1-\gamma+\alpha}$ we find
\begin{equation}
b=-(\gamma-1) \frac{b+\E V^{\pi_A}}{1-\gamma+\alpha}
\end{equation}
which yields
\begin{equation}
b=\frac{1-\gamma}{\alpha} \E V^{\pi_A}
\end{equation}
contrary to $b=(1-\gamma)\E V^{\pi_A}$ we had before. 
This translates back to
\begin{equation}
V(s)=V^{\pi_A}(s)-\frac{b+\E_{\rho^\pi} V^{\pi_A}}{1+\alpha-\gamma}
=V^{\pi_A}(s)-\frac1\alpha \E_{\rho^\pi} V^{\pi_A}
\end{equation}
with this value of $b$.

The constraint $\E V=0$ is satisfied if and only if $\alpha=1$.

In conclusion, when $\alpha=1$, everything coincides with the desired
solution; however for $\alpha\neq 1$ this is not the case.

\paragraph{A variant to make the learning of $A$ and $b$ insensitive to
translations of $V$, and the dynamics independent from
$\alpha_3/\alpha_5$.} Consider the modified off-policy Bellman equation
for all states $s$,
\begin{equation}
0=\E \left(
r(s,a)-b+\gamma V(s')-V(s)+(1-\gamma)V(\tilde s)
\right)
\end{equation}
where $a$ is from some policy $\pi$, $s'$ is the next state given $s$ and
$a$, and $\tilde s$ is a random state taken from any empirical
distribution $\rho$. Minimize this by TD with $s$ taken from the same
empirical distribution $\rho$, adding a gradient term to minimize
$\frac{\alpha}{2} (\E_\rho V)^2$ as before.

The TD step is
\begin{equation}
\E_s \,\partial_\theta V(s)\left(
r(s,a)-b+\gamma V(s')-V(s)+(1-\gamma)V(\tilde s)-\alpha V(s)
\right)
\end{equation}
and its fixed point solves $r(s,a)-b+\gamma V(s')-V(s)+(1-\gamma)V(\tilde
s)-\alpha V(s)$ for all $s$. The unique solution for $V$ given $b$ is
\begin{equation}
V(s)=V^\pi(s)-\E_\rho V^\pi-\frac{b-(1-\gamma)\E_\rho V^\pi}{\alpha}
\end{equation}

Meanwhile, $b$ is trained to minimize $\E_\rho (\E)$


\end{document}

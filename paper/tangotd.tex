\documentclass[11pt,a4paper]{article}
\input{header.en.tex}

\newcommand{\Dir}{\mathcal{D}}


\begin{document}

\section{Recovering known convergence results for TD, and a few ideas}

We consider the Bellman equation for a policy $\pi$ in a finite MDP with
expected reward $R$ and transition probability matrix $P$ ($P$ includes $\pi$ and
the environment response). The Bellman equation is
\begin{equation}
(\Id-P)V=R
\end{equation}

We assume we can sample transitions $(s,a,r,s')$ according to some
distribution $\rho$ on $s$. In the on-policy case, $\rho$ will be the
stationary distribution. Typically $a$ has to be taken from $\pi$ (or a
distribution close to $\pi$ after resampling/by using only those $a$
sampled from $\pi$; this covers $\eps$-greedy) so that we do get
transitions from
$P$. (Alternatively one could work with pairs $(s,a)$ TODO.) Thus, we
have access to unbiased samples of $\rho P$ and $\rho R$:
\begin{equation}
\rho P=\E[\1_{s}\transp{\1_{s'}}\,],\qquad \rho R=\E[r(s,a,s')\1_{s}]
\end{equation}
and thus also to samples of $\rho(\Id-P)$. Thus, rewriting the equation
as
\begin{equation}
\rho(\Id-P)V=\rho R
\end{equation}
(equivalent if $\rho$ is positive), both sides are \emph{samplable}, and we can try to solve the above by
stochastic gradient-like methods.

For instance, if we parameterize $V$ by some parameter $\theta$, we can apply
parametric TD. The expected TD step is
\begin{equation}
\theta\gets \theta -\eta \,\transp{\Phi}\rho (V-PV-R)
\end{equation}
where 
\begin{equation}
\Phi\deq \frac{\partial V}{\partial \theta}
\end{equation}
is the Jacobian matrix of $V$ with respect to its parameter. The linear
case $V=\Phi \theta$ corresponds to constant $\Phi$. The tabular case is
$\Phi=\Id$.

A fixed point of TD solves
\begin{equation}
\transp{\Phi}\rho(\Id-P)V=\transp{\Phi}\rho R
\end{equation}
and if the system is overparameterized ($\Phi$ invertible) and
well-sampled ($\rho>0$) this implies $(\Id-P)V=R$. All terms in this
equation are \emph{samplable}: we can sample from $\rho R$ and from
$\rho(\Id-P)$.


Thus, from now on we are going to focus on solving
\begin{equation}
\transp{\Phi}\rho (\Id-P)V=\transp{\Phi}\rho R
\end{equation}
using various SGD-like solvers.

We now switch to the linear case and will de-linearize later. This
becomes
\begin{equation}
\transp{\Phi}\rho (\Id-P)\Phi \theta=\transp{\Phi}\rho R
\end{equation}
to be solved in $\theta$.

\paragraph{Solving $A\theta=b$ where $A$ and $b$ are samplable.} An
obvious way to solve $A\theta=b$ via a stochastic gradient method is
\begin{equation}
\theta\gets \theta - \eta (A\theta-b)
\end{equation}
using samples for $A$ and $b$: the only fixed point solves $A\theta=b$.
TD does exactly this, 
with $A=\transp{\Phi}\rho(\Id-P)\Phi$ and $b=\transp{\Phi}\rho R$.

If $A$ is symmetric positive definite, this is also the
gradient descent of the loss function
$\transp{(A\theta-b)}A^{-1}(A\theta-b)$. However, we want to apply this
to matrices $A$ that are not necessarily of this type.

Under this update for $\theta$, the error $\theta-A^{-1}b$ is updated as
\begin{equation}
\theta-A^{-1}b \gets (\Id-\eta A)(\theta-A^{-1}b)
\end{equation}

This converges if only if $A$ is \emph{stable}.


\begin{defi}
A matrix $A$ is \emph{stable} if one of the following equivalent
conditions are satisfied:
\begin{itemize}
\item All the eigenvalues of $A$ have positive real part.
\item The matrix $(\Id-\eta A)$ has spectral radius $<1$ for small enough
$\eta$.
\item For $\eta$ small enough, iterating $\theta\gets (\Id-\eta A)\theta$
converges to $0$.
\item The differential equation $\theta'=-A\theta$ converges to $0$ for
any initial value.
\item There exists a symmetric, positive definite matrix $L$ (Lyapunov
function) such that $\transp{\theta}L\theta$ is decreasing along the
solutions of the differential equation $\theta'=-A\theta$.
\item There exists a symmetric, positive definite matrix $L$ such that
\begin{equation}
\transp{\theta} L A \theta>0
\end{equation}
for all $\theta\neq 0$. (This is the same $L$ as in the previous
condition.)
\end{itemize}
\end{defi}

In particular, a symmetric, definite positive matrix is stable (last
criterion with $L=\Id$).
Stability is invariant by matrix similarity $A\gets B^{-1}A B$, since
this preserves eigenvalues.

Since the
solution of $\theta_t'=-A\theta_t$ is $\theta_t=e^{-tA}\theta_0$, a
Lyapunov function that works is $\transp{\theta_0}L\theta_0\deq \int_{t \geq 0}
\norm{\theta_t}^2$. Indeed this is decreasing, because
$\transp{\theta_t}L\theta_t$ is just the same integral starting at $t$
instead of $0$. Explicitly this is
$L=\int_{t\geq 0} \transp{(e^{-t A})}
e^{-tA}$.

\paragraph{Making the TD iteration stable.} In linear TD the matrix $A$ is
\begin{equation}
A\deq \transp{\Phi}\rho (\Id-P)\Phi
\end{equation}
and we want to solve
\begin{equation}
A\theta=b\,\qquad b\deq \transp{\Phi}\rho R
\end{equation}
If $A$ is stable, then iterating $\theta\gets \theta -\eta(A\theta-b)$ will
converge.

We know $A$ is stable in several cases: (TODO here I just ignore constant
functions)
\begin{enumerate}
\item On-policy: $\rho=\mu$, the invariant distribution. Then we know that $\rho(\Id-P)$ is positive
(Dirichlet form). This implies that $A=\transp{\Phi}\rho (\Id-P)\Phi$ is
positive too. So we can take $L=\Id$ and we recover convergence of linear
TD on-policy.
\item In the tabular case: $\Phi=\Id$. Indeed we have $A=\rho (\Id-P)$,
which is not positive in general. But $\mu (\Id-P)$ is positive, so we can take
$L=\mu/\rho$ so that $LA$ is positive. Thus, tabular TD works
off-policy.
\end{enumerate}

One can also \emph{precondition} the gradient descent. This can make it
stable and/or make convergence faster. The step becomes
\begin{equation}
\theta\gets \theta-\eta B(A\theta-b)
\end{equation}
for some matrix $B$. Some choices of $B$ correspond to natural choices of
loss functions on $\theta$, as we will see.

Of course the values of $B$ that are most interesting are obtained as the
inverse of something; for instance $B=A^{-1}$ solves the problem
instantly... Two-timescale methods or TANGO-like algorithms can apply an
inverse of something to $A\theta-b$, via an auxiliary gradient descent.
TANGO works for stable matrices, if they are samplable \footnote{In
TANGO, the consecutive steps absolutely have to be \emph{independent}. So even in
the on-policy case, we will need a replay buffer from $\mu$ for the auxiliary
gradient descent.}

A few choices are:
\begin{itemize}
\item GTD is a gradient descent of
$\transp{(A\theta-b)}(A\theta-b)$ (Sutton 2009).
This corresponds to $B=\transp{A}$. Then
$BA=\transp{A}A$ is
obviously stable. This helps with $A$ being non-symmetric.

TODO: check if
GTD uses the same sample for $\transp{A}$ and $A$. If so, it will
converge to a different stable point. TODO: Can be applied TANGO-style
(by formally inverting $\Id$ in TANGO), advantage=fewer variance due to
two-sampling (Is this equivalent to eligibility traces without the
variance??). (TANGO uses two-sampling but on a sum of past terms, so
that a past term gets TANGOified whenever we happen to reach a state that
activates the same features in the future, which is probably fine). Can
also be TANGOified with $\gamma\to 0$ (two-timescale) which now requires
only single sampling (no replay buffers).

\item $B=A^{-1}$: obviously stable ($BA=\Id$), jumps to the solution in one step if
$\eta=1$. Requires computing the inverse of $A$, namely, solving the
problem by first solving the problem. But if $A$ is stable and samplable,
this can be done via TANGO. More on that later. This should speed up
things in situations where $A$ is stable in the first place (eg.,
on-policy).

\item $B=(\transp{\Phi}\rho\Phi)^{-1}$. This amounts to orthonormalizing
the basis before applying TD. In general, this is not stable off-policy: take the
standard two-point counterexample with a single $\Phi=\transp{(1,2)}$,
orthonormalizing $\Phi$ is not going to help as $\transp{\Phi}\rho\Phi$
is just a number.

Still, this is stable in two cases:
on-policy (take $L=B^{-1}$, given that $A$ is stable on-policy), or
assuming overparameterization ($\Phi$ invertible: then we can take
$L=\transp{\Phi}\mu \Phi$ so that $LBA=\transp{\Phi}\mu (\Id-P)\Phi$
which is positive because $\mu(\Id-P)$ is positive; essentially, if
$\Phi$ is invertible we are back to the tabular case.)
% Then we have to prove th
% $(\transp{\Phi}\Phi)^{-1}\transp{\Phi}\rho(\Id-P)\Phi$ is stable. Taking
% $L=\transp{\Phi}(\mu/\rho)\Phi$, and computing
% \begin{align}
% \transp{\theta}LBA\theta&=
% \transp{\theta}L(\transp{\Phi}\rho\Phi)^{-1}\transp{\Phi}\rho(\Id-P)\Phi\theta
% \end{align}
% and note that $(\transp{\Phi}\rho\Phi)^{-1}\transp{\Phi}\rho$ is the
% solution of linear regression with design matrix $\Phi$ in $L^2(\rho)$.
% This implies that
% \begin{equation}
% \Phi(\transp{\Phi}\rho\Phi)^{-1}\transp{\Phi}\rho=\Pi_\Phi
% \end{equation}
% where $\Pi$ is the orthogonal projector on the image of $\Phi$ in
% $L^2(\rho)$. Now taking
% \begin{equation}
% L=\transp{\Phi}(\mu/\rho)\Phi
% \end{equation}
% we have
% \begin{equation}
% \transp{\theta}LBA\theta=\transp{\theta} \transp{\Phi}(\mu/\rho)\Pi_\Phi
% (\Id-P)\Phi\theta
% \end{equation}
% TODO: on-policy or overparameterized

\item The choice from Sutton 2009 "TDC" corresponds to a gradient descent
of the projected norm of the Bellman gap on the subspace generated by
$\Phi$, projected in $L^2(\rho)$. This is GTD with $L^2(\rho)$-orthonormalization of
$\Phi$, namely,
\begin{equation}
B=\transp{A} (\transp{\Phi}\rho\Phi)^{-1}=\transp{(\transp{\Phi}\rho(\Id-P)\Phi)}(\transp{\Phi}\rho\Phi)^{-1}
\end{equation}
from which it is clear that $BA$ is symmetric and stable. This is obtained as a
gradient descent of the projected norm of the Bellman gap on the image of
$\Phi$, projected in $L^2(\rho)$. TODO: TDC and GTD2 differ by using that
$A$ has $(\Id-P)$ on  the right, and expanding the $\Id$ term and the $P$
term separately, which results in simplifications and make TDC much
closer to TD and less reliant on the inversion.

\item The choice $B=A^{-1}$ can also be written as
$B=\transp{A}(A\transp{A})^{-1}$. The advantage is that we now have to
invert a stable matrix, so, better behaved with TANGO. However, variance
will be terrible: this requires four independent samples from $A$.

\item A gradient descent of
$\transp{(A\theta-b)}(A+\transp{A})^{-1}(A\theta-b)$, corresponding to
$B=\transp{A}(A+\transp{A})^{-1}$. This is probably the closest to
unmodified TD in behavior: for instance when $A$ is symmetric ($\rho=\mu$
and $P$ reversible) this coincides with TD. $A+\transp{A}$ corresponds to
the Dirichlet form for $\rho$, namely, $\E_{\rho} (f(s)-f(s'))^2$ TODO
CHECK.

\end{itemize}

Only $B=A^{-1}$ and $B=\transp{A}(A\transp{A})^{-1}$ are truly
second-order (namely, solve the problem in one step in the non-stochastic
case).

The other algorithms involving an inverse basically have the cost of
second-order algorithms without the full benefits.  They are second-order
in feature-space (they orthonormalize $\Phi$) but not in Markov process
space (they don't backpropagate rewards).

TODO sampling: if $A=\E \tilde A$, can do $\transp{(\E \tilde A)}(\E
\tilde A)$ or
$\E(\transp{\tilde A}\tilde A)$, which is not equal to $\transp{A}A$ but
is still definite positive (hence stable). This would avoid some
quadruple-sampling problems with $B=\transp{A}(A\transp{A})^{-1}$. But
breaks second-order correctness?

TODO: define second-order with respect to $\Phi$ (second-order
provided $\transp{\Delta}\Delta=\Id$, eg in a Fourier frequency-adjusted
basis), with respect to the Markov chain (second-order provided
$\transp{\Phi}\Phi=\Id$).

Almost-one-timescale (such as TANGO) needs more independence than two-timescale. I
\emph{think} that two-timescale can deal with taking the same sample for
$A$ and $\transp{A}$ in $\transp{A}(...)^{-1}A$, and taking correlated
consecutive samples (eg, online) for the matrix to be inverted. I think
TANGO would need uncorrelated samples for the matrix to be inverted (eg,
a replay buffer) and two samples for $A$ and $\transp{A}$, but not sure
about the latter.

TODO: I'm more interested in an algo that's second-order wrt the Markov
process (I don't necessarily want to orthonormalize features).

TODO: in the original TANGO paper: prove that the main learning rate
doesn't have to be too small even for non-quadratic problems, namely,
even if the Hessian changes, we're still applying a positive operator to
earlier gradients so we still have a valid gradient direction.

\paragraph{Nonlinear case.} In the nonlinear case, we still set $\Phi\deq
\partial V/\partial \theta$ and TD is
\begin{equation}
\theta \gets \theta-\eta \transp{\Phi}\rho (V-PV-R)
\end{equation}
and the resulting change of $V$ is
\begin{equation}
V\gets V-\eta \Phi\transp{\Phi}\rho (V-PV-R)+O(\eta^2)
\end{equation}

Preconditioning TD with a matrix $B$ amounts to using $B\transp{\Phi}$ instead of
$\transp{\Phi}$ in the update of $\theta$, and $\Phi B \transp{\Phi}$ instead of
$\Phi\transp{\Phi}$ in the update of $V$.

Let us consider a loss function $\transp{(V-PV-R)}L(V-PV-R)$ on the
Bellman gap, with $L$ a symmetric definite positive matrix. For small
$\eta$, the derivative of this loss function wrt $\theta$ is
\begin{equation}
\transp{(V-PV-R)}L(\Phi-P\Phi)
\end{equation}

The loss function $L$ may not depend on $\theta$, otherwise we are
decreasing a variable Lyapunov function which does not prove convergence.

If the algorithm is some form of preconditioned TD, namely
$\delta\theta=-\eta B\transp{\Phi}\rho (V-PV-R)$, the loss function will
decrease if and only if
\begin{equation}
\transp{(V-PV-R)}L(\Phi-P\Phi)B\transp{\Phi}\rho (V-PV-R)
\end{equation}
is positive, namely, if $L\Delta \Phi B \transp{\Phi}\rho $ is a positive
operator.

On-policy, a choice would seem to be $L=\mu\Phi B \transp{\Phi}\mu$ which
would result in $(\mu\Phi B \transp{\Phi})(\mu \Delta) \mu\Phi B
\transp{\Phi}\mu$ which is positive; however, this depends on $\theta$
since $\Phi$ does. So this is not a suitable Lyapunov function.

In the reversible case, $\mu\Delta$ is symmetric.\footnote{$\rho \Delta$
can be symmetric if and only if $\rho=\mu$. Indeed, if $\rho \Delta$ is
symmetric then $\rho=\rho'$ and $\rho$ is the stationary distribution.} We can use the
Dirichlet form which amounts to $L\deq\mu(\mu\Delta)^{-1}\mu$; then
$L\Delta=\mu$ and we have to prove that $\mu\Phi B\transp{\Phi}\rho$ is
positive.  On-policy, $\rho=\mu$ and this is obviously true, for any
positive $B$. Thus, in the reversible case, nonlinear TD converges
on-policy, for any $B$. It does so by minimizing the Dirichlet norm.

In the non-reversible case, let $\rho\Delta=S+A$ with $S$ symmetric and
$A$ antisymmetric. Assume $S$ is positive. Let $L\deq \rho S^{-1} \rho$. Then we have to prove that
$\rho S^{-1} \rho \Delta \Phi B\transp{\Phi}\rho$ is positive. For any
function $f$,
\begin{align}
\transp{f} \rho S^{-1} \rho \Delta \Phi B\transp{\Phi}\rho f
&=\transp{f} \rho S^{-1} (S+A) \Phi B\transp{\Phi}\rho f
\\&=\transp{f} \rho \Phi B\transp{\Phi}\rho f + \transp{f} \rho S^{-1}
A \Phi B\transp{\Phi}\rho f
\end{align}

The first term $\transp{f}
\rho \Phi B\transp{\Phi}\rho f$ is always positive. For the second
term, since $A$ is antisymmetric, we have $\transp{x}Ax=0$ for all $x$.
Thus, we can add any multiple of $\transp{x}Ax=0$; let us take
$x=S^{-1}\rho f$:
\begin{equation}
\transp{f} \rho S^{-1}
A \Phi B\transp{\Phi}\rho f=
\transp{f} \rho S^{-1}A (\Phi B\transp{\Phi}-S^{-1})\rho f
\end{equation}

In particular, if $S^{-1}=\Phi B\transp{\Phi}$ then this vanishes.
The latter can only happen in the overparameterized case (indeed, the
right-hand-side lives in the image of $\Phi$). In that case $\Phi$ is
invertible and $B=(\transp{\Phi}S\Phi)^{-1}$ solves the problem; a
two-timescale algorithm is required to compute $B$.

Thus, assuming $S$ is positive, we have a two-timescale algorithm for non-linear TD in the off-policy
overparameterized case; it minimizes the Dirichlet norm.
However, in general $S$ is not positive. $S$ is positive if $\rho=\mu$, so,
we have a two-timescale algorithm for non-linear on-policy
overparameterized TD.

(Still works for non-overparameterized if $A$ does not couple the image
of $\Phi$ and its complement...)

TODO: Why do I end up using $\rho\Delta+\transp{(\rho \Delta)}$ while the
Dirichlet form is $\rho \Delta + \rho'-\transp{(\rho P)}$ which is
different, I think...  Probably use the right combination of $\rho$ and
$\rho'$ and the Dirichlet form rather than $\rho S^{-1}\rho$.

Nonlinear GTD2 etc use a function $L$ that depends on $\theta$ (via
$\Phi$); this adds Hessian terms to the gradient of $L$, and it's not
obvious how to interpret the local minima of $L$.

\paragraph{Why $S$ is not positive.} The Dirichlet form associated
with the set of transitions $s\to s'$ is
\begin{align}
\Dir&=\rho+\rho'-\rho P-\transp{(\rho P)}
\\&=\E[\1_{ss}+\1_{s's'}-\1_{ss'}-\1_{s's}]
\end{align}
where the expectation is over transitions in the dataset. The Dirichlet
form satisfies
\begin{equation}
\qquad \transp{f}\Dir f=\E_{ss'}(f(s)-f(s'))^2
\end{equation}
which is obviously positive. Meanwhile,
\begin{align}
\rho\Delta&=\rho-\rho P=\E[\1_{ss}-\1_{ss'}]
\\&=\frac12 \Dir+\frac12 \E[\1_{ss}-\1_{s's'}]+\frac12
\E[\1_{s's}-\1_{ss'}]
\end{align}
and the first two terms are symmetric while the last term is
antisymmetric, so
\begin{equation}
S=\frac12 \Dir+\frac12 \E[\1_{ss}-\1_{s's'}],\qquad A=\frac12
\E[\1_{s's}-\1_{ss'}]
\end{equation}
and
\begin{equation}
\transp{f}Sf=\E f(s)^2-\E f(s)f(s')
\end{equation}
which in general is not positive (take $\rho$ concentrated on a single $s$, and a large value
for $f(s')$).

\paragraph{Minimizers of errors.} Rather than the fixed point of TD, one
can look for minimizers of the error in various norms.

A local minimizer of the $L^2(\rho)$ error between $V$ and
$(\Id-P)^{-1}R$ solves
\begin{equation}
\transp{\Phi} \rho V=\transp{\Phi}\rho (\Id-P)^{-1} R
\end{equation}
which is not quite the same as the fixed point of TD. We have no access
to the right-hand-side, indeed, computing $(\Id-P)^{-1} R$ is exactly the
original problem.

A local minimizer of the
$L^2(\rho)$ error between $(\Id-P)V$ and $R$ is
\begin{equation}
\transp{\Phi}\transp{(\Id-P)}\rho(\Id-P)V=\transp{\Phi}\transp{(\Id-P)}\rho R
\end{equation}
and in the linear case $V=\Phi\theta$, this is the standard solution to
the linear regression problem $(\Id-P)\Phi\theta\approx R$ in
$L^2(\rho)$. However, the matrix on the left is not samplable: sampling
$\transp{(\Id-P)}\rho(\Id-P)$ is exactly equivalent to double sampling
from $s\sim \rho$.
Indeed, $\transp{P}\rho P$ is equal to $\E[\1_{s' s''}]$ where $s'$ and
$s''$ are two independent states from a transition $s\to s'$ with $s\sim
\rho$.

The minimizer of the $L^2(\rho^2)$ error between $R$
and $(\Id-P)V$ is samplable:
\begin{equation}
\transp{\Phi}\transp{(\Id-P)}\rho^2(\Id-P)V=\transp{\Phi}\transp{(\Id-P)}\rho^2
R
\end{equation}
Indeed, denoting $\Delta\deq \Id-P$, we can sample $\rho \Delta$ and
therefore we can take two samples from $\rho \Delta$ which provides a
sample from $\transp{(\rho\Delta)}\rho\Delta$. However, the double sampling
problem resurfaces: Most of the time, taking the product
$\transp{(\rho\Delta)}\rho\Delta$ from two samples of $\rho
\Delta$ is $0$ unless we are lucky enough to sample two transitions $s\to
s'$ with the same $s$.
Even the right-hand-side $\rho^2 R$ involves double sampling.

If one has access to some features $\Psi$, this can be mitigated:
the minimizer of the norm $L^2(\rho \Psi\transp{\Psi}\rho)$ is better
samplable, because now in double sampling it's enough to sample states \emph{which share
some
features}, not just the exact same state. This is basically what happens
in the linear case, with $\Psi=\Phi$. In the nonlinear case taking
$\Psi=\Phi$ is not acceptable because $\Phi$ depends on $\theta$, so the
objective is not a fixed norm to be minimized. The features $\Psi$ should not be orthogonal. Should use a
hash of states instead? Or apply ReLU to orthogonal features.

The Dirichlet norm associated with $\rho$ is $\E_{ss'}
(f(s)-f(s'))^2$. Its matrix is $\rho+\rho'-\rho P-\transp{(\rho P)}$ where
$\rho'$ is the distribution of $s'$ (and is samplable).
A local minimizer of the Dirichlet error between $V$ and
$(\Id-P)^{-1}R$ solves
\begin{equation}
\transp{\Phi} (\rho+\rho'-\rho P-\transp{(\rho P)})(V-(\Id-P)^{-1}R)=0
\end{equation}
If $\rho$ is the stationary distribution $\mu$ and $P$ is reversible, one
has $\transp{(\mu P)}=\mu P$ and $\rho'=\mu$ so $\rho+\rho'-\rho P-\transp{(\rho
P)}=2\mu(\Id-P)$ and this is equivalent to
\begin{equation}
\transp{\Phi}\mu ((\Id-P)V-R)=0
\end{equation}
namely, the same as a fixed point of TD.

Instead of applying the Dirichlet norm to $(V-(\Id-P)^{-1}R)$, one can
apply the inverse Dirichlet norm to $((\Id-P)V-R)$. When $P$ is
reversible and $\rho=\mu$ this
is the same. One tries to minimize
$\transp{((\Id-P)V-R)}\rho\Dir^{-1}\rho((\Id-P)V-R)$ where
$\Dir\deq \rho+\rho'-\rho P-\transp{(\rho P)}$ as above, and use
TANGO to invert $\Dir$. However this suffers again from a double
sampling problem, in several places. Still, this can be rewritten as a
correction to TD where only the correction suffers from double
sampling/large variance. Namely, the gradient of the above with respect
to $\theta$ is $\transp{\Phi}\transp{(\rho\Delta)} \Dir^{-1}\rho(\Delta V-R)$. Since
$\rho\Delta$ is half of $\Dir$, this gradient is 
\begin{gather}
\transp{\Phi}\transp{(\rho\Delta)} \Dir^{-1}\rho(\Delta V-R)=
\\
\frac12\transp{\Phi}(\transp{(\rho \Delta)}+\rho'-\rho P)\Dir^{-1}\rho(\Delta
V-R)+\frac12 \transp{\Phi}(\transp{(\rho \Delta)}+\rho P-\rho')\Dir^{-1}\rho(\Delta
V-R)
\\=\frac12\transp{\Phi}\rho(\Delta
V-R)+\frac12 \transp{\Phi}(\transp{(\rho \Delta)}+\rho P-\rho')\Dir^{-1}\rho(\Delta
V-R)
\end{gather}
the first part of which is TD. Thus one can write a correction to TD such
that only the correction suffers from double-sampling. The correction vanishes for reversible
chains. The double sampling can be simulated with high variance in finite
state spaces (just take samples from $P$ and $\transp{P}$ until they
match) but does not make sense in continuous state spaces. Use a model of
transitions only for the correction? TODO: use the same trick on the
right? No, because the $R$ term doesn't go away and I'd rather have a
correction that vanishes when $\Delta V-R$ is small.

\section{A function that almost solves the Bellman equation can be very
far from the true Bellman function}

Combined with all these problems, is the problem that a Bellman solution on the
support of $\rho$ may be very, very far from the global Bellman solution,
even on the support of $\rho$ (take random walk on a circle, with $\rho$
being everything but one point: then any linear function solves Bellman
on the support of $\rho$).

TODO seems to be much worse off-policy.

\section{Storing a summary of possible transitions?}

The derivative of the average reward with respect to the policy
parameters $\psi$ is equal to
\begin{align}
\partial_\psi \left(\E_\mu r\right)&=
\E_{s,a,s'} \partial_\psi \log \pi(a|s)\left(
r+V(s')
\right)
\\&=\E_{s,a}\partial_\psi \log \pi(a|s)\E_{s'|s,a}\sum_{k\geq 0}P^k R(s')
\\&=\E_{s,a,s'}\partial_\psi \log \pi(a|s)(\Delta^{-1}R)(s')
\end{align}
by expanding $V(s')$ as a sum of expected future rewards (and attributing
the reward of a transition $s\to s'$ to $s'$, which makes sense if $s'$
is a sufficient statistic).
(Here I'm neglecting constant functions again.)

Thus if we denote
\begin{equation}
g(s')\deq \E_{s,a,s'}\partial_\psi \log \pi(a|s)\1_{s'}
\end{equation}
we have
\begin{equation}
\partial_\psi \left(\E_\mu r\right)=\transp{g}\Delta^{-1}R
\end{equation}
Let us define the row vector
\begin{equation}
G\deq \transp{g}\Delta^{-1},\qquad \partial_\psi \left(\E_\mu
r\right)=GR
\end{equation}
then one has
\begin{equation}
G(s)=\partial_\psi \mu(s)
\end{equation}
namely, $G$ encodes the derivative of the invariant distribution with
respect to the policy parameters. Thus, if we find a state $s$ with high
reward $R(s)$, $G$ tells us how we can visit $s$ more often. This
explains that the derivative of the average reward is $GR$.

Eligibility traces are a random, unbiased estimator of $G$: if $s_t$ is a
trajectory from the current policy then
\begin{equation}
\E(\sum_{k\geq 1}\partial_\psi \pi(a_{t-k}|s_{t-k})\1_{s_t})=G
\end{equation}
Moreover $G$ satisfies a backward Bellman equation on-policy,
\begin{equation}
G=\E_{s,a,s'} \partial_\psi \pi(a|s)\1_{s'}+GP
\end{equation}
thanks to $\Delta^{-1}=\sum P^k=\Id+\Delta^{-1}P$. Eligibility traces are
updated this way at each step, via a sampling of $P$.

$G$ cannot be stored but one can store an approximate model. For instance
one could approximate $G$ as a sum of parameters times functions:
\begin{equation}
G\approx \sum_i \psi_i \phi_{\theta_i}(s)
\end{equation}
where each $\psi_i$ is a policy parameter, and each $\phi_{\theta_i}$ is
a function of the state parameterized by some model. Then the reverse
Bellman equation above allows us to learn $\psi_i$ and $\theta_i$ via 
a TD-like algorithm.

TODO: not the right representation, plus works only on-policy. Maybe learn a function $a(s,s')$ that
returns the first action to be taken if we want to reach $s'$ from $s$?

\end{document}

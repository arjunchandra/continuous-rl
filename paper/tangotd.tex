\documentclass[11pt,a4paper]{article}
\usepackage[colorlinks=true,linkcolor=blue]{hyperref}
\input{header.en.tex}

\newcommand{\Dir}{\mathcal{D}}
\newcommand{\green}{\mathcal{G}}
\DeclareMathOperator{\sgn}{sgn}

\begin{document}

These notes contain: Section~\ref{sec:tdconv}: remarks on recovering the known convergence results
for TD in a single framework, plus ideas on systematic variants around
two-timescale algorithms or improved ``almost-one-timescale'' algorithms.

Section~\ref{sec:norms}: algorithms for convergence of the Bellman error
in various norms. In particular, solving the double sampling problem by
using a model for one of the samples, and a real transition for the other
sample, has nice mathematical properties.

Section~\ref{sec:quasibellman}: the problem of the existence of near-solutions to the Bellman
equation that are very far from the true value function (I think this is
quite serious off-policy).

Section~\ref{sec:green}: Ideas on how, even if there's no reward signal, we should
still learn how to navigate in the Markov chain and learn which
actions/policy to use to go to which states. This extends eligibility
traces in a systematic way.

\section{Recovering known convergence results for TD, and a few ideas}
\label{sec:tdconv}

We consider the Bellman equation for a policy $\pi$ in a finite MDP with
expected reward $R$ and transition probability matrix $P$ ($P$ includes $\pi$ and
the environment response). The Bellman equation is
\begin{equation}
(\Id-P)V=R
\end{equation}

We assume we can sample transitions $(s,a,r,s')$ according to some
distribution $\rho$ on $s$. In the on-policy case, $\rho$ will be the
stationary distribution. Typically $a$ has to be taken from $\pi$ (or a
distribution close to $\pi$ after resampling/by using only those $a$
sampled from $\pi$; this covers $\eps$-greedy) so that we do get
transitions from
$P$. (Alternatively one could work with pairs $(s,a)$ TODO.) Thus, we
have access to unbiased samples of $\rho P$ and $\rho R$:
\begin{equation}
\rho P=\E[\1_{s}\transp{\1_{s'}}\,],\qquad \rho R=\E[r(s,a,s')\1_{s}]
\end{equation}
and thus also to samples of $\rho(\Id-P)$. Thus, rewriting the equation
as
\begin{equation}
\rho(\Id-P)V=\rho R
\end{equation}
(equivalent if $\rho$ is positive), both sides are \emph{samplable}, and we can try to solve the above by
stochastic gradient-like methods.

For instance, if we parameterize $V$ by some parameter $\theta$, we can apply
parametric TD. The expected TD step is
\begin{equation}
\theta\gets \theta -\eta \,\transp{\Phi}\rho (V-PV-R)
\end{equation}
where 
\begin{equation}
\Phi\deq \frac{\partial V}{\partial \theta}
\end{equation}
is the Jacobian matrix of $V$ with respect to its parameter. The linear
case $V=\Phi \theta$ corresponds to constant $\Phi$. The tabular case is
$\Phi=\Id$.

A fixed point of TD solves
\begin{equation}
\transp{\Phi}\rho(\Id-P)V=\transp{\Phi}\rho R
\end{equation}
and if the system is overparameterized ($\Phi$ invertible) and
well-sampled ($\rho>0$) this implies $(\Id-P)V=R$. All terms in this
equation are \emph{samplable}: we can sample from $\rho R$ and from
$\rho(\Id-P)$.


Thus, from now on we are going to focus on solving
\begin{equation}
\transp{\Phi}\rho (\Id-P)V=\transp{\Phi}\rho R
\end{equation}
using various SGD-like solvers.

We now switch to the linear case and will de-linearize later. This
becomes
\begin{equation}
\transp{\Phi}\rho (\Id-P)\Phi \theta=\transp{\Phi}\rho R
\end{equation}
to be solved in $\theta$.

\paragraph{Solving $A\theta=b$ where $A$ and $b$ are samplable.} An
obvious way to solve $A\theta=b$ via a stochastic gradient method is
\begin{equation}
\theta\gets \theta - \eta (A\theta-b)
\end{equation}
using samples for $A$ and $b$: the only fixed point solves $A\theta=b$.
TD does exactly this, 
with $A=\transp{\Phi}\rho(\Id-P)\Phi$ and $b=\transp{\Phi}\rho R$.

If $A$ is symmetric positive definite, this is also the
gradient descent of the loss function
$\transp{(A\theta-b)}A^{-1}(A\theta-b)$. However, we want to apply this
to matrices $A$ that are not necessarily of this type.

Under this update for $\theta$, the error $\theta-A^{-1}b$ is updated as
\begin{equation}
\theta-A^{-1}b \gets (\Id-\eta A)(\theta-A^{-1}b)
\end{equation}

This converges if only if $A$ is \emph{stable}.


\begin{defi}[ (Stable matrix)]
A matrix $A$ is \emph{stable} if one of the following equivalent
conditions are satisfied:
\begin{itemize}
\item All the eigenvalues of $A$ have positive real part.
\item The matrix $(\Id-\eta A)$ has spectral radius $<1$ for small enough
$\eta$.
\item For $\eta$ small enough, iterating $\theta\gets (\Id-\eta A)\theta$
converges to $0$.
\item The differential equation $\theta'=-A\theta$ converges to $0$ for
any initial value.
\item There exists a symmetric, positive definite matrix $L$ (Lyapunov
function) such that $\transp{\theta}L\theta$ is decreasing along the
solutions of the differential equation $\theta'=-A\theta$.
\item There exists a symmetric, positive definite matrix $L$ such that
\begin{equation}
\transp{\theta} L A \theta>0
\end{equation}
for all $\theta\neq 0$. (This is the same $L$ as in the previous
condition.)
\end{itemize}
\end{defi}

In particular, a symmetric, definite positive matrix is stable (last
criterion with $L=\Id$).
Stability is invariant by matrix similarity $A\gets B^{-1}A B$, since
this preserves eigenvalues.

Since the
solution of $\theta_t'=-A\theta_t$ is $\theta_t=e^{-tA}\theta_0$, a
Lyapunov function that works is $\transp{\theta_0}L\theta_0\deq \int_{t \geq 0}
\norm{\theta_t}^2$. Indeed this is decreasing, because
$\transp{\theta_t}L\theta_t$ is just the same integral starting at $t$
instead of $0$. Explicitly this is
$L=\int_{t\geq 0} \transp{(e^{-t A})}
e^{-tA}$.


\begin{table}
\begin{tabular}{cccc}
Algorithm & Conditions & Preconditioner & Lyapunov
\\
\hline
\emph{Linear $V$} \\
TD	& On-policy & $B=\Id$ & $L=\Id$ \\
TD & Tabular & $B=\Id$ & $L=\mu/\rho$ \\
GTD & 2ts & $B=\transp{A}$ & $L=\Id$\\
– & On-policy, 2ts & $B=(\transp{\Phi}\rho\Phi)^{-1}$ & $L=B^{-1}$ \\
– & Overparameterized, 2ts & $B=(\transp{\Phi}\rho\Phi)^{-1}$ &
$L=\transp{\Phi}\mu\Phi$ \\
TDC, GTD2 & 2ts & $B=\transp{A}(\transp{\Phi}\rho\Phi)^{-1}$ &
$L=\Id$ \\
2nd-order & On-policy, 2ts & $B=A^{-1}$ & $L=\Id$\\
2nd-order & 2ts & $B=\transp{A}(A\transp{A})^{-1}$ & $L=\Id$\\
– & 2ts & $B=\transp{A}(A+\transp{A})^{-1}$ & $L=\Id$\\
\hline
\emph{Non-linear $V$} \\
TD & On-policy, reversible & any $B\succ 0$ & $L=\mu(\mu(\Id-P))^{-1}\mu$
\\
– & On-policy, overparameterized, 2ts & $B=(\transp{\Phi}S\Phi)^{-1}$ &
$L= \mu S^{-1} \mu$ \\
Non-lin GTD2 & 2ts, different objective &
$B=\transp{A}(\transp{\Phi}\rho\Phi)^{-1}$  & $L=\Id$
\\\hline
\end{tabular}
\label{tab:TDlist}
\caption{Various cases of convergence of TD-like algorithms, the
preconditioner $B$ for the TD step, and the matrix $L$ that proves
convergence via the Lyapunov criterion. 2ts=two-timescale.}
\end{table}

\paragraph{Making the TD iteration stable.} In linear TD the matrix $A$ is
\begin{equation}
A\deq \transp{\Phi}\rho (\Id-P)\Phi
\end{equation}
and we want to solve
\begin{equation}
A\theta=b\,\qquad b\deq \transp{\Phi}\rho R
\end{equation}
If $A$ is stable, then iterating $\theta\gets \theta -\eta(A\theta-b)$ will
converge. (TODO: I will ignore the problem with constant functions for
$\gamma=1$.)

We now review several cases in which $A$ is known to be stable, or cases
in which a modified TD algorithm makes the iteration stable (by
preconditioning the gradient descent by a matrix $B$). See also
Table~\ref{tab:TDlist}.

\begin{enumerate}
\item On-policy: $\rho=\mu$, the invariant distribution. Then we know that $\rho(\Id-P)$ is positive
(Dirichlet form: $\transp{f}\rho(\Id-P)f=\E(f(s_t)-f(s_{t+1}))^2$). This implies that $A=\transp{\Phi}\rho (\Id-P)\Phi$ is
positive too. So we can take $L=\Id$ and we recover convergence of linear
TD on-policy.
\item In the tabular case: $\Phi=\Id$. Indeed we have $A=\rho (\Id-P)$,
which is not positive in general. But $\mu (\Id-P)$ is positive, so we can take
$L=\mu/\rho$ so that $LA$ is positive. Thus, tabular TD works
off-policy.
\end{enumerate}

One can also \emph{precondition} the gradient descent. This can make it
stable and/or make convergence faster. The step becomes
\begin{equation}
\theta\gets \theta-\eta B(A\theta-b)
\end{equation}
for some matrix $B$. Some choices of $B$ correspond to natural choices of
loss functions on $\theta$, as we will see.

Of course the values of $B$ that are most interesting are obtained as the
inverse of something; for instance $B=A^{-1}$ solves the problem
instantly... Two-timescale methods or TANGO-like algorithms
\footnote{TANGO looks much like a two-timescale algorithm, but is
``almost one-timescale'' in that the faster timescale can be fixed and
does not need to tend to $0$.} can apply an
inverse of something to $A\theta-b$, via an auxiliary gradient descent.
TANGO works for stable matrices, if they are samplable \footnote{In
TANGO, the consecutive steps absolutely have to be \emph{independent}. So even in
the on-policy case, we will need a replay buffer from $\mu$ for the auxiliary
gradient descent.}

A few choices are:
\begin{itemize}
\item GTD is a gradient descent of
$\transp{(A\theta-b)}(A\theta-b)$ (Sutton 2009). This helps with $A$ being non-symmetric.
This corresponds to $B=\transp{A}$. Then
$BA=\transp{A}A$ is
obviously stable, hence convergence.

TODO: check if
GTD uses the same sample for $\transp{A}$ and $A$. If so, it will
converge to a different stable point. TODO: Can be applied TANGO-style
(by formally inverting $\Id$ in TANGO), advantage=fewer variance due to
two-sampling (Is this equivalent to eligibility traces without the
variance??). (TANGO uses two-sampling but on a sum of past terms, so
that a past term gets TANGOified whenever we happen to reach a state that
activates the same features in the future, which is probably fine). Can
also be TANGOified with $\gamma\to 0$ (two-timescale) which now requires
only single sampling (no replay buffers).

\item $B=A^{-1}$: obviously stable ($BA=\Id$), jumps to the solution in one step if
$\eta=1$. Requires computing the inverse of $A$, namely, solving the
problem by first solving the problem. But if $A$ is stable and samplable,
this can be done via TANGO. More on that later. This should speed up
things in situations where $A$ is stable in the first place (eg.,
on-policy).

\item $B=(\transp{\Phi}\rho\Phi)^{-1}$. This amounts to orthonormalizing
the basis before applying TD. In general, this is not stable off-policy: take the
standard two-point counterexample with a single $\Phi=\transp{(1,2)}$,
orthonormalizing $\Phi$ is not going to help as $\transp{\Phi}\rho\Phi$
is just a number.

Still, this is stable in two cases:
\emph{on-policy} (take $L=B^{-1}$, given that $A$ is stable on-policy), or
assuming \emph{overparameterization} ($\Phi$ invertible: then we can take
$L=\transp{\Phi}\mu \Phi$ so that $LBA=\transp{\Phi}\mu (\Id-P)\Phi$
which is positive because $\mu(\Id-P)$ is positive; essentially, if
$\Phi$ is invertible we are back to the tabular case).
% Then we have to prove th
% $(\transp{\Phi}\Phi)^{-1}\transp{\Phi}\rho(\Id-P)\Phi$ is stable. Taking
% $L=\transp{\Phi}(\mu/\rho)\Phi$, and computing
% \begin{align}
% \transp{\theta}LBA\theta&=
% \transp{\theta}L(\transp{\Phi}\rho\Phi)^{-1}\transp{\Phi}\rho(\Id-P)\Phi\theta
% \end{align}
% and note that $(\transp{\Phi}\rho\Phi)^{-1}\transp{\Phi}\rho$ is the
% solution of linear regression with design matrix $\Phi$ in $L^2(\rho)$.
% This implies that
% \begin{equation}
% \Phi(\transp{\Phi}\rho\Phi)^{-1}\transp{\Phi}\rho=\Pi_\Phi
% \end{equation}
% where $\Pi$ is the orthogonal projector on the image of $\Phi$ in
% $L^2(\rho)$. Now taking
% \begin{equation}
% L=\transp{\Phi}(\mu/\rho)\Phi
% \end{equation}
% we have
% \begin{equation}
% \transp{\theta}LBA\theta=\transp{\theta} \transp{\Phi}(\mu/\rho)\Pi_\Phi
% (\Id-P)\Phi\theta
% \end{equation}
% TODO: on-policy or overparameterized

\item The choice from Sutton 2009 "TDC" combines the previous two tricks
and corresponds to a gradient descent
of the projected norm of the Bellman gap on the subspace generated by
$\Phi$, projected in $L^2(\rho)$. This is GTD with $L^2(\rho)$-orthonormalization of
$\Phi$, namely,
\begin{equation}
B=\transp{A} (\transp{\Phi}\rho\Phi)^{-1}=\transp{(\transp{\Phi}\rho(\Id-P)\Phi)}(\transp{\Phi}\rho\Phi)^{-1}
\end{equation}
from which it is clear that $BA$ is symmetric and stable. This is obtained as a
gradient descent of the projected norm of the Bellman gap on the image of
$\Phi$, projected in $L^2(\rho)$. TODO: TDC and GTD2 differ by using that
$A$ has $(\Id-P)$ on  the right, and expanding the $\Id$ term and the $P$
term separately, which results in simplifications and make TDC much
closer to TD and less reliant on the inversion.

\item The choice $B=A^{-1}$ can also be written as
$B=\transp{A}(A\transp{A})^{-1}$. The advantage is that we now have to
invert $(A\transp{A})$ which is a stable matrix, so, better behaved with
TANGO or two-timescale. However, variance
will be terrible: this requires four independent samples from $A$.

\item A gradient descent of
$\transp{(A\theta-b)}(A+\transp{A})^{-1}(A\theta-b)$, corresponding to
$B=\transp{A}(A+\transp{A})^{-1}$. This is probably the closest to
unmodified TD in behavior: for instance when $A$ is symmetric ($\rho=\mu$
and $P$ reversible) this coincides with TD. $A+\transp{A}$ corresponds to
the Dirichlet form for $\rho$, namely, $\E_{\rho} (f(s)-f(s'))^2$ TODO
CHECK.

\end{itemize}

Only $B=A^{-1}$ and $B=\transp{A}(A\transp{A})^{-1}$ are truly
second-order (namely, solve the problem in one step in the non-stochastic
case).

The other algorithms involving an inverse basically have the cost of
second-order algorithms without the full benefits.  They are second-order
in feature-space (they orthonormalize $\Phi$) but not in Markov process
space (they don't backpropagate rewards).

TODO sampling: if $A=\E \tilde A$, can do $\transp{(\E \tilde A)}(\E
\tilde A)$ or
$\E(\transp{\tilde A}\tilde A)$, which is not equal to $\transp{A}A$ but
is still definite positive (hence stable). This would avoid some
quadruple-sampling problems with $B=\transp{A}(A\transp{A})^{-1}$. But
breaks second-order correctness?

TODO: define second-order with respect to $\Phi$ (second-order
provided $\transp{\Delta}\Delta=\Id$, eg in a Fourier frequency-adjusted
basis), with respect to the Markov chain (second-order provided
$\transp{\Phi}\Phi=\Id$).

Almost-one-timescale (such as TANGO) needs more independence than two-timescale. I
\emph{think} that two-timescale can deal with taking the same sample for
$A$ and $\transp{A}$ in $\transp{A}(...)^{-1}A$, and taking correlated
consecutive samples (eg, online) for the matrix to be inverted. I think
TANGO would need uncorrelated samples for the matrix to be inverted (eg,
a replay buffer) and two samples for $A$ and $\transp{A}$, but not sure
about the latter.

TODO: I'm more interested in an algo that's second-order wrt the Markov
process (I don't necessarily want to orthonormalize features).

TODO: in the original TANGO paper: prove that the main learning rate
doesn't have to be too small even for non-quadratic problems, namely,
even if the Hessian changes, we're still applying a positive operator to
earlier gradients so we still have a valid gradient direction.

\paragraph{Nonlinear case.} In the nonlinear case, we still set $\Phi\deq
\partial V/\partial \theta$ and TD is
\begin{equation}
\theta \gets \theta-\eta \transp{\Phi}\rho (V-PV-R)
\end{equation}
and the resulting change of $V$ is
\begin{equation}
V\gets V-\eta \Phi\transp{\Phi}\rho (V-PV-R)+O(\eta^2)
\end{equation}

Preconditioning TD with a matrix $B$ amounts to using $B\transp{\Phi}$ instead of
$\transp{\Phi}$ in the update of $\theta$, and $\Phi B \transp{\Phi}$ instead of
$\Phi\transp{\Phi}$ in the update of $V$.

Let us consider a loss function $\transp{(V-PV-R)}L(V-PV-R)$ on the
Bellman gap, with $L$ a symmetric definite positive matrix. For small
$\eta$, the derivative of this loss function wrt $\theta$ is
\begin{equation}
\transp{(V-PV-R)}L(\Phi-P\Phi)
\end{equation}

The loss function $L$ may not depend on $\theta$, otherwise we are
decreasing a variable Lyapunov function which does not prove convergence.

If the algorithm is some form of preconditioned TD, namely
$\delta\theta=-\eta B\transp{\Phi}\rho (V-PV-R)$, the loss function will
decrease if and only if
\begin{equation}
\transp{(V-PV-R)}L(\Phi-P\Phi)B\transp{\Phi}\rho (V-PV-R)
\end{equation}
is positive, namely, if $L\Delta \Phi B \transp{\Phi}\rho $ is a positive
operator.

On-policy, a choice would seem to be $L=\mu\Phi B \transp{\Phi}\mu$ which
would result in $(\mu\Phi B \transp{\Phi})(\mu \Delta) \mu\Phi B
\transp{\Phi}\mu$ which is positive; however, this depends on $\theta$
since $\Phi$ does. So this is not a suitable Lyapunov function.

In the reversible case, $\mu\Delta$ is symmetric.\footnote{$\rho \Delta$
can be symmetric if and only if $\rho=\mu$. Indeed, if $\rho \Delta$ is
symmetric then $\rho=\rho'$ so that $\rho$ is the stationary distribution.} We can use the
Dirichlet form which amounts to $L\deq\mu(\mu\Delta)^{-1}\mu$; then
$L\Delta=\mu$ and we have to prove that $\mu\Phi B\transp{\Phi}\rho$ is
positive.  On-policy, $\rho=\mu$ and this is obviously true, for any
positive $B$. Thus, in the reversible case, nonlinear TD converges
on-policy, for any $B$. It does so by minimizing the Dirichlet norm.

In the non-reversible case, let $\rho\Delta=S+A$ with $S$ symmetric and
$A$ antisymmetric. Assume $S$ is positive. Let $L\deq \rho S^{-1} \rho$. Then we have to prove that
$\rho S^{-1} \rho \Delta \Phi B\transp{\Phi}\rho$ is positive. For any
function $f$,
\begin{align}
\transp{f} \rho S^{-1} \rho \Delta \Phi B\transp{\Phi}\rho f
&=\transp{f} \rho S^{-1} (S+A) \Phi B\transp{\Phi}\rho f
\\&=\transp{f} \rho \Phi B\transp{\Phi}\rho f + \transp{f} \rho S^{-1}
A \Phi B\transp{\Phi}\rho f
\end{align}

The first term $\transp{f}
\rho \Phi B\transp{\Phi}\rho f$ is always positive. For the second
term, since $A$ is antisymmetric, we have $\transp{x}Ax=0$ for all $x$.
Thus, we can add any multiple of $\transp{x}Ax=0$; let us take
$x=S^{-1}\rho f$:
\begin{equation}
\transp{f} \rho S^{-1}
A \Phi B\transp{\Phi}\rho f=
\transp{f} \rho S^{-1}A (\Phi B\transp{\Phi}-S^{-1})\rho f
\end{equation}

In particular, if $S^{-1}=\Phi B\transp{\Phi}$ then this vanishes.
The latter can only happen in the overparameterized case (indeed, the
right-hand-side lives in the image of $\Phi$). In that case $\Phi$ is
invertible and $B=(\transp{\Phi}S\Phi)^{-1}$ solves the problem; a
two-timescale algorithm is required to compute $B$.

Thus, assuming $S$ is positive, we have a two-timescale algorithm for non-linear TD in the off-policy
overparameterized case; it minimizes the Dirichlet norm.
However, in general $S$ is not positive.

Still, $S$ is positive if $\rho=\mu$, so,
we have a two-timescale algorithm for \emph{non-linear on-policy
overparameterized TD}.

(Still works for non-overparameterized if the antisymmetric part $A$ does not couple the image
of $\Phi$ and its complement...)

TODO: Why do I end up using $\rho\Delta+\transp{(\rho \Delta)}$ while the
Dirichlet form is $\rho \Delta + \rho'-\transp{(\rho P)}$ which is
different, I think...  Probably use the right combination of $\rho$ and
$\rho'$ and the Dirichlet form rather than $\rho S^{-1}\rho$.

Nonlinear GTD2 etc use a function $L$ that depends on $\theta$ (via
$\Phi$); this adds Hessian terms to the gradient of $L$, and it's not
obvious how to interpret the local minima of $L$.

\paragraph{Why $S$ is not positive.} The Dirichlet form associated
with the set of transitions $s\to s'$ is
\begin{align}
\Dir&=\rho+\rho'-\rho P-\transp{(\rho P)}
\\&=\E[\1_{ss}+\1_{s's'}-\1_{ss'}-\1_{s's}]
\end{align}
where the expectation is over transitions in the dataset. The Dirichlet
form satisfies
\begin{equation}
\qquad \transp{f}\Dir f=\E_{ss'}(f(s)-f(s'))^2
\end{equation}
which is obviously positive. Meanwhile,
\begin{align}
\rho\Delta&=\rho-\rho P=\E[\1_{ss}-\1_{ss'}]
\\&=\frac12 \Dir+\frac12 \E[\1_{ss}-\1_{s's'}]+\frac12
\E[\1_{s's}-\1_{ss'}]
\end{align}
and the first two terms are symmetric while the last term is
antisymmetric, so
\begin{equation}
S=\frac12 \Dir+\frac12 \E[\1_{ss}-\1_{s's'}],\qquad A=\frac12
\E[\1_{s's}-\1_{ss'}]
\end{equation}
and
\begin{equation}
\transp{f}Sf=\E f(s)^2-\E f(s)f(s')
\end{equation}
which in general is not positive (take $\rho$ concentrated on a single $s$, and a large value
for $f(s')$).

\section{Minimizing the error in some norm via TD}
\label{sec:norms}

Rather than the fixed point of TD, one
can look for minimizers of the error in various norms.

A local minimizer of the $L^2(\rho)$ error between $V$ and
$(\Id-P)^{-1}R$ solves
\begin{equation}
\transp{\Phi} \rho V=\transp{\Phi}\rho (\Id-P)^{-1} R
\end{equation}
which is not quite the same as the fixed point of TD. We have no access
to the right-hand-side, indeed, computing $(\Id-P)^{-1} R$ is exactly the
original problem.

A local minimizer of the
$L^2(\rho)$ error between $(\Id-P)V$ and $R$ is
\begin{equation}
\transp{\Phi}\transp{(\Id-P)}\rho(\Id-P)V=\transp{\Phi}\transp{(\Id-P)}\rho R
\end{equation}
and in the linear case $V=\Phi\theta$, this is the standard solution to
the linear regression problem $(\Id-P)\Phi\theta\approx R$ in
$L^2(\rho)$. However, the matrix on the left is not samplable: sampling
$\transp{(\Id-P)}\rho(\Id-P)$ is exactly equivalent to double sampling
from $s\sim \rho$.
Indeed, $\transp{P}\rho P$ is equal to $\E[\1_{s' s''}]$ where $s'$ and
$s''$ are two independent states from a transition $s\to s'$ with $s\sim
\rho$.

The minimizer of the $L^2(\rho^2)$ error between $R$
and $(\Id-P)V$ is samplable:
\begin{equation}
\transp{\Phi}\transp{(\Id-P)}\rho^2(\Id-P)V=\transp{\Phi}\transp{(\Id-P)}\rho^2
R
\end{equation}
Indeed, denoting $\Delta\deq \Id-P$, we can sample $\rho \Delta$ and
therefore we can take two samples from $\rho \Delta$ which provides a
sample from $\transp{(\rho\Delta)}\rho\Delta$. However, the double sampling
problem resurfaces: Most of the time, taking the product
$\transp{(\rho\Delta)}\rho\Delta$ from two samples of $\rho
\Delta$ is $0$ unless we are lucky enough to sample two transitions $s\to
s'$ with the same $s$.
Even the right-hand-side $\rho^2 R$ involves double sampling.

If one has access to some features $\Psi$, this can be mitigated:
the minimizer of the norm $L^2(\rho \Psi\transp{\Psi}\rho)$ is better
samplable, because now in double sampling it's enough to sample states \emph{which share
some
features}, not just the exact same state. This is basically what happens
in the linear case, with $\Psi=\Phi$. In the nonlinear case taking
$\Psi=\Phi$ is not acceptable because $\Phi$ depends on $\theta$, so the
objective is not a fixed norm to be minimized. The features $\Psi$ should not be orthogonal. Should use a
hash of states instead? Or apply ReLU to orthogonal features.

The Dirichlet norm associated with $\rho$ is $\E_{ss'}
(f(s)-f(s'))^2$. Its matrix is $\rho+\rho'-\rho P-\transp{(\rho P)}$ where
$\rho'$ is the distribution of $s'$ (and is samplable).
A local minimizer of the Dirichlet error between $V$ and
$(\Id-P)^{-1}R$ solves
\begin{equation}
\transp{\Phi} (\rho+\rho'-\rho P-\transp{(\rho P)})(V-(\Id-P)^{-1}R)=0
\end{equation}
If $\rho$ is the stationary distribution $\mu$ and $P$ is reversible, one
has $\transp{(\mu P)}=\mu P$ and $\rho'=\mu$ so $\rho+\rho'-\rho P-\transp{(\rho
P)}=2\mu(\Id-P)$ and this is equivalent to
\begin{equation}
\transp{\Phi}\mu ((\Id-P)V-R)=0
\end{equation}
namely, the same as a fixed point of TD.

Instead of applying the Dirichlet norm to $(V-(\Id-P)^{-1}R)$, one can
apply the inverse Dirichlet norm to $((\Id-P)V-R)$. When $P$ is
reversible and $\rho=\mu$ this
is the same. One tries to minimize
$\transp{((\Id-P)V-R)}\rho\Dir^{-1}\rho((\Id-P)V-R)$ where
$\Dir\deq \rho+\rho'-\rho P-\transp{(\rho P)}$ as above, and use
TANGO to invert $\Dir$. However this suffers again from a double
sampling problem, in several places. Still, this can be rewritten as a
correction to TD where only the correction suffers from double
sampling/large variance. Namely, the gradient of the above with respect
to $\theta$ is $\transp{\Phi}\transp{(\rho\Delta)} \Dir^{-1}\rho(\Delta V-R)$. Since
$\rho\Delta$ is half of $\Dir$, this gradient is 
\begin{gather}
\transp{\Phi}\transp{(\rho\Delta)} \Dir^{-1}\rho(\Delta V-R)=
\\
\frac12\transp{\Phi}(\transp{(\rho \Delta)}+\rho'-\rho P)\Dir^{-1}\rho(\Delta
V-R)+\frac12 \transp{\Phi}(\transp{(\rho \Delta)}+\rho P-\rho')\Dir^{-1}\rho(\Delta
V-R)
\\=\frac12\transp{\Phi}\rho(\Delta
V-R)+\frac12 \transp{\Phi}(\transp{(\rho \Delta)}+\rho P-\rho')\Dir^{-1}\rho(\Delta
V-R)
\end{gather}
the first part of which is TD. Thus one can write a correction to TD such
that only the correction suffers from double-sampling. The correction vanishes for reversible
chains. The double sampling can be simulated with high variance in finite
state spaces (just take samples from $P$ and $\transp{P}$ until they
match) but does not make sense in continuous state spaces. Use a model of
transitions only for the correction? TODO: use the same trick on the
right? No, because the $R$ term doesn't go away and I'd rather have a
correction that vanishes when $\Delta V-R$ is small.

\paragraph{Double sampling: using a model for one of the
samples.} Double sampling allows us to minimize the $L^2$ norm of the
Bellman gap. This is only possible if we have model of the environment,
that allows for sampling $s'$ given $s$ and $a$.

However, using Bellman on a model is dangerous because we compute the
value function of the model, and even if the model is accurate for one
step $s\to s'$, it may quickly diverge when iterated. Therefore its value
function may be quite different from the true value function. The model
needs to be accurate at time horizon $1/(1-\gamma)$.

But in double-sampling, one could also use one real transition and one
simulated transition. We show here that this is equivalent, at first order,
to learning the value function of an environment in which every step is
either a real or simulated step with probability $1/2$. This should be a
bit more stable, as the real transitions may keep the model from
hallucinating completely irrealistic trajectories.

The $L^2(\rho)$ norm of the Bellman gap is $\transp{(\Delta
V-R)}\rho(\Delta V-R)$. Its gradient with respect to $V$ is
$2\transp{\Phi}\transp{\Delta}\rho (\Delta V-r)$. The presence of
$\transp{\Delta}\rho\Delta$ indicates double sampling; this operator is
not samplable.

Let us assume that we have a model $\tilde P$ of the operator $P$. Thus
we can sample $\transp{\tilde \Delta}\rho \Delta$ (just sample one
transition $s\to s'$ from $\rho P$, and a second transition $s\to \tilde
s'$ from the same $s$). We can also sample from $\transp{\tilde
\Delta}\rho R$, by sampling a reward during $s\to s'$ from the true
model, and a simulated second
transition from $s$. Note that this does not use a model of rewards, only of
transitions.
\footnote{
Moreover, \emph{if rewards during $s\to s'$ are
independent from $s'$}, we can sample from $\transp{\Delta}\rho R$. 
(This also covers the case where rewards depend only on $s'$, by shifting
rewards by one step.) Not covered is the case where the rewards really
depends on the pair
$(s,s')$ (in that case, the state $s'$ is not quite a sufficient
statistic of the process); then we could have a model $\tilde R$ of rewards and
sample from $\transp{\Delta}\rho \tilde{R}$.

Anyway we will not assume any of this: just using
$\transp{\tilde
\Delta}\rho R$ serves our purpose.}


Therefore, by various combinations of actual and simulated transitions,
we can easily compute samples of
\begin{equation}
\tfrac12 \transp{\Phi}(\transp{\Delta}\rho \tilde \Delta+\transp{\tilde
\Delta}\rho\Delta)V
-
\transp{\Phi}\transp{\tilde \Delta}
\rho R
\end{equation}
(variant 1) and
\begin{equation}
\tfrac12 \transp{\Phi}\transp{\Delta}
\rho(\tilde \Delta V-R)
+
\tfrac12 \transp{\Phi}\transp{\tilde \Delta}
\rho(\Delta V-R)
\end{equation}
(variant 2, under the conditions of the footnote). Variant 2 corresponds
to fully symmetrizing the roles of the synthetic and real samples in
two-sampling.

Setting
\begin{equation}
C\deq \tfrac12 (\transp{\Delta}\rho \tilde \Delta+\transp{\tilde
\Delta}\rho\Delta)
\end{equation}
variant 1 is the gradient of
\begin{equation}
\norm{V-C^{-1}\transp{\tilde\Delta}\rho R}^2_C
\end{equation}
while variant 2 is the gradient of
\begin{equation}
\norm{V-\frac12 C^{-1}(\transp{\tilde\Delta}+\transp{\Delta})\rho R}^2_C
\end{equation}

Therefore, if $C$ is positive definite, then variant 1 and variant 2 will converge to
the best approximation (in the parametric family $V$) of
\begin{equation}
\hat V_1\deq C^{-1}\transp{\tilde\Delta}\rho R \qquad\text{and}\qquad \hat
V_2\deq \frac12
C^{-1}(\transp{\tilde\Delta}+\transp{\Delta})\rho R
\end{equation}
respectively.
If $\tilde \Delta=\Delta$, then both $\hat V_1$ and $\hat V_2$
are equal to the desired target
$\Delta^{-1}R$.

Let us study how these fixed points approximate the true target
$\Delta^{-1}R$ when $\tilde \Delta \approx \Delta$. First,
\begin{equation}
C=\frac14 \transp{(\tilde \Delta+\Delta)}\rho(\tilde \Delta+\Delta)-\frac14
\transp{(\tilde
\Delta-\Delta)}\rho(\tilde
\Delta-\Delta)
\end{equation}
Therefore, $C$ is positive definite provided
\begin{equation}
\transp{(\tilde
\Delta-\Delta)}\rho(\tilde
\Delta-\Delta)\prec \transp{(\tilde \Delta+\Delta)}\rho(\tilde \Delta+\Delta)
\end{equation}
which is the case when the model $\tilde \Delta$ is close enough
to the true environment $\Delta$. (Moreover the left-hand-side is of
second order in the error, while the right-hand-side is of order $0$,
therefore this condition should be relatively easy to satisfy.) Therefore, convergence is guaranteed in a relatively large
zone around $\Delta$. But the method will diverge if $\tilde \Delta$ is
too far from $\Delta$—contrary to double sampling using the model twice,
which always converges to the value function of the model.

Next, since $C=\frac14 \transp{(\tilde \Delta+\Delta)}\rho(\tilde
\Delta+\Delta)+O(\tilde\Delta-\Delta)^2$, the fixed point $\hat V_2$ for variant 2
is
\begin{equation}
\hat V_2=\left(\frac{\tilde
\Delta+\Delta}{2}\right)^{-1}R+O(\tilde\Delta-\Delta)^2
\end{equation}
namely, at first order, the fixed point of variant 2 is the value
function of a Markov chain with transitions $\tfrac12(\tilde P+P)$,
namely, each transition is real or simulated with probability $1/2$.
Intuitively this should keep reasonable properties of the true value
function: even if the model $\tilde P$ diverges when iterated, the Markov chain $\tfrac12(\tilde
P+P)$ will have a dynamics closer to the true environment.

The fixed point $\hat V_1$ for variant 1 has less clear an
interpretation. Define the matrix $\eps$ via $\Delta \eqd(\Id+\eps\rho)\tilde \Delta$ where
$\eps$ is some hopefully small model error. Then
\begin{align}
\hat V_2&=C^{-1}\transp{(\rho\tilde \Delta)}R
\\&=\left(
\Delta+\frac{(\transp{(\rho\tilde \Delta)})^{-1}\transp{\Delta} \rho \tilde
\Delta-\Delta}{2}
\right)^{-1}R
\\&=\left(
\Delta+\frac12(\transp{\eps}-\eps)\rho\tilde \Delta
\right)^{-1}R
\end{align}
In particular, if $\eps$ is symmetric the error vanishes, but if $\eps$
is antisymmetric we end up computing $\tilde \Delta^{-1}R$, namely the
Bellman function of the model. Usually $\eps$ would be somewhere in
between; I do not know if it is possible to force the model to favor
symmetric values of $\eps$.

In view of this, variant 2 seems more promising, although it relies on
rewards depending only on $s$ or only on $s'$, not on the pair $(s,s')$.

\section{A function that almost solves the Bellman equation can be very
far from the true value function}
\label{sec:quasibellman}

Combined with all these problems, is the problem that an approximate
solution to the Bellman equation may be very, very far from the true
Bellman solution.  I believe this is an absolutely major problem off-policy.

In a finite MDP, the solution to the Bellman equation is unique. However,
in infinite MDPs this is not the case at all. And even in finite MDPs,
the non-uniqueness problem becomes an ill-conditioning problem: some
functions are almost perfect solutions to the Bellman equation but can be
arbitrarily far from the true value function.

As an example, consider a one-dimensional problem with state $s\in \Z$.
Assume the policy $\pi$ can only jump to a nearest neighbor or stay in
place (with arbitrary probabilities). Then the Bellman equation on $V$
is, at each state $s$, a linear equation involving $V(s)$, $V(s+1)$ and
$V(s-1)$. It is easy to build many solutions by choosing arbitrary
values for $V(0)$ and $V(1)$, and then just solving the Bellman equation
in $V(s+1)$ for $s\geq 2$ and solving in $V(s-1)$ for $s\leq -1$.

For instance, imagine that the policy is a nearest-neighbor random walk
on $\Z$, with probabilities $1/2$ to jump right or left, and that
$\gamma=1$. Then any linear function $f$ satisfies $f=Pf$, therefore,
linear functions lie in the kernel of the Bellman gap operator.
Consequently, if $V$ solves the Bellman equation, then $V+f$ is another
solution, for any linear function $f$. (This works in any dimension.)

What happens with linear functions is that the solution ``believes''
there's a source of reward at infinity, and this belief cannot be
falsified via the Bellman equation alone.

(If the policy and environment are ergodic with invariant measure $\mu$,
and if rewards lie in $L^2(\mu)$, then the true value function is the
only solution of Bellman in $L^2(\mu)$, so penalizing the $L^2$-norm might
help a bit. However, some simple examples below are immune to this solution.)

Let us take another example. The state is $s\in \Z$. With nearest-neighbor transitions, the Bellman function of the optimal
policy satisfies (with $\gamma=1$)
\begin{equation}
\label{eq:Vopt}
V(s)=R(s)+\max_{s'\in \{s-1,s,s+1\}} V(s')
\end{equation}

For instance, if the reward is $0$ at $0$ and $-1$ everywhere else
(a typical episodic problem of reaching a specific state as fast as
possible), the
optimal policy moves towards $0$, and the value function is
$V(s)=-\abs{s}$. However, it is easy to check that
$V(s)=\abs{s}+\1_{s=0}$ also solves the Bellman equation. This solution
believes in ``rewards at infinity''.

This ``almost'' happens in a finite state space. For instance, take a
state space $[-n;n]$ with reward $0$ at the boundaries $\pm n$, and
reward $-1$
everywhere else. The optimal policy goes to the nearest boundary point,
and the value function is the distance to the boundary, $V(s)=\abs{s}-n$, shaped like a ``V''.

This solution is unique; however, there are near-solutions that violate
the Bellman equation only at one or a few points. For instance, $-V$ is a
solution everywhere except at $s=0$. More generally one can start with a
solution and ``flip'' it
vertically in a segment; this creates a few points at which Bellman is
violated, but Bellman is still satisfied at most points.

Assume we start with the pseudo-solution $-V$ in this example.
In an off-policy situation where we sample transitions $(s,s')$ and apply
TD at $(s,s')$, it would take a long time to realize that $-V$ is
not the correct solution, and convergence to the true solution would be
slow. This solution ``believes'' there is a good reward at $s=0$, and
this belief is consistent as long as we do not actually reach $s=0$.

This phenomenon is fully general and not specific to 1D problems; in any
MDP one can build near-Bellman solutions
that ``believe'' there is a reward at a state $s_0$ and solve Bellman
exactly everywhere except at $s_0$. Off-policy, these violations would
rarely be
detected. On-policy, the policy would move towards $s_0$ and detect the
problem. I believe this is a serious limitation of off-policy approaches.

Are these pseudo-solutions frequent, or does a good initialization move
away from such solutions?
The continuous-state limit is interesting for this.

We can view $[-n;n]$ as a
discretization of a problem with state $s\in [-1;1]$, where the state is
discretized with step $\eps=1/n$. Then the continuous-state limit of the
Bellman equation \eqref{eq:Vopt} for the optimal policy (with $\gamma=1$)
is just
\begin{equation}
\abs{\partial V(s)/\partial s}=-R(s)
\end{equation}
(as one can see by expanding $V(s+\eps)$ and $V(s-\eps)$ in the equation
$V(s)=R(s)\eps +\max \{V(s+\eps),V(s-\eps)\}$; for simplicity I have
discarded the possibility to stay in place). \footnote{One gets a constraint that
rewards are negative for technical reasons due to $\gamma=1$: in such a situation, the optimal policy
moves to the state with highest reward, so with $\gamma=1$ the value
function is well-defined only if one shifts the rewards by the maximum
reward so that the reward of the terminal state becomes $0$. This is the
typical setup for many episodic environments.}

This equation is wildly underdetermined. For instance, if $V$ is a
solution then $-V$ is also a solution.

More generally, in dim $>1$, an equation such as $\abs{\partial
V(s)/\partial s}=-R(s)$ leaves total freedom as to the direction of the
gradient of $V$. Thus, with a randomly initialized $V$, my guess is that
TD-like optimization would quickly ``freeze'' upon locally random choices of 
directions of the gradient of $V$, each corresponding to a local belief
that ``the optimum lies in this direction''. Regions with different
directions of $\partial
V(s)/\partial s$ would meet on a $(D-1)$-dimensional set where Bellman
would be violated, but Bellman would still be satisfied on a set of
measure $1$.

Thus, off-policy algorithms based on sampling random transitions $(s,s')$
from a dataset would be slow to detect a problem, while on-policy (or
partly on-policy) algorithms would ``follow the local belief'' until
reaching a state where it becomes clear that this belief is unfounded, or
until they reach infinity.

\section{Storing a summary of possible transitions?}
\label{sec:green}

In this paragraph I'm trying to store a summary of ``which actions lead
where'' in the long term. Indeed, the value function can be written as
$\Delta^{-1}R$ where $R$ is the reward, and the derivative of the average
reward wrt the policy parameters can be written, by the actor-critic
formula, as $\transp{g}V=\transp{g}\Delta^{-1}R$ where $g$ contains the
log-derivatives of the probabilities of actions leading to a given state
(see below).
The Bellman equation first computes $\Delta^{-1}R$. But we could also
first compute $\transp{g}\Delta^{-1}$: this (large) object contains
information about which policy parameters increase the stationary
distribution of which states. This does not depend on a reward signal and
can be learned in a non-supervised way by observing trajectories.
Eligibility traces are an unbiased, rank-one approximation of this.

Also, when observing a transition $s\to s'$, eligibility traces update
the info about how to reach $s'$, while TD(0) updates the info about the
reward when starting from $s$. I feel that somehow we should use both:
intuitively when solving a problem we work ``from both ends'', expanding
both from the starting point and around the target.

Precisely, the derivative of the average reward with respect to the policy
parameters $\psi$ is equal to
\begin{align}
\partial_\psi \left(\E_\mu r\right)&=
\E_{s,a,s'} \partial_\psi \log \pi(a|s)\left(
r+V(s')
\right)
\\&=\E_{s,a}\partial_\psi \log \pi(a|s)\E_{s'|s,a}\sum_{k\geq 0}P^k R(s')
\\&=\E_{s,a,s'}\partial_\psi \log \pi(a|s)(\Delta^{-1}R)(s')
\end{align}
by expanding $V(s')$ as a sum of expected future rewards (and attributing
the reward of a transition $s\to s'$ to $s'$, which makes sense if $s'$
is a sufficient statistic).
(Here I'm neglecting constant functions again.)

Thus if we define the vector
\begin{equation}
g\deq \E_{s,a,s'}\partial_\psi \log \pi(a|s)\1_{s'}
\end{equation}
we have
\begin{equation}
\partial_\psi \left(\E_\mu r\right)=\transp{g}\Delta^{-1}R
\end{equation}
Let us define the row vector
\begin{equation}
G\deq \transp{g}\Delta^{-1},\qquad \partial_\psi \left(\E_\mu
r\right)=GR
\end{equation}
then one has
\begin{equation}
G(s)=\partial_\psi \mu(s)
\end{equation}
namely, $G$ encodes the derivative of the invariant distribution with
respect to the policy parameters. Thus, if we find a state $s$ with high
reward $R(s)$, $G$ tells us how we can visit $s$ more often. This
explains that the derivative of the average reward is $GR$.

[$g$ sums to $0$ on-policy, so we don't have to worry about removing
constants and can directly take $\gamma=1$: $\transp{g}\Delta^{-1}$ is
well-defined.]

Eligibility traces are a random, unbiased estimator of $G$: if
$(s_t)_{t\in \Z}$ is a
trajectory from the current policy then
\begin{equation}
\E\sum_{k\geq 1}\partial_\psi \log \pi(a_{t-k}|s_{t-k})\1_{s_t}=G
\end{equation}
namely, if $v$ is the eligibility trace vector, then the expectation of
$\transp{v}\1_{s_t}$ is $G$, at every time $t$.

Moreover $G$ satisfies a backward Bellman equation on-policy,
\begin{equation}
G=\E_{s,a,s'} \partial_\psi \log \pi(a|s)\1_{s'}+GP
\end{equation}
thanks to $\Delta^{-1}=\sum P^k=\Id+\Delta^{-1}P$. Eligibility traces are
precisely
updated this way at each step, via a sampling of $P$.

$G$ cannot be stored but one can store an approximate model. For instance
one could approximate $G$ via a neural network with input $s$ and output
in the policy parameter space. \footnote{The policy parameter space may
be quite large. This means the neural network should have a small
next-to-last layer. Indeed, with a next-to-last layer of size $r$,
the weights of the last layer are represented by $r$ policy parameters.
This corresponds to a rank-$r$ approximation of $G$,
as a sum of parameters times functions:
\begin{equation}
G\approx \sum_i \psi_i \phi_i(s)
\end{equation}
where each $\psi_i$ is a policy parameter, and each $\phi_i$ is
a function of the state, corresponding to a feature in the network.} Then the reverse
Bellman equation above should allows us to learn this model via
a TD-like algorithm, \emph{on-policy}. TODO: HOW? not clear how to
sample from $\phi P$ if $\phi$ is a function. Would need a model of
reverse transitions? \footnote{I think one has to learn $G\mu^{-1}$ and
the equation $G=\Id+GP$ rewrites as $\mu
(G\mu^{-1})\mu=\mu+\mu(G\mu^{-1})\mu P$ where everything is samplable.
However I fear an implicit double sampling in there; probably
featurize/dualize
by asking that this is satisfied for any test functions on the left and right.

Another way to exploit the equation on the right is to impose that $G$
and $P$ commute (indeed $G=\sum P^t$). This commutation equation should
transfer left-learning to right-learning. It can be written in samplable
terms
via $\mu(G\mu^{-1})(\mu P\psi)=(\mu P)(G \mu^{-1})(\mu
\psi)$ for any feature $\psi$. This apparently avoids double sampling!
}

TODO: not the right representation, plus works only on-policy. Maybe learn a function $a(s,s')$ that
returns the first action to be taken if we want to reach $s'$ from $s$?

In the end we should perhaps just learn a representation of $\Delta^{-1}$
in a hidden state that represents distributions on $s$ (trained to
faithfully reproduce averages of functions). $\Delta^{-1}$ satisfies the
two-sided equation $\Delta^{-1}=\Id+\Delta^{-1}P=\Id+P\Delta^{-1}$, so
when observing a transition $s\to s'$ we both update the model of
how to arrive at $s'$ and of what happens when starting at $s$.

\paragraph{A learned representation for $\Delta^{-1}$.} $\Delta^{-1}$
contains all the necessary information to do policy gradient or actor-critic with
arbitrary reward functions, as seen above. Is it feasible to store a
compact and useful representation of $\Delta^{-1}$? Ideally, such a
representation should be learned from the \emph{two-sided} Bellman
equations
\begin{equation}
\label{eq:greenbell}
\Delta^{-1}=\Id + P \Delta^{-1},\qquad \Delta^{-1}=\Id + \Delta^{-1}P
\end{equation}
With these two equations, when observing a transition $s\to s'$, we update
both the model of what happens when we start at $s$ (left equation,
standard Bellman), and the model of how to arrive at $s'$ (right
equation, rarely used as far as I know; somehow eligibility traces
implement some stochastic form of it).

More precisely, $\Delta^{-1}_{ss'}$ is the number of paths from $s$ to
$s'$ weighted by their probabilities. When observing a transition $s_1\to
s_2$, the left equation updates paths from $s_1$; the right equation
updates paths to $s_2$. I believe both are important. The right equation
does not exist when working with a single ``target'' reward function $R$.

The left Bellman equation on $\Delta^{-1}$ is equivalent to
simultaneously solving the Bellman equation with all possible reward
functions, generated by $\1_{s'}$ for all possible target states $s'$.
Using only the left Bellman equation treats all targets $s'$ as
completely independent problems for learning.

``Universal value function approximators'' (Schaul--Horgan--Gregor--Silver ICML 2015) do a bit of that, but only
with the left (ordinary) Bellman equation. It amounts to solving a number
of
ordinary Bellman equations with different reward functions, and relying on the generalization
abilities of the network, but the right equation is not exploited at all.

We propose to learn two functions $\green(s)$ and $f(s)$ from states into
a state representation $E$, such that for any test function $\psi\from
S\to \R$, for any $s_1\in S$,
\begin{equation}
\label{eq:greenrep}
\green(s_1)\cdot \E_{s_2\sim \rho}
f(s_2)\psi(s_2)\approx \transp{\1_{s_1}}\,\Delta^{-1} \psi
\end{equation}

If the state space is finite, an exact
solution is indeed $E=\R^{\#S}$ with a one-hot representation for $f$,
and the true matrix $\Delta^{-1}$ for $\green$, namely,
\begin{equation}
f(s_2)=\1_{s_2}/\rho(s_2),\qquad
(\green(s_1))_{s_2}=(\Delta^{-1})_{s_1s_2}
\end{equation}
or equivalently, $f(s_2)=\1_{s_2}$ and
$(\green(s_1))_{s_2}=(\Delta^{-1})_{s_1s_2}/\rho(s_2)$. \footnote{The solution to
such representation problems is never unique because $E$ is an arbitrary
space and one may always make a change of basis in $E$. More precisely,
if $B$ is any invertible linear map on $E$, then changing $f\gets Bf$,
$\green\gets (\transp{B})^{-1}\green$ produces a new solution; indeed
this preserves $\green\cdot f$. This may
be useful to ``renormalize'' once in a while to help with numerical
conditioning: eg setting $f$ to have
unit covariance, or $f$ and $\green$ to have the same covariance.

Actually the most symmetric solution may be something like
$f(s_2)=\Delta^{-1/2}\1_{s_2}/\rho(s_2)$ and
$\green(s_1)=\transp{(\Delta^{-1/2})}\1_{s_1}$. This may be the fastest
to reach, by splitting eigenvalues of $\Delta$ halfway between $f$ and
$\green$.}
But with infinite or continuous state spaces, we have to find a more
compact representation.

The reason for this form is the following: $E$ is supposed to represent
distributions or functions over states. $f$ is supposed to map $s_2$ to
something like
$\1_{s_2}$ (up to a factor $1/\rho$ to compensate for sampling
probabilities). 
In
particular, $\E_{s_2\sim \rho}f(s_2)\psi(s_2)$ is supposed to represent
the vector with components $\psi(s_2)$, namely, the function $\psi$. $\green$ is supposed to map each
$s_1$ to the
the occupation
distribution of the random walk starting from $s_1$, namely,
$\transp{\1_{s_1}}\sum_t P^t$, which is $\transp{\1_{s_1}}\,\Delta^{-1}$
(as usual I
omit constants\footnote{To deal with constants, we probably have to be
able to predict \emph{differences} $\psi(s_2)-\psi(s_2')$, or just use
relative value functions and remove $\psi(s_0)$ for some $s_0$.}). Then the scalar product in
$E$ represents integration/summation over the state space.
This representation keeps linearity of the map $R\mapsto V$, while
allowing for a nonlinear $f$ and $\green$.

Of course we cannot directly learn $\green$ from \eqref{eq:greenrep},
since $\Delta^{-1}$ in the right-hand-side is exactly what we want to
learn. But $\Delta^{-1}$ is the unique solution to the Bellman equations \eqref{eq:greenbell}; after
applying \eqref{eq:greenbell} to an arbitrary state $s_1$ and test
function $\psi$, and
substituting the model $\green$ for $\Delta^{-1}$ via
$\eqref{eq:greenrep}$, the left Bellman equation becomes
\begin{gather}
\transp{\1_{s_1}}\,\Delta^{-1}\psi=\transp{\1_{s_1}}\,\psi+\transp{\1_{s_1}}\,P\Delta^{-1}\psi
\qquad \Longrightarrow
\\
\green(s_1)\cdot\E_{s_2\sim \rho}
f(s_2)\psi(s_2)
\approx \psi(s_1)+ \E_{s'\sim P(s'|s_1)}\green(s')\cdot
\E_{s_2\sim \rho}\,
f(s_2)\psi(s_2)
\end{gather}
because $\transp{\1_{s_1}}\,P=\E_{s'\sim P(s'|s_1)}\transp{\1_{s'}}\,$.
For a fixed $f$, 
this is basically a multiple Bellman equation on $\green$ with multiple
rewards $\psi$, and features $f\psi$.

Meanwhile,
the right Bellman equation becomes
\begin{gather}
\transp{\1_{s_1}}\,\Delta^{-1}\psi=\transp{\1_{s_1}}\,\psi+\transp{\1_{s_1}}\,\Delta^{-1}P\psi
\qquad \Longrightarrow
\\
\green(s_1)\cdot\E_{s_2\sim \rho}
f(s_2)\psi(s_2)
\approx \psi(s_1)+ \green(s_1)\cdot\E_{s_2\sim \rho}\E_{s'_2\sim P(s'_2|s_2)}
f(s_2)\psi(s'_2)
\end{gather}
(indeed, 
$P\psi$ is
represented by $\E_{s_2\sim \rho} f(s_2)(P\psi)(s_2)$, and
$(P\psi)(s_2)=\E_{s'_2\sim P(s'_2|s_2)}\psi(s'_2)$). If we view this as
an equation on $f$ for fixed $\green$, this imposes a kind of spatial
regularity of features $f$.

\emph{Both} these equations are samplable. Therefore, one may
parameterize $f$ and $\green$ by neural networks and train the
parameters. This works off-policy: we have not used that $\rho$ is the
invariant distribution, just some distribution on states we can sample
from.

Of course, we should apply this to relevant test functions $\psi$,
such as the reward function or any useful (predictive) features, namely,
any function we're interested in predicting the future values of. 
If just using a
random function $\psi$, there is very little signal to train
from. \footnote{An extreme case is to take a purely random $\psi$ at each
training step; then $\psi(s_1)$ and $\psi(s_2)$ are independent unless $s_1=s_2$,
which is a very unlikely event. This amounts to training with white noise
features. Formally it works thanks to the rare case $s_1=s_2$, but
variance is just terrible.} Even training only on $\psi=R$ should be
interesting: the left Bellman equation just becomes a particular
parameterization of the value function as $V(s_1)=\green(s_1)\cdot
\E_{s_2} R(s_2)f(s_2)$, so if the left Bellman equation is used, this
will approximate the true $V$.

Thus we may learn $\green$ and $f$ via TD-like algorithms. (Does it make
sense to use the left equation for $\green$ and the right equation for
$f$?) We may even have
double sampling that works, thanks to featurization (using test functions
$\psi$), at least for the right equation. TODO should we featurize on the
left as well? Or train only with the right equation?

When sampling for the right Bellman equation, it is wasteful to sample a
state $s_2$ and apply a test function $\psi$ if $\psi(s_2)$ is very
small. Instead one may want to sample $s_2$ according to
$\rho_\psi\deq \rho(s_2)\abs{\psi(s_2)}/\E_{\rho}\abs{\psi}$, in which case
$\E_{s_2\sim \rho} f(s_2)\psi(s_2)$ is equal to $(\E_{\rho}\abs{\psi})\E_{s_2\sim \rho_\psi}
f(s_2)\sgn(\psi(s_2))$. This will significantly reduce variance,
especially for test functions $\psi$ that are concentrated around a few
points.

The value function is obtained with $\psi=R$, namely,
\begin{equation}
V(s_1)=\transp{\1_{s_1}}\Delta^{-1}R\approx
\green(s_1)\cdot\E_{s_2\sim \rho}
f(s_2)R(s_2)
\end{equation}
Of course it is not convenient to have to sample from $s_2$. Instead one
might want to just learn the representation $f(R)$, defined by abuse of
notation as
\begin{equation}
f(R)\deq \E_{s_2\sim \rho} \,f(s_2)R(s_2)
\end{equation}
(which can be computed online)
and then
\begin{equation}
V(s_1)\approx \green(s_1)\cdot f(R)
\end{equation}
and of course it might be helpful to learn the remainder $V-\green\cdot
f(R)$ via ordinary TD, in case $R$ has no component along the features
learned.

More generally, we can learn the representation of any function $\psi$.
This could be useful if we want to train a few policy parameters to lead in
certain directions/lead to states with certain features.

Another equation that might be enforced is commutation between
$P$ and $\Delta^{-1}$; it is expressed similarly to the above, but I do
not know if there's a gain. It might transfer left to right Bellman
equations.

$\green$ is basically a model of the world (summed over time). The
difference with ordinary model-based is: $\green$ is trained as a fixed
point, considering all timescales at once. $\Delta^{-1}$ contains all
paths of all lengths from $s$ to $s'$. In ordinary model-based, the
model $\tilde P$ obviously commutes with its iterates ($\tilde P^t\tilde
P=\tilde P\tilde P^t$), but does not provide an easy and accurate
representation of the occupation measure $\tilde \Delta^{-1}=\sum_t \tilde P^t$. So even in
the model-based case, the above might be useful.

One may also compute something like $\Delta^{-1}_{s,a,s'}$ (paths that start
at $s$ with action $a$ and arrive at $s'$). Unclear if more useful.

Nico suggests the references around successor representation, proto-value
functions, by Marlos Machado.

\paragraph{Enumerative combinatorics, the middle Bellman equation, and
the Newton method for $\Delta^{-1}$.} As mentioned above, $(\Id-\gamma
P)^{-1}$ counts the number of paths from state $s_1$ to $s_2$, weighted
by their probability and $\gamma^\mathrm{length}$. When observing a new
transition $s\to s'$, the left Bellman equation on $P$ produces new paths
from $s$, by adding the edge $s\to s'$ to all paths from $s'$. Similarly
the right Bellman equation produces new paths to $s'$.

The Newton method for matrix inversion is a classical tool to compute
$\Delta^{-1}$. It consists in iterating
\begin{equation}
Z\gets 2Z-Z\Delta Z
\end{equation}
It converges extremely fast if initialized reasonably close to $Z^{-1}$ (namely if the spectral
radius of $\Id-ZX$ is less than $1$), but it can diverge if badly
initialized. A suitable initialization is $Z=\eps\transp{\Delta}$ for
small enough $\eps$. For stochastic estimates $\tilde \Delta$ of $\Delta$ (as we have
here through transitions), the stochastic update version with learning
rate $\eta$ is
\begin{equation}
Z\gets Z+\eta Z-\eta Z\tilde \Delta Z
\end{equation}

With $\Delta=\Id-\gamma P$, 
this method corresponds to a ``middle'' Bellman equation
\begin{equation}
\Delta^{-1}\gets \Delta^{-1}+\eta\Delta^{-1}-\eta \Delta^{-2}+\eta \gamma \Delta^{-1}P\Delta^{-1}
\end{equation}
Importantly, when a new transition $s\to s'$ is observed from $P$, this equation
produces new paths by connecting all paths that end at $s$ with all paths
starting at $s'$, thus doubling path length (thanks to the $\Delta^{-1}$
on the right and left of $P$). This doubling corresponds to the
quadratic convergence of the Newton method. (However, this method does not
produce new starting points or endpoints.)

Here we do not have access to $\Delta$, but have access to transitions
$s\to s'$, which are samples of
$\Delta$, namely, $\Delta=\sum_s\E_{s'\sim P(s'|s)} \1_s\transp{(\1_s-\gamma
\1_{s'})}$. This corresponds to a stochastic version of the Newton
method, with learning rate $\eta\ll 1$,
\begin{equation}
Z\gets Z+\eta Z-\eta Z \E_{ss'} \frac{1}{\rho(s)}\1_s\transp{(\1_s-\gamma \1_{s'})} Z
\end{equation}
Indeed, in expectation over $s\to s'$ this corresponds to $Z\gets Z+\eta
Z-\eta Z \Delta Z$, which converges to $Z=\Delta^{-1}$. This
also corresponds to the rank-one update formyla for matrix inversion,
$(D+\eta u\transp{v})^{-1}=D^{-1}-\eta
D^{-1}u\transp{v}D^{-1}+O(\eta^2)$. Namely,
if we update our estimate of $\Delta$ by
$\Delta\gets (1-\eta)\Delta+\eta \frac{1}{\rho(s)} \1_s\transp{(\1_s-\gamma
\1_{s'})}$ when a transition $s\to s'$ is observed, then
$\Delta^{-1}$ gets updated as above.

With $Z=\green\cdot \E_{\rho} f$ as above, everything is fully samplable
(thanks to the $\E_\rho$ which cancels the $1/\rho$ factors), see below.
It is not clear to me how to mix the left, right and middle Bellman
equations, but the correspondence with path combinatorics and the Newton
method suggests this
might be an important point for the navigation problem.

Namely, set
\begin{equation}
Z=\transp{\green}F, \qquad \green=\sum_s \green(s)\transp{\1_s},\qquad F=\E_{s\sim
\rho}
f(s)\transp{\1_s}
\end{equation}
which is the same representation as above, namely $Z=\Delta^{-1}$ is
equivalent to $\green(s)\cdot \E_{s'\sim
\rho} \psi(s')f(s')=\transp{\1_s}\Delta^{-1}\psi$ for all $s$ and test
functions $\psi$. Then the Newton equation for $Z$ reads
\begin{equation}
\transp{\green}F\gets (1+\eta)\transp{\green}F-\eta \transp{\green}F \E_{ss'} \frac{1}{\rho(s)}\1_s\transp{(\1_s-\gamma
\1_{s'})} \transp{\green}F
\end{equation}
and since $F\1_s=\rho(s)f(s)$ and
$\transp{\1_s}\transp{\green}=\transp{\green(s)}$, this simplifies to
\begin{equation}
\transp{\green}F\gets (1+\eta)\transp{\green}F-\eta
\transp{\green} \E_{ss'} f(s)\transp{(\green(s)-\gamma
\green(s'))}F
\end{equation}
and we propose to update separately for $\green$ given $f$ and $f$ given
$\green$: the advantage being that for fixed $f$, we can obtain the
update above by updating $\green$ via
\begin{equation}
\transp{\green}\gets (1+\eta)\transp{\green}-\eta
\transp{\green} \E_{ss'} f(s)\transp{(\green(s)-\gamma
\green(s'))}
\end{equation}
and likewise for $f$ given $\green$,
\begin{equation}
F\gets (1+\eta)F-\eta
\E_{ss'} f(s)\transp{(\green(s)-\gamma
\green(s'))}F
\end{equation}
both of which can be implemented via gradient descent on the parameters
of $\green$ and $f$. (This doubles the learning rate $\eta$.)
There is no need to sample on test functions.



(Equivalent to solving for $ \Id=\E_{ss'} f(s)\transp{(\green(s)-\gamma
\green(s'))}$, namely, $\Id=F\Delta\transp{\green}$ in the representation space $E$, but via a different gradient
descent.)

The Newton version should not be used alone. Indeed, with a matrix $Z$ of
rank $r$, it just settles on some $r$-dimensional subspace and inverts 
$\Delta^{-1}$ on that subspace. The subspace in question does not
change at all along optimization. (This is because the equations on $F$
and $\green$ leave $\Ker F$ and $\Img \transp{\green}$ invariant; more
precisely, $\Delta Z=\Id$ on $\Img \transp{\green}$ and $\Img(\Delta
Z-\Id)\subset\Ker F$)
From a purely algebraic point of view,
all these $r$-dimensional subspaces are equally important: there are as
many quasi-inverses as such subspaces, and the subspace is fixed at the
initialization of $F$ and $\green$. (For instance, if $r=1$ one can
choose any vector for $\transp{\green}$, and choose for $F$ any linear
form that evaluates to $1$ on $\Delta \transp{\green}$).

On the other hand the left and right Bellman equations prioritize
inverting $\Delta$ on test functions $\psi$. So this introduces
(implicitly) a quantitative criterion that breaks symmetry between
possible quasi-inverses.

Intuitively, the middle Bellman equation does not create new starting
points or endpoints on paths, only works in the middle. (This corresponds
to the fixed image and kernel.) [TODO: though if I start with $Z=\Id$ (a
path from every state to itself) and apply Newton then I do create paths,
so the meaning of this is unclear and initialization might be important.
It might be important to initialize $F$ and $\green$ so that $Z\approx \Id$.] Meanwhile, the left Bellman equation
creates starting points and the right Bellman equation creates endpoints
when observing a new transition $s\to s'$. This introduces spatial
smoothness over the image and kernel.

I still don't know how to rightly combine these three equations. I'm not
even sure this kernel/image thing is actually an issue: if $f$ and
$\green$ are represented as neural networks with some input features,
the kernel of $f$ is actually the set of functions/distributions on
states which have the same average features, and the image of
$\transp{\green}$ is the set of functions that can be represented as sums
$V(s)=\sum_i \alpha_i\green_i(s)$ over the components of $\green$: if $V$
indeed belongs to this class then we will compute it correctly. (But this
class does not change during training...)

The above can be turned to an infinite-rank approximation by writing
\begin{equation}
\transp{1_{s_1}}Z\psi=\E_{s_2\sim \rho}\, \E_{\omega}\,
\green(s_1,\omega)f(s_2,\omega)\psi(s_2)
\end{equation}
where integration is over $\omega$ in an arbitrary space (eg $\omega\in
[0;1]$ or $\omega\in \R^d$ with Gaussian distribution). This amounts to
sending each state to a function over $\omega$, and using the dot product
from $L^2(\omega)$. This should solve some kernel/image issues and
increase representational power. The equation
$\Id=F\Delta\transp{\green}$ becomes $\delta_{\omega\omega'}=\E_{s\sim
\rho, \,s'\sim P(s'|s)}
f(s,\omega)(\green(s,\omega')-\green(s',\omega'))$ TODO CHECK, which is not easy to
solve because of the singular $\delta_{\omega\omega'}$, but should be better with things on the right and left.

\paragraph{Better variant?}
Everything works equally well if we just learn a kernel $Z(s_1,s_2)$ such
that
\begin{equation}
\transp{\1_{s_1}}\Delta^{-1}\psi\approx \E_{s_2\sim \rho} \,Z(s_1,s_2)
\psi(s_2)
\end{equation}
and this is still linear in $\psi$. This represents the inverse Laplace
operator by
\begin{equation}
Z=\sum_{s_1,s_2}\rho(s_2)  Z(s_1,s_2)\1_{s_1}\transp{\1_{s_2}}
\end{equation}
[TODO CHANGE NOTATION because currently the entries of $Z$ are not
$Z(s_1,s_2)$ but $Z_{s_1s_2}=\rho(s_2)Z(s_1,s_2)$ which is confusing...]
In particular the value function is
\begin{equation}
V(s_1)=\E_{s_2\sim \rho} \, Z(s_1,s_2) R(s_2)
\end{equation}
Downside: this representation of the value function does not allow for first
computing a representation of $R$ as $\E_{s_2} R(s_2)f(s_2)$ online, then taking a dot product with
$\green$. Here we would have to learn an auxiliary network to imitate
$\E_{s_2\sim \rho} Z(s_1,s_2) R(s_2)$ (but this is a supervised learning
problem; still, it looks suspiciously like $TD(1)$ except maybe for the
$O(1/(1-\gamma))$ variance). Alternatively, policy gradient can just directly be obtained
via
\begin{equation}
\partial \pi(a|s)\E_{s'|s,a} \E_{s_2\sim \rho} \, Z(s',s_2)R(s_2)
\end{equation}
(this works even if $\rho$ is \emph{not} the invariant distribution). We
can apply this each time a new reward is observed at some point $s_2$. We
can even set another arbitrary distribution on $s_1$: this policy
gradient will maximize
the reward obtained from a starting point from that distribution.

The left Bellman equation is
\begin{gather}
\transp{\1_{s_1}}\,\Delta^{-1}\psi=\transp{\1_{s_1}}\,\psi+\transp{\1_{s_1}}\,P\Delta^{-1}\psi
\qquad \Longrightarrow
\\
\E_{s_2\sim \rho}
\,Z(s_1,s_2)\psi(s_2)
\approx \psi(s_1)+ \E_{s'\sim P(s'|s_1)}
\E_{s_2\sim \rho}\,
Z(s',s_2)\psi(s_2)
\end{gather}
for any $s_1$ and $\psi$.
The right Bellman equation becomes
\begin{gather}
\transp{\1_{s_1}}\,\Delta^{-1}\psi=\transp{\1_{s_1}}\,\psi+\transp{\1_{s_1}}\,\Delta^{-1}P\psi
\qquad \Longrightarrow
\\
\E_{s_2\sim \rho}\,
Z(s_1,s_2)\psi(s_2)
\approx \psi(s_1)+ \E_{s_2\sim \rho}\E_{s'_2\sim P(s'_2|s_2)}\,
Z(s_1,s_2)\psi(s'_2)
\end{gather}
These equations impose a kind of smoothness on $Z$ with respect to the
random walk $P$.

The Newton update/middle Bellman equation is
\begin{equation}
Z\gets (1+\eta)Z-\eta Z^2+\eta \gamma ZPZ
\end{equation}
which yields (after applying to $\1_{s_1}$ on the left and $\1_{s_2}/\rho(s_2)$
on the right)
\begin{multline}
Z(s_1,s_2)\gets (1+\eta)Z(s_1,s_2)-\eta \E_{s_3\sim \rho}\,
Z(s_1,s_3)Z(s_3,s_2)\,+\\\eta \gamma \E_{s_3\sim\rho,\, s'_3\sim
P(s'_3|s_3)}\,
Z(s_1,s_3)Z(s'_3,s_2)
\end{multline}
which is perfectly samplable. We can simulate the assignment
$Z(s_1,s_2)\gets...$ via gradient descent on the parameters of
$Z(s_1,s_2)$. \footnote{Should we adjust the size of the gradient descent
to get the right change of $Z(s_1,s_2)$? If we just use SGD, since
gradients may change from point to point, the speed may be different at
various states and we depart from Newton. This is clear when $Z$ is very
small: Newton grows exponentially while a standard gradient descent would
make small steps.} (Or via double sampling, which is not problematic here!?)

Initialization: A first idea would be to start with paths from $s$ to
$s$, namely, initialize
$Z$ so that $Z\approx \Id$ at time
$0$, namely $Z(s,s)=1/\rho(s)$, although starting with $Z(s,s)=1$ should be
quite fine. (Small initializations of $Z$ are safer.) $Z>0$ (unless we
work with $\gamma=1$ and relative stuff), so the parameterization should
reflect that. Indeed the Newton iteration is unstable with negative
values.

Another initialization might be $Z(s_1,s_2)\approx \frac{1}{1-\gamma}$
for every pair $s_1,s_2$. Indeed, on-policy, $Z$ is the density with
respect to the invariant distribution $\mu$, and for $\gamma$ close to
$1$, $Z$ will be approximately $\frac{1}{1-\gamma}$ plus a smaller function (see
below). Note that $Z(s_1,s_2)=\frac{1}{1-\gamma}$ is an exact solution of
the middle Bellman equation (as is $Z=0$), but it is a repulsive fixed
point, I think, so initializing to $\frac{1}{1-\gamma}$ plus noise should
converge to the true solution. Initializing there should have the
advantage of starting close to reasonable values.

In the end this Newton equation alone might be sufficient to
encompass the left and right Bellman. It does not rely on test functions,
but the smoothing is implicit via the model used for $Z(s_1,s_2)$.

Newton has a potential to be numerically less stable: first because of
the $(1+\eta)$ factor which makes $Z$ quickly grow when it's small (until
the $-Z^2$ kicks in), then because of the $-Z^2$ factors which can be
problematic when $Z$ is large. Small enough learning rates should be
stable. (TODO NOT TRUE: even in 1d, Newton is unstable if initialized to
a negative number, for any learning rate. Stable only in some
neighborhood of the true solution, but then, the stochastic noise might
produce divergence... this is potentially a serious issue)

Ideally, given the transition $s_3\to s'_3$, I'd like to sample $s_1$ and
$s_2$ proportionally to $Z$ (would cancel the $Z$ factors, and avoid
applying the equation to states that have nothing to do with
$s_3$—although the $Z$ factors are different for the three terms...). But
I don't know how to build an efficient sampler from $Z(s_3,\cdot)$.
Something like Metropolis? but this function $Z$ is not constant...

On-policy, sampling from $Z(\cdot,s_3)$ is easy: just sample one of the
recent states with geometric rates $\gamma$. This should divide the
right part of the equation by $Z(s_1,s_3)$ (up to some $\gamma$-dependent
factor). Thus we would sample two states $s_1,s_2$ along the trajectory,
sample a state $s_3$ in between on the trajectory from $s_1$, and (with
the right $\gamma$ factors) we would get a term with $Z(s_1,s_2)$, a term
with $Z(s_3,s_2)$ and a term with $Z(s'_3,s_2)$. This looks suspiciously
like a large-scale Bellman from $s_1$... Variance could be higher since
we don't use the model $Z$ and use sampling instead; plus, we lose any
generalization introduced by $Z$. To be tested. Write
carefully because $s'_3$ should be uncorrelated from $s_2$!! So not
possible with a single
trajectory?
On-policy, another
option is to sample triplets $(s_1,s_3,s_2)$ that are some time apart on
the trajectory, e.g., starting with $s_3$ uniform along the past
trajectory, and moving back for $s_1$ and forward for $s_2$ (with factors
$\gamma$). Looks a lot like eligibility traces?

On-policy, working with $\gamma\to 1$ is quite easy: indeed $Z(s_1,s_2)$ is
a density with respect to $\mu(s_2)$ so that the asymptotics when
$\gamma\to 1$ is $Z(s_1,s_2)=\frac{1}{1-\gamma}+Z_0(s_1,s_2)$, and we can
directly learn $Z_0$ by inserting $\frac{1}{1-\gamma}+Z_0$ in the middle
Bellman equation. This yields (omitting the expectations)
\begin{multline}
Z_0(s_1,s_2)\gets (1+\eta)Z_0(s_1,s_2)-\eta
Z_0(s_1,s_3)(Z_0(s_3,s_2)-\gamma Z_0(s'_3,s_2))\\-\eta
Z_0(s_1,s_3)+\frac{\eta}{1-\gamma}(\gamma Z_0(s'_3,s_2)-Z_0(s_3,s_2))
\end{multline}
TODO CHECK which we cannot directly apply with $\gamma=1$...
However ON-POLICY $s_3$ and
$s'_3$ have the same law, so in expectation the last term is $-\eta
\E_{s_3} Z_0(s_3,s_2)$. This yields the more symmetric
\begin{multline}
Z_0(s_1,s_2)\gets (1+\eta)Z_0(s_1,s_2)-\eta
Z_0(s_1,s_3)\left(Z_0(s_3,s_2)-\gamma Z_0(s'_3,s_2)\right)\\-\eta
Z_0(s_1,s_3)-\eta Z_0(s_3,s_2)
\end{multline}
on-policy, for any $\gamma\leq 1$.

[TODO: Since the standard iteration is unstable for $Z<0$, this means the
iteration on $Z_0$ will be unstable for $Z_0<-\frac{1}{1-\gamma}$.
Use that somehow: clip at that value?]

\paragraph{Early version of the above: Learning to navigate.} Define a function $V(s,s')$ that
represents the time spent in $s'$ if starting at $s$ and acting optimally
towards $s'$ (just as if the reward was $\1_{s'}$). It is defined by an iterated expectimax to reach $s'$, and
satisfies the Bellman equation
\begin{equation}
V(s,s')=\1_{s=s'}+\gamma \max_a \E_{s_1|s,a} V(s_1,s')
\end{equation}
and it also satisfies
\begin{equation}
V(s,s')=\1_{s=s'}+\gamma \sum_{s_{t-1}} V(s,s_{t-1})\max_a
\E_{s_t|s_{t-1},a}\1_{s_t=s'}
\end{equation}
although it's not clear to me how to exploit the last expression. 
\footnote{On-policy it works, I think, because on-policy we can sample
reverse transitions. But I think in that case $V(s,s')$ has to be defined
as a density wrt $\mu(s')$.\\Should also work if we maintain a low-rank
estimate of $V$ as a sum of Diracs over a few $s'$.}
But I
think we have to work from both ends if we want to represent
compositionality of actions. Indeed with the first equation, different values
of $s'$ are just completely separate Bellman problems; the generalization
abilities of the function approximator for $V$ may help, but only so
much. Each time we observe a transition $s\to s'$, we should be able to
update the probabilities to reach $s'$ based on the probabilities to
reach $s$.

Then we may learn a goal-oriented policy $\pi_\theta(a|s,s')$ that returns an
action $a$ at state $s$ which produces a move towards $s'$. Learning
$\pi_\theta$ may be done by observing transitions and maximizing
$V(s,s')$.

It's not clear either how to exploit $V(s,s')$ to maximize a reward. If
we were considering a fixed policy we would have
$V(s)=\sum_{s'}V(s,s')R(s')$ (namely $V(s,s')$ would be the value
function of the reward function $\1_{s'}$). But here $V(s,s')$ uses a
strategy that depends on the goal $s'$.

Another possibility is to learn a goal-oriented policy towards desired
\emph{features} of the final state $s'$. Given a basis $(\Psi_i)$ of
features, and a set of coefficient $\alpha_i$, one might want
$\pi_\theta(a|s,(\alpha_i))$ to produce a policy that maximizes the
expected reward $\sum \alpha_i \Psi_i$ (thus, simultaneously learning to
solve a number of RL problems). 
This can be done either via learning a value function $V(s,(\alpha_i))$
then actor-critic, or via an eligibility-trace-like thing (but then it
has to be on-policy, for which policy??)

See \emph{Universal value function approximators},
Schaul--Horgan--Gregor--Silver ICML 2015

Then we can learn an approximate decomposition of the actual reward over
the $\Psi_i$, and we know what to do. (This assumes the reward is well
approximated by the $\Psi_i$; or that we learn the complement.) Advantage
= even if there is no reward signal for some time, we still learn to
navigate the environment.

There are many ways to build interesting
$\Psi_i$'s: hidden features of a world model/reward model, eigenfunctions
of $P$, or just setting a basis of random time-dependent functions on a
trajectory and smoothing them via a NN (``zeitgeist'').

(The relationship between $V(s,s')$ and $V(s,(\alpha_i))$ is: the latter
is more general, by taking a full basis of Dirac features $\1_{s'}$. Then
$V(s,(\alpha_i))$ does not just learn how to go to individual states, but
how to maximize time spent in an arbitrary weighted sum of states. This
is why we can later extend to an arbitrary reward function.)

\end{document}

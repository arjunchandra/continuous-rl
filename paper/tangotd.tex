\documentclass[11pt,a4paper]{article}
\input{header.en.tex}



\begin{document}

We consider the Bellman equation for a policy $\pi$ in a finite MDP with
expected reward $R$ and transition probability matrix $P$ ($P$ includes $\pi$ and
the environment response). The Bellman equation is
\begin{equation}
(\Id-P)V=R
\end{equation}

We assume we can sample transitions $(s,a,r,s')$ according to some
distribution $\rho$ on $s$. In the on-policy case, $\rho$ will be the
stationary distribution. Typically $a$ has to be taken from $\pi$ (or a
distribution close to $\pi$ after resampling/by using only those $a$
sampled from $\pi$; this covers $\eps$-greedy) so that we do get
transitions from
$P$. (Alternatively one could work with pairs $(s,a)$ TODO.) Thus, we
have access to unbiased samples of $\rho P$ and $\rho R$:
\begin{equation}
\rho P=\E[\1_{s}\transp{\1_{s'}}\,],\qquad \rho R=\E[r(s,a,s')\1_{s}]
\end{equation}

If we parameterize $V$ by some parameter $\theta$, we can apply
parametric TD. The expected TD step is
\begin{equation}
\theta\gets \theta -\eta \,\transp{\Phi}\rho (V-PV-R)
\end{equation}
where 
\begin{equation}
\Phi\deq \frac{\partial V}{\partial \theta}
\end{equation}
is the Jacobian matrix of $V$ with respect to its parameter. The linear
case $V=\Phi \theta$ corresponds to constant $\Phi$. The tabular case is
$\Phi=\Id$.

A fixed point of TD solves
\begin{equation}
\transp{\Phi}\rho(\Id-P)V=\transp{\Phi}\rho R
\end{equation}
and if the system is overparameterized ($\Phi$ invertible) and
well-sampled ($\rho>0$) this implies $(\Id-P)V=R$. All terms in this
equation are \emph{samplable}: we can sample from $\rho R$ and from
$\rho(\Id-P)$.

A local minimizer of the $L^2(\rho)$ error between $V$ and
$(\Id-P)^{-1}R$ solves
\begin{equation}
\transp{\Phi} \rho V=\transp{\Phi}\rho (\Id-P)^{-1} R
\end{equation}
which is not quite the same as the above. A local minimizer of the
$L^2(\rho)$ error between $(\Id-P)V$ and $R$ is
\begin{equation}
\transp{\Phi}\transp{(\Id-P)}\rho(\Id-P)V=\transp{\Phi}\transp{(\Id-P)}\rho R
\end{equation}
and in the linear case $V=\Phi\theta$, this is the standard solution to
the linear regression problem $(\Id-P)\Phi\theta\approx R$ in
$L^2(\rho)$.

The Dirichlet norm associated with $\rho$ is $\E_{ss'}
(f(s)-f(s'))^2$. Its matrix is $\rho+\rho'-\rho P-\transp{(\rho P)}$ where
$\rho'$ is the distribution of $s'$.
A local minimizer of the Dirichlet error between $V$ and
$(\Id-P)^{-1}R$ solves
\begin{equation}
\transp{\Phi} (\rho+\rho'-\rho P-\transp{(\rho P)})(V-(\Id-P)^{-1}R)=0
\end{equation}
If $\rho$ is the stationary distribution $\mu$ and $P$ is reversible, one
has $\transp{(\mu P)}=\mu P$ so $\rho+\rho'-\rho P-\transp{(\rho
P)}=2\mu(\Id-P)$ and this is equivalent to
\begin{equation}
\transp{\Phi}\mu ((\Id-P)V-R)=0
\end{equation}
namely, the same as a fixed point of TD.

\bigskip

TD is the only one of those fixed point equations that is
\emph{samplable} given that we can sample $\rho$, $\rho P$ and $\rho R$.
\footnote{Actually the minimizer of the $L^2(\rho^2)$ error between $R$
and $(\Id-P)V$ is also samplable:
\begin{equation}
\transp{\Phi}\transp{(\Id-P)}\rho^2(\Id-P)V=\transp{\Phi}\transp{(\Id-P)}\rho^2
R
\end{equation}
however, the $\rho^2$ is a bit weird...
}
Thus, from now on we are going to focus on solving
\begin{equation}
\transp{\Phi}\rho (\Id-P)V=\transp{\Phi}\rho R
\end{equation}
using various SGD-like solvers.

We now switch to the linear case and will de-linearize later. This
becomes
\begin{equation}
\transp{\Phi}\rho (\Id-P)\Phi \theta=\transp{\Phi}\rho R
\end{equation}
to be solved in $\theta$.

\paragraph{Solving $A\theta=b$ where $A$ and $b$ are samplable.} An
obvious way to solve $A\theta=b$ via a stochastic gradient method is
\begin{equation}
\theta\gets \theta - \eta (A\theta-b)
\end{equation}
using samples for $A$ and $b$.
This is TD. If $A$ is symmetric positive definite, this is also the
gradient descent of the loss function
$\transp{(A\theta-b)}A^{-1}(A\theta-b)$. However, we want to apply this
to matrices $A$ that are not necessarily of this type.

This works only if $A$ is \emph{stable}.


\begin{defi}
A matrix $A$ is \emph{stable} if one of the following equivalent
conditions are satisfied:
\begin{itemize}
\item All the eigenvalues of $A$ have positive real part.
\item The matrix $(\Id-\eps A)$ has spectral radius $<1$ for small enough
$\eps$.
\item The differential equation $\theta'=-A\theta$ converges to $0$ for
any initial value.
\item There exists a symmetric, positive definite matrix $L$ (Lyapunov
function) such that $\transp{\theta}L\theta$ is decreasing along the
solutions of the differential equation $\theta'=-A\theta$.
\item There exists a symmetric, positive definite matrix $L$ such that
\begin{equation}
\transp{\theta} L A \theta>0
\end{equation}
for all $\theta\neq 0$. (This is the same $L$ as in the previous
condition.)
\end{itemize}
\end{defi}

In particular, a symmetric, definite positive matrix is stable (last
criterion with $L=\Id$).
Stability is invariant by matrix similarity $A\gets B^{-1}A B$, since
this preserves eigenvalues.

Since the
solution of $\theta_t'=-A\theta_t$ is $\theta_t=e^{-tA}\theta_0$, a
Lyapunov function that works is $\transp{\theta}L\theta=\int_{t \geq 0}
\norm{\theta_t}^2$, namely
$L=\int_{t\geq 0} \transp{(e^{-t A})}
e^{-tA}$.

\paragraph{Making the TD iteration stable.} In linear TD the matrix $A$ is
\begin{equation}
A=\transp{\Phi}\rho (\Id-P)\Phi
\end{equation}
and we know it is stable in several cases: (TODO here I just remove
constants)
\begin{enumerate}
\item On-policy: $\rho=\mu$, the invariant distribution. Then we know that $\rho(\Id-P)$ is positive
(Dirichlet form). This implies that $\transp{\Phi}\rho (\Id-P)\Phi$ is
positive too. So we recover the convergence result on-policy for linear
TD.
\item In the tabular case: $\Phi=\Id$. Indeed we have $A=\rho (\Id-P)$,
which is not positive in general, but since $\mu (\Id-P)$ is, we can take
$L=\mu/\rho$ so that $LA$ is positive.
\end{enumerate}

One can also \emph{precondition} the gradient descent. This can make it
stable and/or make convergence faster. The step becomes
\begin{equation}
\theta\gets \theta-\eta B(A\theta-b)
\end{equation}
for some matrix $B$.

Of course the values of $B$ that are most interesting are obtained as the
inverse of something; for instance $B=A^{-1}$ solves the problem
instantly... Two-timescale methods or TANGO-like algorithms can apply an
inverse of something to $A\theta-b$, via an auxiliary gradient descent.
TANGO works for stable matrices, if they are samplable \footnote{In
TANGO, the consecutive steps absolutely have to be \emph{independent}. So even in
the on-policy case, we will need a replay buffer from $\mu$ for the auxiliary
gradient descent.}

A few choices are:
\begin{itemize}
\item $B=A^{-1}$: obviously stable, jumps to the solution in one step if
$\eta=1$. Requires computing the inverse of $A$, namely, solving the
problem. But if $A$ is stable and samplable, this can be done via TANGO.

\item $B=(\transp{\Phi}\rho\Phi)^{-1}$. Then we have to prove that
$(\transp{\Phi}\Phi)^{-1}\transp{\Phi}\rho(\Id-P)\Phi$ is stable. Taking
$L=\transp{\Phi}(\mu/\rho)\Phi$, and computing
\begin{align}
\transp{\theta}LBA\theta&=
\transp{\theta}L(\transp{\Phi}\rho\Phi)^{-1}\transp{\Phi}\rho(\Id-P)\Phi\theta
\end{align}
and note that $(\transp{\Phi}\rho\Phi)^{-1}\transp{\Phi}\rho$ is the
solution of linear regression with design matrix $\Phi$ in $L^2(\rho)$.
This implies that
\begin{equation}
\Phi(\transp{\Phi}\rho\Phi)^{-1}\transp{\Phi}\rho=\Pi_\Phi
\end{equation}
where $\Pi$ is the orthogonal projector on the image of $\Phi$ in
$L^2(\rho)$. Now taking
\begin{equation}
L=\transp{\Phi}(\mu/\rho)\Phi
\end{equation}
we have
\begin{equation}
\transp{\theta}LBA\theta=\transp{\theta} \transp{\Phi}(\mu/\rho)\Pi_\Phi
(\Id-P)\Phi\theta
\end{equation}
\end{itemize}

\end{document}

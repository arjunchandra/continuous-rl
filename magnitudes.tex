%!TEX root = main.tex

\section{Intuitions}
\label{sec:intuitions}
% {{{
Reinforcement learning aims at tackling a wide variety of real world problems.
Among them obviously stand physical problems, where an agent interacts with a
physical environment, e.g.\ the real world.

Resistance to a change of time discretization is a desirable property for
algorithms aiming at attacking the reinforcement learning field in physical
setups: such algorithms should only improve as the time discretization is
refined, as refining the time discretization can only increase the amount of 
eploitable information.

However, some reinforcement learning algorithms such as \emph{Q-Learning} are
ill-behaved in the regime of small time discretization. In what follow, we
analyze in an handwavy manner the reason for this failure, and give an alternative
algorithm that remains viable in the limit of small discretizations.

% }}}

\subsection{Notations}
% {{{
In what follows, we loosely consider the notion of a $\emph{Markov Decision Process}$
$\mathcal{M}$ with discretization step $\deltat$. Such an MDP is the discretization
of a continuous MDP with discretization step $\deltat$.
Formally,
\begin{align}
	s_{t + \deltat} &= s_t + F(s_t, a_t) \deltat + \gauss(0, C(s_t, a_t)) \sqrt{\deltat}\\
	r(s, a) &= \tilde{r}(s, a) \deltat + \gauss(0, \sigma_r(s, a)) \sqrt{\deltat}.
\end{align}
Policies are functions from the state space $\statespace$ to distributions on the
$\actionspace$
\begin{equation}
	\pi\colon \statespace \mapsto \mathcal{D}\left(\actionspace\right).
\end{equation}
The probability of an action $a$ in a given state $s$ under policy $\pi$ is
denoted by $\pi\left(a\mid s\right)$.

Durations of the order of $\deltat$ are referred to as infinitesimal
timesteps, while durations of the order of $1$ are referred to as timesteps.
If the discount factor in timesteps is $\gamma$, the corresponding
discount in infinitesimal timesteps is
\begin{equation}
	\gamma^\deltat \approx (1 - (1 - \gamma \deltat)).
\end{equation}

The state value function and action-state value function under policy
$\pi$ are defined as
\begin{align}
	Q^\pi(s, a) &= \BE_\pi\left[
		\sum\limits_{k=0}^\infty
	{\gamma}^{k \deltat} r(s_k, a_k)
		\mid
		s_0 = s, a_0 = a
	\right]\\
	&= \BE_\pi\left[
		\sum\limits_{k=0}^\infty
		{\gamma}^{k \deltat} \tilde{r}(s_k, a_k) \deltat
		\mid
		s_0 = s, a_0 = a
	\right]\\
	V^\pi(s) &= \BE_\pi\left[Q^\pi(s_t, a_t) | s_t=s\right].
\end{align}
Both functions satisfy recurrent \emph{Bellman equations}, e.g.
\begin{align}
	Q^\pi(s, a) &= \BE_\pi\left[
		\tilde{r}(s_t, a_t) \deltat + \gamma^\deltat V^\pi(s_{t + \deltat})
		\mid
	s_t = s, a_t = a\right]
\end{align}
as well as $K$ steps \emph{Bellman equations}, for any $K$, and notably for
$K = \frac{1}{\deltat}$, e.g.
\begin{align}
	V^\pi(s) = \BE_\pi\left[
		\sum\limits_{k=0}^{\frac{1}{\deltat} - 1} \gamma^{k\deltat} \tilde{r}(s_k, a_k) \deltat + \gamma V^\pi(s_{t + 1})
		\mid
	s_0 = s\right].
\end{align}
With a fixed policy $\pi$, the trajectory of states in rollouts is a
\emph{Markov Chain}. Under mild conditions, there exists a stationnary
distribution $\rho$ for this markov chain. With those notations, the goal to
maximize, the average return, is defined as
$\BE_{s\sim\rho}\left[V^{\pi}(s)\right]$.
Optimal $V$ and $Q$ functions, i.e. $V^*=V^{\pi^*}$ and $Q^*=Q^{\pi^*}$ with
$\pi^*$ the policy that maximizes the average return, also satisfy Bellman equations
equations, i.e.
\begin{align}
	Q^*(s, a) &= \BE\left[
		\tilde{r}(s, a) \right] \deltat + \gamma^\deltat \max\limits_{a'} \BE\left[Q^*(s_{t + \deltat}, a')
		\mid
	s_t = s, a_t = a\right].
	\label{eq:optq}
\end{align}
Popular approaches to optimizing
the average return mainly fall in two categories:
\begin{itemize}
	\item $Q$-learning approaches, which use~\eqref{eq:optq} as a fixed point equation
		to derive iterative approximations of the optimal $Q$ function.
	\item Policy gradient approaches, which search through a parameterized-sub family of
		all policies in order to optimize the average return through gradient descent. 
		It is possible to estimate the gradient of the average return
		using the policy gradient theorem, which yields
		\begin{equation}
			\nabla_\theta\BE_{s\sim\rho_\theta}\left[V^{\pi_\theta}(s)\right]
			=
			\BE_{s\sim\rho_\theta, a\sim\pi(\cdot\mid s)}\left[
				Q^{\pi_\theta}(s, a)
				\nabla_\theta \log \pi_\theta(a \mid s)
			\right].
		\end{equation}
\end{itemize}
Both approaches require estimating $Q^{\pi_\theta}$, and we will show that these
estimations need to be precise up to $\bigO{\deltat}$.

% }}}

\paragraph{$Q$-functions and $V$-functions only differ by $\bigO{\deltat}$}
\label{sec:QContinuous}
% {{{
\emph{Q-learning} uses the state-action value function to determine which
action optimizes the future return.

However, in continuous time, the state-action value function no longer carries
any information on which action yields the best return. Intuitively, as the
discretization step goes to zero, the expected difference in resulting state
values between two actions is of magnitude $\bigO{\deltat}$. Consequently, the
state-action value functions only differ by a factor of magnitude
$\bigO{\deltat}$.

Formally,
\begin{align}
	&V^\pi(s) - Q^\pi(s, a)\\ 
	= &\BE_{a'\sim \pi(\cdot\mid s)}
	\left[\tilde{r}(s, a') - \tilde{r}(s, a)\right] \deltat
	+
	\gamma^\deltat 
	(\BE\left[V^\pi(s_{t+\deltat}) \mid s_t=s\right]
	-
	\BE\left[V^\pi(s_{t+\deltat}) \mid s_t=s, a_t=a\right]).\nonumber
\end{align}
Now $s_{t + \deltat} = s_t + \F(s_t, a_t) \deltat +
\gauss(0, C(s_t, a_t)) \sqrt{\deltat}$, thus 
\begin{equation}
	V(s_{t+\deltat}) = V(s_t) + \partial_s V(s_t) \delta s_t
	+ \frac{1}{2}\mathrm{tr}(C(s_t, a_t) H_V(s_t))\deltat.
\end{equation}
In expectation, irrespective of wether the expectation is conditionned on $a$,
$V(s_{t+\deltat})$ and $V(s_t)$ are at distance $\bigO{\deltat}$. Consequently,
$V^\pi(s) - Q^\pi(s, a) = \bigO{\deltat}$.

Notably, decision based on the values or relative values of an approximation of
the $Q^\pi(s, a)$'s for different $a$ are only meaningful when the
approximation is tighter than $\bigO{\deltat}$, at least in term of relative
values, i.e.  
\begin{equation*}
	|\tilde{Q}^\pi(s, a) - \tilde{Q}^\pi(s, a') - Q^\pi(s, a) +
	Q^\pi(s, a')| = \bigO{\deltat}.
\end{equation*}

Approximating with this degree of precision becomes harder as $\bigO{\deltat}$
decreases. Typically, if the Q-function is learnt only from \emph{Monte Carlo} samples
of the return, the observed variance of the quantity to learn is of order
$\frac{1}{1 - \gamma}$. An optimist estimate of the number of
samples required to learn this quantity up to order $\bigO{\deltat}$ is 
$\bigO{\frac{1}{(1 - \gamma) \deltat^2}}$. If a sampling rate of $\deltat$ is
assumed, the time it takes to learn $Q$ up to the correct degree of precission,
increases linearily with $\frac{1}{\deltat}$. Indeed, in a single real time step, with a
sampling rate of $\deltat$, $\frac{1}{\deltat}$ samples are aquired. Consequently,
$\frac{1}{(1 - \gamma) \deltat}$ timesteps are required to obtain an estimate up
to the desired precision. As $\deltat \to 0$, it takes an infinite number of samples
to obtain a viable estimate of $Q$, rendering $Q$-learning methods theoretically
unsound.
% \NDC{This is not convincing at the moment: as $\deltat$ decreases,
% 	the number of observations made increases, and this may help get a better
% 	estimate $\tilde{Q}^\pi(s, a)$. My guess is that the number of observations
% 	required to obtain an approximation of order $\varepsilon$ is superlinear in
% 	$\frac{1}{\varepsilon}$ and thus the previous argument holds. We either need
% a simple theoretical setup where this works or an experimental one.}
% }}}

\paragraph{Policy gradient methods in continuous time}~\\
% {{{
% }}}

\paragraph{Directly learning the advantage function.}
% {{{
(See Kenji Doya, Reinforcement Learning in Continuous Space and Time,
Neural Comp.\ 2000.)
Let the system be defined by a state- and action-driven stochastic
differential equation
\begin{equation}
s_{t+\deltat}=s_t+\deltat F(s_t,a_t)+\sqrt{\deltat}\mathcal{N}(0,C(s_t))
\end{equation}
where we assume that the covariance noise does not depend on $a_t$. (If
$a_t$ itself is noisy, this will produce noise on $F$, but this noise
would typically be of order $\deltat$ unless the noise on $a_t$ is of
order $1/\sqrt{\deltat}$.)

Then with $\gamma=1$ the Bellman equation is
\begin{align}
Q(s_t,a_t)=&\E[ \reward_t(s_t,a_t) \deltat + V(s_{t+\deltat}) \mid a_t]
\\&=V(s_t)+\deltat \,\E\left[\reward_t(s_t,a_t)+\frac{\partial V(s_t)}{\partial
s}F(s_t,a_t)+\frac12 \Tr \left(C(s_t)\frac{\partial^2 V(s_t)}{\partial
s^2}\right)\mid a_t\right]+o(\deltat)
\end{align}

We only need to compute the advantage function $A$ defined by
$Q(s_t,a_t)\eqd V(s_t)+\deltat A(s_t,a_t)$; or just the advantage function
up to any term that depends only $s_t$ but not on $a_t$. By isolating the
part that
depends on $a_t$ in the above, the advantage function is
\begin{equation}
A(s_t,a_t)=\E\left[\reward_t(s_t,a_t)+\frac{\partial
V(s_t)}{\partial
s}F(s_t,a_t)\mid a_t\right]+\cdots(s_t)+o(1)
\end{equation}

Thus it is enough to learn a model of the instantaneous reward, a model
of the value function $V$ (sufficiently regularized to allow for
estimation of $\partial V(s)/\partial s$), and a model of the
instantaneous dynamics $F(s,a)$.

This extends to situations with continuous time and continuous or
discrete space, where the state jumps from $s$ to $s'$ with probability $\deltat
L(s,a,s')$ at each time. (Here $L$ is signed, with $L(s,a,s)$ usually
negative to represent the jump from $s$; $L$ is the generator of the
Markov process.) Writing $p_J(s,a,s')$ the law of
$s'$ knowing that a jump has occurred, and $\deltat J(s,a)$ the
probability of jumping, we have $L(s,a,s')=J(s,a)(p_J(s,a,s')-\delta(s))$
and
\begin{equation}
A(s_t,a_t)=\E\left[\reward_t(s_t,a_t)+J(s_t,a_t)\,\E_{s'\sim
p_J(s_t,a_t,s')} (V(s')-V(s_t))
\mid a_t\right]+\cdots(s_t)
\end{equation}

This also extends to any combination of continuous trajectories and
probabilistic jumps with probability $O(\deltat)$. However, this does not
cover some kinds of jumps, such as jumps occurring with probability $1$
if some part of the state reaches some threshold. Indeed, this relies on
$s_{t+\deltat}$ being somehow close to $s_t$, either in distance or in
probability.

(Should also check ``Advantage updating'' Baird 1993, Harmon et al 1996.)
% }}}

\paragraph{Questions left unanswered with advantage learning.}
% {{{
As mentionned, advantage learning requires learning the gradient of $V$
with respect to the states, or, in the discrete case, the difference of
$V$'s between two neighboring rescaled by $\frac{1}{\deltat}$. Now, as
mentionned in~\ref{sec:QContinuous}, $V$ can only be learned up to $\bigO{1}$
in reasonable time. Consequently, $V(s) - V(s') = \bigO{1}$ in the general case,
and $\frac{V(s) - V(s')}{\deltat} = \bigO{\frac{1}{\deltat}}$. This means that
the learning procedure of $A$ can endure a bias of order $\bigO{\frac{1}{\deltat}}$
which goes to $+\infty$ as $\deltat \to 0$.

One could think that adding a gradient penalization to the learning criterion of
$V$, or similarily, restricting the class of candidate function to a subset of 
the set of $K$-lipschitz functions would enforce convergence of the gradient of the
approximate $V$ function to the gradient of the true $V$ function as the approximate
$V$ function gets closer to the true $V$ function. This is not true
in the general case, as any constant function can be uniformly approximated by a
$K$-lipschitz function whose derivative has unit infinity norm. A $K$-lipschitz
approximation of the true $V$ can get arbitrarily close, while its derivative remains
far from the true derivative.
% }}}

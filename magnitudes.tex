%!TEX root = main.tex

\section{Intuitions}
\label{sec:intuitions}
Reinforcement learning aims at tackling a wide variety of real world problems.
Among them obviously stand physical problems, where an agent interacts with a
physical environment, e.g.\ the real world.

Resistance to a change of time discretization is a desirable property for
algorithms aiming at attacking the reinforcement learning field in physical
setups: such algorithms should only improve as the time discretization is
refined, as refining the time discretization can only increase the amount of 
eploitable information.

However, some reinforcement learning algorithms such as \emph{Q-Learning} are
ill-behaved in the regime of small time discretization. In what follow, we
analyze in an handwavy manner the reason for this failure, and give an alternative
algorithm that remains viable in the limit of small discretizations.

\subsection{Notations}
In what follows, we loosely consider the notion of a $\emph{Markov Decision Process}$
$\mathcal{M}$ with discretization step $\deltat$. Such an MDP is the discretization
of a continuous MDP with discretization step $\deltat$. Notably, this means that
contiguous states are at distance $\bigO{\deltat}$ and that instantaneous rewards
have magnitude $\bigO{\deltat}$ for any policy, i.e.
\begin{align}
	\|s_{t + \deltat} - s_t\| &= \bigO{\deltat}\\
	r_t = \reward_t \deltat, &\quad \reward_t = \bigO{1}.
\end{align}
Policies are functions from the state space $\statespace$ to distributions on the
$\actionspace$
\begin{equation}
	\pi\colon \statespace \mapsto \mathcal{D}\left(\actionspace\right).
\end{equation}
The probability of an action $a$ in a given state $s$ under policy $\pi$ is
denoted by $\pi\left(a\mid s\right)$.

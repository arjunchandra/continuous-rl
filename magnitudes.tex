%!TEX root = main.tex

\section{Intuitions}
\label{sec:intuitions}
Reinforcement learning aims at tackling a wide variety of real world problems.
Among them obviously stand physical problems, where an agent interacts with a
physical environment, e.g.\ the real world.

Resistance to a change of time discretization is a desirable property for
algorithms aiming at attacking the reinforcement learning field in physical
setups: such algorithms should only improve as the time discretization is
refined, as refining the time discretization can only increase the amount of 
eploitable information.

However, some reinforcement learning algorithms such as \emph{Q-Learning} are
ill-behaved in the regime of small time discretization. In what follow, we
analyze in an handwavy manner the reason for this failure, and give an alternative
algorithm that remains viable in the limit of small discretizations.

\subsection{Notations}
In what follows, we loosely consider the notion of a $\emph{Markov Decision Process}$
$\mathcal{M}$ with discretization step $\deltat$. Such an MDP is the discretization
of a continuous MDP with discretization step $\deltat$. Notably, this means that
contiguous states are at distance $\bigO{\deltat}$ and that instantaneous rewards
have magnitude $\bigO{\deltat}$ for any policy, i.e.
\begin{align}
	\|s_{t + \deltat} - s_t\| &= \bigO{\deltat}\\
	r_t = \reward_t \deltat, &\quad \reward_t = \bigO{1}.
\end{align}
Policies are functions from the state space $\statespace$ to distributions on the
$\actionspace$
\begin{equation}
	\pi\colon \statespace \mapsto \mathcal{D}\left(\actionspace\right).
\end{equation}
The probability of an action $a$ in a given state $s$ under policy $\pi$ is
denoted by $\pi\left(a\mid s\right)$.

We consider discount factors of the form
\begin{equation}
	\gamma' = \left[1 - (1 - \gamma) \deltat\right],
\end{equation}
so has to have an effective discount factor close to $\gamma$.

The state value function and action-state value function of a policy
$\pi$ are defined as
\begin{align}
	V^\pi(s) &= \BE_\pi\left[
		\sum\limits_{t=s}^\infty
		{\gamma'}^{s - t} \reward_{t + (s-t)\deltat} \deltat
		\mid
		s_t = s
	\right]\\
	Q^\pi(s, a) &= \BE_\pi\left[
		\sum\limits_{t=s}^\infty
		{\gamma'}^{s - t} \reward_s \deltat
		\mid
		s_t = s, a_t = a
	\right].
\end{align}
Both functions satisfy recurrent \emph{Bellman equations}
\begin{align}
	V^\pi(s) &= \BE_\pi\left[
		\reward_t \deltat + \gamma' V^\pi(s_{t + \deltat})
		\mid
	s_t = s\right]\\
	Q^\pi(s, a) &= \BE_\pi\left[
		\reward_t \deltat + \gamma' V^\pi(s_{t + \deltat})
		\mid
	s_t = s, a_t = a\right].
\end{align}

\subsection{$Q^\pi = V^\pi$ in continuous time}
\emph{Q-learning} uses the state-action value function to determine which
action optimizes the future return.

However, in continuous time, the state-action value function no longer carries
any information on which action yields the best return. Intuitively, as the
discretization step goes to zero, the difference in resulting state between two
actions is of magnitude $\bigO{\deltat}$. Consequently, the value functions of
the corresponding states, and thus the state-action value functions only differ
by a factor of magnitude $\bigO{\deltat}$.

Formally,
\begin{align}
	V^\pi(s) - Q^\pi(s, a) &= 
	(\BE_\pi\left[\reward_t \deltat \mid s_t=s\right] -
	\BE_\pi\left[\reward_t \deltat \mid s_t=s\right]) \deltat +\\
	&\gamma'(\BE_\pi\left[
		V^\pi(s_{t + \deltat})
		\mid s_t = s
	\right] - \BE_\pi\left[
		V^\pi(s_{t + \deltat})
		\mid s_t = s, a_t=a
	\right]).\nonumber
\end{align}
Since $s_{t+\deltat} = s_t + \bigO{\deltat}$ and $V^\pi$ is differentiable,
$V^\pi(s_{t+\deltat}) = V^\pi(s_t) + \bigO{\deltat}$. Consequently,
\begin{equation}
	V^\pi(s) - Q^\pi(s, a) = \bigO{\deltat}.
\end{equation}

Notably, decision based on the values or relative values of an approximation of
the $Q^\pi(s, a)$'s for different $a$ are only meaningful when the
approximation is tighter than $\bigO{\deltat}$, at least in term of relative
values, i.e.  
\begin{equation*}
	|\tilde{Q}^\pi(s, a) - \tilde{Q}^\pi(s, a') - Q^\pi(s, a) +
	Q^\pi(s, a')| = \bigO{\deltat}.
\end{equation*}

Approximating with this degree of precision becomes harder as $\bigO{\deltat}$
decreases. \NDC{This is not convincing at the moment: as $\deltat$ decreases,
	the number of observations made increases, and this may help get a better
	estimate $\tilde{Q}^\pi(s, a)$. My guess is that the number of observations
	required to obtain an approximation of order $\varepsilon$ is superlinear in
	$\frac{1}{\varepsilon}$ and thus the previous argument holds. We either need
a simple theoretical setup where this works or an experimental one.}

\paragraph{Directly learning the advantage function.}
(See Kenji Doya, Reinforcement Learning in Continuous Space and Time,
Neural Comp.\ 2000.)
Let the system be defined by a state- and action-driven stochastic
differential equation
\begin{equation}
s_{t+\deltat}=s_t+\deltat F(s_t,a_t)+\sqrt{\deltat}\mathcal{N}(0,C(s_t))
\end{equation}
where we assume that the covariance noise does not depend on $a_t$. (If
$a_t$ itself is noisy, this will produce noise on $F$, but this noise
would typically be of order $\deltat$ unless the noise on $a_t$ is of
order $1/\sqrt{\deltat}$.)

Then with $\gamma=1$ the Bellman equation is
\begin{align}
Q(s_t,a_t)=&\E[ \reward_t(s_t,a_t) \deltat + V(s_{t+\deltat}) \mid a_t]
\\&=V(s_t)+\deltat \,\E\left[\reward_t(s_t,a_t)+\frac{\partial V(s_t)}{\partial
s}F(s_t,a_t)+\frac12 \Tr \left(C(s_t)\frac{\partial^2 V(s_t)}{\partial
s^2}\right)\mid a_t\right]+o(\deltat)
\end{align}

We only need to compute the advantage function $A$ defined by
$Q(s_t,a_t)\eqd V(s_t)+\deltat A(s_t,a_t)$; or just the advantage function
up to any term that depends only $s_t$ but not on $a_t$. By isolating the
part that
depends on $a_t$ in the above, the advantage function is
\begin{equation}
A(s_t,a_t)=\E\left[\reward_t(s_t,a_t)+\frac{\partial
V(s_t)}{\partial
s}F(s_t,a_t)\mid a_t\right]+\cdots(s_t)+o(1)
\end{equation}

Thus it is enough to learn a model of the instantaneous reward, a model
of the value function $V$ (sufficiently regularized to allow for
estimation of $\partial V(s)/\partial s$), and a model of the
instantaneous dynamics $F(s,a)$.

This extends to situations with continuous time and continuous or
discrete space, where the state jumps from $s$ to $s'$ with probability $\deltat
L(s,a,s')$ at each time. (Here $L$ is signed, with $L(s,a,s)$ usually
negative to represent the jump from $s$; $L$ is the generator of the
Markov process.) Writing $p_J(s,a,s')$ the law of
$s'$ knowing that a jump has occurred, and $\deltat J(s,a)$ the
probability of jumping, we have $L(s,a,s')=J(s,a)(p_J(s,a,s')-\delta(s))$
and
\begin{equation}
A(s_t,a_t)=\E\left[\reward_t(s_t,a_t)+J(s_t,a_t)\,\E_{s'\sim
p_J(s_t,a_t,s')} (V(s')-V(s_t))
\mid a_t\right]+\cdots(s_t)
\end{equation}

